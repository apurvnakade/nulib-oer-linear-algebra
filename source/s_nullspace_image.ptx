<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="s_nullspace_image">
  <title>Null space and image</title>
  <introduction>
    <p>
      In this section we introduce two subspaces that are associated naturally to a linear transformation <m>T\colon V\rightarrow W</m>: the <em>null space</em> and <em>image</em>.
    </p>
  </introduction>


  <subsection xml:id="ss_nullspace_image">
    <title>Null space and image of a linear transformation</title>
    <definition xml:id="d_nullspace_image">
      <title>Null space and image</title>
      <idx><h>linear transformation</h><h>null space</h></idx>
      <idx><h>linear transformation</h><h>image</h></idx>
      <idx><h>null space</h><h>of a linear transformation</h></idx>
      <idx><h>image</h></idx>

      <statement>
        <p>
          Let <m>T\colon V\rightarrow W</m> be a linear transformation.
        </p>
        <ol>
          <li>
            <title>Null space</title>
            <p>
              The <term>null space</term> of <m>T</m>, denoted <m>\NS T</m>, is defined as
              <me>
                \NS T=\{\boldv\in V\colon T(\boldv)=\boldzero_W\}
              </me>.
            </p>
          </li>
          <li>
            <title>Image</title>
            <p>
              The <term>image</term> (or <term>range</term>) of <m>T</m>, denoted <m>\im T</m>, is defined as
              <me>
                \im T=\{\boldw\in W\colon \boldw=T(\boldv) \text{ for some }  \boldv\in V \}
              </me>.
            </p>
          </li>
        </ol>
      </statement>
    </definition>
      <remark xml:id="rm_nullspace_image">
    <statement>
      <p>
      A few remarks:
      </p>
      <ol>
        <li>
          <p>
            Let <m>T\colon V\rightarrow W</m>. It is useful to keep in mind where <m>\NS T</m> and <m>\im T</m> <q>live</q> in this picture: we have <m>\NS T\subseteq V</m> and <m>\im T\subseteq W</m>. In other words, the null space is a subset of the domain, and the image is a subset of the codomain.
          </p>
        </li>
        <li>
          <p>
            Note that the image <m>\im T</m> of a linear transformation is just its image when considered simply as a function of sets. (See <xref ref="d_image"/>.)
          </p>
        </li>
        <li>
          <p>
            The notion of a null space is analogous to the set of zeros (or roots) of a real-valued function <m>f\colon X\rightarrow \R</m>,
            <me>
              \{x\in X\colon f(x)=0\}
            </me>,
            and <q>the zeros of <m>T</m></q> is a useful English shorthand for <m>\NS T</m>.
             However, there is an important difference between the null space of a linear transformation and the zeros of an arbitrary real-valued function: the null space of a linear transformation comes with the added structure of a vector space (<xref ref="th_nullspace_image"/>), whereas the zeros of an arbitrary function in general do not.
          </p>
          <p>
            The same observation can be made about the image of a linear transformation (<xref ref="th_nullspace_image"/>), in comparison to the image of an arbitrary function.
          </p>
        </li>
      </ol>
    </statement>
  </remark>
  <example xml:id="eg_nullspace_image_matrix">
    <title>Matrix transformation</title>
    <statement>
      <p>
        Let
        <me>
          A=\begin{amatrix}[rrrr] 1\amp 2\amp 3\amp 4\\ 2\amp 4\amp 6\amp 8  \end{amatrix}
        </me>,
        and let <m>T_A\colon \R^4\rightarrow \R^2</m> be its associated matrix transformation.
        Give parametric descriptions of <m>\NS T_A</m> and <m>\im T_A</m>.
      </p>
    </statement>
    <solution>
      <p>
        By definition
        <md>
          <mrow>\NS T_A \amp=\{\boldx=(x_1,x_2,x_3,x_4)\colon A\boldx=\boldzero\} </mrow>
        </md>.
        Thus we must solve the matrix equation <m>A\boldx=\boldzero</m>. The corresponding augmented matrix row reduces to
        <me>
            \begin{amatrix}[rrrr|r] \boxed{1}\amp 2\amp 3\amp 4\amp 0\\ 0\amp 0\amp 0\amp 0\amp 0  \end{amatrix}
        </me>.
        Following <xref ref="proc_solveSystem"/> we conclude that
        <me>
          \NS T_A=\{(-2r-3s-4t,r,s,t)\colon r,s,t\in \R\}
        </me>.
        Next, <m>\im T_A</m> is the set of <m>\boldy=(a,b)</m> for which there is an <m>\boldx\in \R^4</m> satisfying <m>A\boldx=\boldy</m>. Thus we are asking which choices of <m>\boldy=(a,b)</m> make the linear system
        <me>
          \begin{linsys}{4}
            x_1\amp +\amp 2x_2\amp +\amp 3x_3\amp+\amp 4x_4\amp=\amp a\\
            2x_1\amp +\amp 4x_2\amp +\amp 6x_3\amp+\amp 8x_4\amp=\amp b
          \end{linsys}
        </me>
        consistent. Again, Gaussian elimination gives us our answer. The corresponding augmented matrix row reduces to
        <me>
          \begin{amatrix}[rrrr|r] \boxed{1}\amp 2\amp 3\amp 4\amp a\\ 0\amp 0\amp 0\amp 0 \amp b-a \end{amatrix}
        </me>,
        and conclude from <xref ref="proc_solveSystem"/> that the system is consistent if and only if <m>a-b=0</m>, or <m>a=b</m>. Thus
        <me>
          \im T_A=\{(a,b)\in \R^2\colon a=b\}=\{(t,t)\colon t\in \R\}
        </me>.
      </p>
    </solution>
  </example>
  <p>
    This first example illustrates that in the special case of a matrix transformation <m>T_A\colon \R^n\rightarrow \R^m</m>, where <m>A</m> is an <m>m\times n</m> matrix, we have
    <me>
      \NS T_A=\{\boldx\in \R^n\colon T_A(\boldx)=\boldzero\}=\{\boldx\in \R^n\colon A\boldx=\boldzero\}
    </me>.
    In other words, the null space of a matrix transformation <m>T_A</m> is just the set of solutions to the matrix equation <m>A\boldx=\boldzero</m>. The situation arises frequently enough that it deserves its own notation.
  </p>
  <definition xml:id="d_nullspace_matrix">
    <idx><h>null space</h><h>of a matrix</h></idx>
    <notation>
      <usage><m>\NS A</m></usage>
      <description>the null space of <m>A</m></description>
    </notation>
    <title>Null space of a matrix</title>
    <statement>
      <p>
        Let <m>A</m> be an <m>m\times n</m> matrix. The <term>null space</term> of <m>A</m>, denoted <m>\NS A</m>,  is defined as
        <me>
          W=\{\boldx\in \R^n\colon A\boldx=\boldzero\}
        </me>.
        Equivalently, <m>\NS A=\NS T_A</m>.
      </p>
    </statement>
  </definition>
  <example xml:id="eg_nullspace_image_transposition">
    <statement>
      <p>
        Define <m>S\colon M_{nn}\rightarrow M_{nn}</m> as <m>S(A)=A^T-A</m>.
      </p>
      <ol>
        <li>
          <p>
            Prove that <m>S</m> is linear.
          </p>
        </li>
        <li>
          <p>
            Identify <m>\NS S</m> as a familiar family of matrices.
          </p>
        </li>
        <li>
          <p>
            Identify <m>\im S</m> as a familiar family of matrices.
          </p>
        </li>
      </ol>
    </statement>
    <solution>
      <p>
        <ol>
          <li>
            <p>
              Linearity is an easy consequence of transpose properties. For any <m>A_1, A_2\in M_{nn}</m> and <m>c_1,c_2\in \R</m>, we have
              <md>
                <mrow>S(c_1A_1+c_2A_2)  \amp= (c_1A_1+c_2A_2)^T-(c_1A_1+c_2A_2)  </mrow>
                <mrow> \amp = c_1A_1^T+c_2A_2^T-c_1A_1-c_2A_2\amp (<xref ref="eg_transform_transpose" text="global"/>) </mrow>
                <mrow>  \amp =c_1(A_1^T-A_1)+c_2(A_2^T-A_2)</mrow>
                <mrow>  \amp =c_1S(A_1)+c_2S(A_2)</mrow>
              </md>.
            </p>
          </li>
          <li>
            <p>
              We have
              <md>
                <mrow>\NS S \amp= \{A\in M_{nn}\colon S(A)=\boldzero\} </mrow>
                <mrow> \amp=\{A\in M_{22}\colon A^T-A=\boldzero\} </mrow>
                <mrow>  \amp=\{A\in M_{22}\colon A^T=A\} </mrow>
              </md>.
              Thus <m>\NS S</m> is the subspace of symmetric <m>n\times n</m> matrices!
            </p>
          </li>
          <li>
            <p>
              Let <m>W=\{B\in M_{nn}\colon B^T=-B\}</m>, subspace of skew-symmetric <m>n\times n</m> matrices. We claim <m>\im S=W</m>. As this is a set equality, we prove it by showing the two set inclusions <m>\im S\subseteq W</m> and <m>W\subseteq \im S</m>. (See <xref ref="ss_set_properties" text="title"/>)
            </p>
            <p>
              The inclusion <m>\im S\subseteq W</m> is the easier of the two. If <m>B\in \im S</m>, then <m>B=S(A)=A^T-A</m> for some <m>A\in M_{nn}</m>. Using various properties of transposition, we have
              <me>
                B^T=(A^T-A)^T=(A^T)^T-A^T=-(A^T-A)=-B
              </me>,
              showing that <m>B</m> is skew-symmetric, and thus <m>B\in W</m>, as desired.
            </p>
            <p>
              The inclusion <m>W\subseteq \im S</m> is trickier: we must show that if <m>B</m> is skew-symmetric, then there is an <m>A</m> such that <m>B=S(A)=A^T-A</m>. Assume we have a <m>B</m> with <m>B^T=-B</m>. Letting <m>A=-\frac{1}{2}B</m> we have
              <me>
                A^T-A=(-\frac{1}{2}B)^T+\frac{1}{2}B=\frac{1}{2}(-B^T+B)=\frac{1}{2}(B+B)=B
              </me>.
              Thus we have found a matrix <m>A</m> satisfying <m>S(A)=B</m>. It follows that <m>B\in\im T</m>.
            </p>
          </li>
        </ol>
      </p>
    </solution>
  </example>
  <example xml:id="eg_nullspace_image_derivative">
    <title>The derivative (calculus refresher)</title>
    <statement>
      <p>
        Let <m>T\colon C^1(\R)\rightarrow C(\R)</m> be the differential operator <m>T(f)=f'</m>. (See <xref ref="rm_subspaces_derivatives" text="global"/>.)  Recall that for function spaces the zero vector <m>\boldzero</m> is by definition the zero function on <m>\R</m>.
      </p>
      <p>
        The null space of <m>T</m> is the set of all differentiable functions whose derivative is the zero function:
        <me>
          \NS T=\{f\in C^1(\R)\colon f'(x)=0 \text{ for all } x\in \R\}
        </me>.
        From calculus we know that this is precisely the set of all <em>constant</em> functions. Thus
        <me>
          \NS T=\{f\in C^1(\R)\colon f \text{ a constant function}\}
        </me>.
        The image of <m>T</m> is defined as
        <md>
          <mrow>\im T\amp =\{g\in C(\R)\colon g=T(f) \text{ for some } f\in C^1(\R)\}</mrow>
          <mrow>  \amp= \{g\in C(\R)\colon g=f' \text{ for some } f\in C^1(\R)\}</mrow>
        </md>.
        In other words, <m>\im T</m> is the set of continuous functions that are the derivative of some other function: <ie />, the set of continuous functions that have an antiderivative. The fundamental theorem of calculus implies that in fact every continuous function <m>g</m> has an antiderivative! Indeed, we may take <m>f(x)=\int_0^xg(t)\, dt</m>. We conclude that <m>\im T=C(\R)</m>.
      </p>
    </statement>
  </example>
  <p>
    You may have noticed that in the examples above the null space and image of the given linear transformation turned out to be subspaces. This is no accident!
  </p>
  <theorem xml:id="th_nullspace_image">
    <title>Null space and image</title>
    <statement>
      <p>
      If <m>T\colon V\rightarrow W</m> is a linear transformation, then <m>\NS T</m> is a subspace of <m>V</m>, and <m>\im T</m> is a subspace of <m>W</m>.
      </p>
    </statement>
    <proof>
        <case>
         <title>Null space of <m>T</m></title>
        <p>
        We use the two-step technique to prove <m>\NS T</m> is a subspace.
        </p>
        <ol>
          <li>
            <p>
              Since <m>T(\boldzero_V)=\boldzero_W</m> (<xref ref="th_trans_props"/>), we see that <m>\boldzero_V\in \NS T</m>.
            </p>
          </li>
          <li>
            <p>
              Suppose <m>\boldv_1, \boldv_2\in \NS T</m>. Given any <m>c,d\in \R</m>, we have
              <md>
                <mrow>T(c\boldv_1+d\boldv_2) \amp=cT(\boldv_1)+dT(\boldv_2) \amp (T \text{ is linear, } <xref ref="th_trans_props"/>)</mrow>
                <mrow> \amp=c\boldzero_W+d\boldzero_W \amp (\boldv_1, \boldv_2\in \NS T) </mrow>
                <mrow>  \amp = \boldzero_W</mrow>
              </md>.
              This shows that <m>c\boldv_1+d\boldv_2\in \NS T</m>, completing our proof.
            </p>
          </li>
        </ol>
        </case>
        <case>
         <title>Image of <m>T</m></title>
        <p>
          The proof proceeds in a similar manner.
        </p>
        <ol>
          <li>
            <p>
              Since <m>T(\boldzero_V)=\boldzero_W</m> (<xref ref="th_trans_props"/>), we see that <m>\boldzero_W</m> is <q>hit</q> by <m>T</m>, and hence is a member of <m>\im T</m>.
            </p>
          </li>
        </ol>
        </case>
    </proof>
  </theorem>
      <remark xml:id="rm_nullspace_matrix">
    <statement>
      <p>
        In the special case where <m>T=T_A</m> is the matrix transformation of an <m>m\times n</m> matrix <m>A</m>, <xref ref="th_nullspace_image"/> tells us that
        <me>
          \NS A=\{\boldx\in \R^n\colon A\boldx=\boldzero\}
        </me>
        is a subspace. We knew this already from <xref ref="th_subspace_matrix_solutions"/>, which we now understand as a special instance of <xref ref="th_nullspace_image"/>.
      </p>
    </statement>
  </remark>

  <p>
    <xref ref="th_nullspace_image"/> gives rise to the following indirect method of proving that a subset <m>W</m> is a subspace.
  </p>
  <algorithm xml:id="proc_subspace_nullspace">
    <statement>
      <p>
        Let <m>W</m> be a subset of a vector space <m>V</m>. We can prove <em>indirectly</em> that <m>W</m> is a subspace by identifying it as the null space of a linear transformation <m>T\colon V\rightarrow W</m>.
      </p>
    </statement>
  </algorithm>

<example>
  <statement>
    <p>
      Define the subset <m>W</m> of <m>P_2</m> as
      <me>
        W=\{p\in P_2\colon p(-1)=p(2)=p(3)=0\}
      </me>.
    Prove that <m>W</m> is a subspace by identifying it as the null space of a linear transformation.
    </p>
  </statement>
  <solution>
    <p>
      Define <m>T\colon P_2\rightarrow \R^3</m> to be the <em>evaluation transformation</em> defined as <m>T(p)=(p(-1), p(2), p(3))</m>. It is a straightforward exercise to show <m>T</m> is a linear transformation. Furthermore, it is clear that <m>W=\NS T</m>. We conclude that <m>W</m> is a subspace.
    </p>
  </solution>
</example>
  <p>
    It is somewhat tricky to give a simple, concise description of the image of a linear transformation. As the next example illustrates, however, we can often come up with a parametric description by relating the problem to systems of linear equations.
  </p>
  <example xml:id="eg_subspace_image">
    <title>Image computation</title>
    <statement>
      <p>
        Consider the matrix transformation <m>T_A\colon \R^2\rightarrow\R^3</m>,
        where
        <me>
        A=\begin{bmatrix}1\amp 1\\ 2\amp 1\\ 3\amp 5 \end{bmatrix}
        </me>. Give a parametric description of <m>\im T_A</m> and identify it as a familiar geoetric object.
      </p>
    </statement>
    <solution>
      <p>
        By definition <m>\im T_A</m> is the set
        <me>
          \{\boldy\in\R^3\colon \boldy=T_A(\boldx) \text{ for some \(\boldx\in \R^3\) } \}=\left\{\boldy\colon \boldy=A\boldx \text{ for some \(\boldx\in\R^2\) } \right\}
        </me>.
        Thus to compute <m>\im T_A</m> we must determine which choice of
        <m>\boldy=(a,b,c)</m> makes the system <m>A\boldx=\boldy</m> consistent.
        We answer this using our old friend <xref ref="proc_solveSystem"/>:
        <md>
          <mrow> \begin{amatrix}[rr|r] 1\amp 1\amp a\\ 2\amp 1\amp b\\ 3\amp 5\amp c \end{amatrix} \amp \xrightarrow[r_3-3r_1]{r_2-2r_1} \begin{amatrix}[rr|r] 1\amp 1\amp a\\ 0\amp 1\amp 2a-b\\ 0\amp 0\amp -7a+2b+c \end{amatrix}</mrow>
        </md>.
        Thus for the system to be consistent we need <m>-7a+2b+c=0</m>, and we conclude
        <me>
          \im T_A=\{(a,b,c)\colon 7a+2b+c=0\}
        </me>.
        Geometrically we recognize this as the plane passing through <m>(0,0,0)</m> with normal vector <m>\boldn=(-7,2,1)</m>. To describe it parametrically we can use <xref ref="proc_solveSystem"/> again on the equation <m>7a+2b+c=0</m>. The unknowns <m>b</m> and <m>c</m> are free here, and we see that
        <me>
        \im T=\{(a,b,c)\colon 7a+2b+c=0\}=\left\{\left(\frac{1}{7}(-2r-s), r,s \right)\colon r,s\in\R\right\}
        </me>.
      </p>
    </solution>
  </example>
<p>
  Our last example in this subsection applies the concept of null space to differential equations.
</p>
  <example xml:id="eg_diff_eq_ex">
    <title>A differential equation</title>
    <statement>
      <p>
      Fix an interval <m>X\subseteq \R</m>. Let
      <m>S</m> be the set of functions of <m>C^1(X)</m> satisfying the differential equation
      <men tag='star' xml:id="eq_diff_eq_ex">
        f'=f
      </men>: <ie />,
      <m>
        S=\{f\in C^1(\R)\colon f'(x)=f(x) \text{ for all } x\in X\}
      </m>.
      Define <m>T\colon C^1(X)\rightarrow C(X)</m> as the differential operator <m>T(f)=f'-f</m>. We have
      <md>
        <mrow>f\in S \amp\iff f'=f </mrow>
        <mrow> \amp\iff f'-f=\boldzero </mrow>
        <mrow>  \amp\iff T(f)=\boldzero </mrow>
        <mrow>  \amp \iff f\in \NS T</mrow>
      </md>. Thus <m>S=\NS T</m>, and we see that the set of solutions to <xref ref="eq_diff_eq_ex"/> has the structure of a subspace. That is helpful information for us. For example, since <m>S=\NS T</m> is closed under vector addition and scalar multiplication, we know that if <m>f</m> and <m>g</m> are solutions to <xref ref="eq_diff_eq_ex"/>, then so is <m>cf+dg</m> for any <m>c,d\in\R</m>.
      </p>
      <!-- <p>
        It is an interesting exercise to try and understand what <m>\im T</m> means in terms of the given differential equation. By definition, <m>\im T</m> is the set of <m>g\in C(X)</m> such that <m>g=T(f)=f'-f</m>, for some <m>f\in C^1(X)</m>. In the language of differential equations, this is the set of all continuous functions <m>g\in C(X)</m> for which the differential equation
        <me>
          f'-f=g
        </me>
        <em>can be solved</em> for <m>f</m>.
      </p> -->
    </statement>
  </example>
  </subsection>
  <subsection xml:id="ss_injective_surjective_transforms">
    <title>Injective and surjective linear transformations</title>
    <p>
       Recall the notions of injectivity and surjectivity from <xref ref="d_injective_surjective_bijective"/>: a function <m>f\colon X\rightarrow Y</m> is injective (or one-to-one) if for all <m>x,x'\in X</m> we have <m>f(x)=f(x')</m> implies <m>x=x'</m>; it is surjective (or onto) if for all <m>y\in Y</m> there is an <m>x\in X</m> with <m>f(x)=y</m>. As with all functions, we will be interested to know whether a given linear transformation is injective or surjective; as it turns out, the concepts of null space and image give us a convenient manner of answering these questions.
       As remarked in <xref ref="d_injective_surjective_bijective"/>, there is in general a direct connection between the surjectivity and the image of a function: namely, <m>f\colon X\rightarrow Y</m> is surjective if and only if <m>\im f=Y</m>. It follows immediately that a linear transformation <m>T\colon V\rightarrow W</m> is surjective if and only if <m>\im T=W</m>. As for injectivity, it is relatively easy to see that <em>if</em> a linear transformation <m>T</m> is injective, <em>then</em> its null space must consist of just the zero vector of <m>V</m>. What is somewhat surprising is that the converse is also true, as described in (2) of the theorem below.
    </p>
    <theorem xml:id="th_nullspace_injective">
      <title>Null space and injectivity</title>
      <statement>
        <p>
          Let <m>T\colon V\rightarrow W</m> be a linear transformation.
        </p>
        <ol>
            <li>
              <p>
                Given vectors <m>\boldv, \boldv'\in V</m>, we have
                <men xml:id="eq_nullspace_injective">
                  T(\boldv)=T(\boldv') \iff \boldv'=\boldv+\boldu \text{ for some } \boldu\in \NS T
                </men>.
                Equivalently, given <m>\boldw\in W</m> and <m>\boldv\in V</m> satisfying <m>T(\boldv)=\boldw</m>, the set <m>X</m> of all <m>\boldv'</m> satisfying <m>T(\boldv')=\boldw</m> can be described as
                <men xml:id="eq_nullspace_inverseimage">
                  X=\{\boldv+\boldu\colon \boldu\in \NS T\}
                </men>.
              </p>
            </li>
            <li>
              <p>
              We have
              <me>
                T\text{ injective}\iff \NS T=\{\boldzero_V\}
              </me>.
            </p>
          </li>
        </ol>
      </statement>
      <proof>
        <ol>
          <li>
            <p>
              We have
              <md>
                <mrow>T(\boldv)=T(\boldv') \amp \iff T(\boldv')-T(\boldv)=\boldzero</mrow>
                <mrow> \amp \iff T(\boldv'-\boldv)=\boldzero \amp (T \text{ is linear})</mrow>
                <mrow>  \amp \iff \boldu=\boldv'-\boldv\in\NS T \amp (\text{def. } \NS T)</mrow>
                <mrow>  \amp \iff \boldv'=\boldv+\boldu \text{ for some } \boldu\in\NS T </mrow>
              </md>.
            Equation <xref ref="eq_nullspace_inverseimage"/> follows directly from <xref ref="eq_nullspace_injective"/> by observing that if <m>T(\boldv)=\boldw</m>, then <m>T(\boldv')=\boldw</m> if and only if <m>T(\boldv)=T(\boldv')</m>.
            </p>
          </li>
          <li>
            <p>
              According to <xref ref="eq_nullspace_injective"/> we have <m>T(\boldv)=T(\boldv')</m> if and only if <m>\boldv'=\boldv+\boldu</m> for some <m>\boldu\in \NS T</m>.</p>
            <p>If <m>\NS T=\{\boldzero_V\}</m>, then <m>T(\boldv)=T(\boldv')</m> implies <m>\boldv'=\boldv+\boldzero=\boldv</m>. Thus <m>T</m> is injective in this case.
            </p>
            <p>
              Conversely, if <m>\NS T\ne \{\boldzero_V\}</m> we can find a nonzero <m>\boldu\in \NS T</m>. It follows that for <em>any</em> <m>\boldv\in V</m> we have <m>T(\boldv)=T(\boldv+\boldu)</m>. Furthermore, since <m>\boldu\ne\boldzero_V</m>, we have <m>\boldv\ne \boldv+\boldu</m>. Thus <m>T</m> is not injective in this case.
            </p>
          </li>
        </ol>
      </proof>
    </theorem>
    <remark>
      <p>To determine whether a function of sets <m>f\colon X\rightarrow Y</m> is injective, we normally have to show that <em>for each</em> output <m>y</m> in the image of <m>f</m> there is exactly one input <m>x</m> satisfying <m>f(x)=y</m>. Think of this as checking injectivity at every output. <xref ref="th_nullspace_injective"/> tells us that in the special case of a linear transformation <m>T\colon V\rightarrow W</m> it is enough to check injectivity at <em>exactly one ouput</em>: namely, <m>\boldzero\in W</m>.
      </p>
    </remark>
    <p>
       Let <m>T\colon V\rightarrow W</m> be a linear transformation, and let <m>\boldw\in W</m>. Equation <xref ref="eq_nullspace_inverseimage"/> can be interpreted as follows: if we can find one <em>particular</em> input <m>\boldv_p</m> satisfying <m>T(\boldv_p)=\boldw</m>, then the set <m>X_\boldw</m> of <em>all</em> inputs <m>\boldv</m> satisfying <m>T(\boldv)=\boldw</m> is given by
       <me>
         X_\boldw=\{\boldv_p+\boldu\colon \boldu\in \NS T\}
       </me>.
       This set <m>X_\boldw</m> is not necessarily a subspace. Indeed, if <m>\boldw\ne \boldzero_W</m>, then <m>\boldzero\notin X_\boldw</m>! Instead, <m>X_\boldw</m> is what is called the <em>translate</em> of the subspace <m>\NS T</m> by the vector <m>\boldv_p</m>, and is denoted as <m>X_\boldw=\boldv_p+\NS T</m>. The corollary below is an application of this observation to solutions to matrix equations (equivalently, linear systems). It is obtained by treating the special case of <xref ref="th_nullspace_injective"/> where <m>T=T_A</m> is a matrix transformation.
    </p>
    <corollary xml:id="cor_matrix_equations">
      <title>Solutions to matrix equations</title>
      <statement>
        <p>
          Fix an <m>m\times n</m> matrix <m>A</m> and column vector <m>\boldb\in \R^m</m>. If <m>\boldx_p</m> is a solution to
          <men xml:id="eq_nonhomo_eq">
            A\boldx=\boldb
          </men>,
          then the set <m>X_\boldb</m> of all solutions to <xref ref="eq_nonhomo_eq"/> is described as
          <me>
            X=\{\boldx_p+\boldu\colon \boldu\in \NS A\}
          </me>.
          In other words, <m>X_\boldb</m> is the translate of <m>\NS A</m> by the vector <m>\boldx_p</m>.
        </p>
      </statement>
    </corollary>
    <project xml:id="sage_solve_matrix_eqn">
      <title>Solving matrix equations</title>
      <p>
        Let's use Sage and  <xref ref="cor_matrix_equations"/> to find the set of solutions <m>S\subseteq \R^5</m> to the matrix equation
        <men xml:id="eq_sage_matrix_eqn">
          \begin{amatrix}[rrrrr]
            0\amp 0\amp -2\amp 0\amp 7\\
            2\amp 4\amp -10\amp 6\amp 12\\
            2\amp 4\amp -5\amp 6\amp -5
          \end{amatrix}
          \begin{amatrix}[c] x_1\\ x_2\\ x_3\\ x_4\\ x_5   \end{amatrix}=
          \begin{amatrix}[r] 12\\ 28\\ -1  \end{amatrix}
        </men>.
        This is the matrix equation form of the linear system we investigated in <xref ref="sage_solve_system"/>.
        The method <c>solve_right</c> can be used to find a <em>particular solution</em> <m>\boldx_p</m> to <xref ref="eq_sage_matrix_eqn"/>.
      </p>
      <sage>
      <input>
        A=Matrix([[0,0,-2,0,7],[2,4,-10,6,12],[2,4,-5,6,-5]])
        b=vector([12,28,-1])
        xp=A.solve_right(b)
        show(xp)
        show(A*xp) # Check xp is solution
      </input>
      <output>
      (7, 0, 1, 0, 2)
      </output>
      </sage>
      <p>
        We get the entire set of solutions <m>S</m> by translating <m>\NS A</m> by the particular solution <m>\boldx_p</m>:
        <me>
          S=\{\boldx_p+\boldu\colon \boldu\in \NS A\}=\boldx_p+\NS A
        </me>.
        We can illustrate this in Sage by taking random elements of <m>\NS A</m> (computed using <c>right_kernel</c>), adding them to <c>xp</c>, and verifying that the result is a solution to <xref ref="eq_sage_matrix_eqn"/>. Each time you evaluate the cell below, a randomly generated element of <m>S</m> will be outputted.
      </p>
      <sage>
      <input>
        NS=A.right_kernel()
        u=NS.random_element(1,-50,50)
        x=xp+u
        show(x) # A solution
        show(A*x) # Check that A*x=(12,28,-1)
      </input>
      <output>
        (22, -99, 1, 61, 2)

        (12, 28, -1)
      </output>
      </sage>
      <p>
        You may wonder just how random these elements of <m>S</m> are, considering that the entries always seem to be integers! Indeed, soliciting information about <c>NS</c> from Sage, we see that it has the structure of a <q>free module</q> defined over the the <q>Integer Ring</q>.
      </p>
      <sage>
      <input>
      NS
      </input>
      <output>
        Free module of degree 5 and rank 2 over Integer Ring
        Echelon basis matrix:
        [ 1  1  0 -1  0]
        [ 0  3  0 -2  0]
      </output>
      </sage>
      <p>
        Without getting too far into the weeds, this is a result of our initial definition of <m>A</m> using <c>Matrix()</c>.
        Without further information, Sage interprets this as a matrix with <em>integer</em> coefficients, as opposed to <em>real</em> coefficients. All further computations (<eg />, <c>xp</c> and <c>NS</c>) are done in a similar spirit. More precisely, the object <c>NS</c> generated by Sage consists of all <em>integer</em> linear combinations of the two rows in the <q>echelon basis matrix</q> displayed in the cell above.
        The next cell shows you how things change when we alert Sage to the fact that we are dealing with matrices over the reals. The only change is adding <c>RR</c> to <c>Matrix()</c>,
        which specifies that matrix coefficients should be understood as real numbers.
      </p>
      <sage>
      <input>
        A=Matrix(RR,[[0,0,-2,0,7],[2,4,-10,6,12],[2,4,-5,6,-5]])
        b=vector([12,28,-1])
        xp=A.solve_right(b)
        NS=A.right_kernel()
        u=NS.random_element(1,-50,50)
        x=xp+u
        show(x) # A solution
        show(A*x) # Check that A*x=(12,28,-1)
      </input>
      <output>
          (-33.2604254229467, -14.3105503616973, 1.00000000000000, 22.9605087154471, 2.00000000000000)
          (12.0000000000000, 28.0000000000000, -1.00000000000000)
      </output>

      </sage>
    </project>
    <p>
      Our final example uses <xref ref="cor_matrix_equations"/> to nicely round out the discussion of lines and planes in <m>\R^2</m> and <m>\R^3</m> begun in <xref ref="eg_lines_planes"/>. The conclusion is that although it is not the case that all lines and planes are subspaces, it is the case that they are <em>translates</em> of subspaces.
    </p>
    <example xml:id="eg_lines_planes_general">
      <title>Lines and planes (again)</title>
      <statement>
        <p>
        Let <m>\ell\colon ax+by=c</m> be a line in <m>\R^2</m>. This line does not necessarily pass through the origin, but the line <m>\ell_0\colon ax+by=0</m> does. Using linear algebra we recognize <m>\ell</m> and <m>\ell_0</m> as the solutions to the matrix equations <m>A\boldx=c</m> and <m>A\boldx=0</m>, respectively, where
        <me>
          A=\begin{bmatrix}a\amp b \end{bmatrix}
        </me>.
        Furthermore we see that <m>\ell_0</m> is none other than <m>\NS A</m>.
        Now, fix a particular point <m>P=(x_0,y_0)\in \ell</m>. Since <m>(x_0, y_0)</m> is a solution to <m>A\boldx=c</m>, according to <xref ref="cor_matrix_equations"/> we have
        <me>
          \ell=\{P+Q\colon Q=(x,y)\in \NS A\}=\{P+Q\colon Q\in \ell_0\}
        </me>.
        In other words we see that <m>\ell</m> is just the translate of the line <m>\ell_0</m> by the vector <m>\vec{OP}=(x_0,y_0)</m>. Since <m>\ell_0=\NS A</m> is a subspace, we conclude that all lines in <m>\R^2</m> are translates of a subspace.
        </p>
        <p>
         A very similar argument can be given for an arbitrary plane <m>\mathcal{P}\colon ax+by+cz=d</m> in <m>\R^3</m> to show that it is a translate of of <m>\mathcal{P}_0\colon ax+bx+cz=0</m>, which itself is a subspace.
        </p>
      </statement>

    </example>
  </subsection>
<xi:include href="./s_nullspace_image_ex.ptx"/>

</section>
