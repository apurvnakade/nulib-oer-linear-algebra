<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2022-02-07T16:15:27Z       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Diagonalization</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="Linear algebra: the theory of vector spaces and linear transformations">
<meta property="book:author" content="Aaron Greicius">
<script>window.MathJax = {
  tex: {
    inlineMath: [['\\(','\\)']],
    tags: "none",
    useLabelIds: true,
    tagSide: "right",
    tagIndent: ".8em",
    packages: {'[+]': ['base', 'extpfeil', 'ams', 'amscd', 'newcommand', 'knowl']}
  },
  options: {
    ignoreHtmlClass: "tex2jax_ignore|ignore-math",
    processHtmlClass: "process-math",
  },
  chtml: {
    scale: 0.88,
    mtextInheritFont: true
  },
  loader: {
    load: ['input/asciimath', '[tex]/extpfeil', '[tex]/amscd', '[tex]/newcommand', '[pretext]/mathjaxknowl3.js'],
    paths: {pretext: "https://pretextbook.org/js/lib"},
  },
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.13/pretext.js"></script><script>miniversion=0.674</script><script src="https://pretextbook.org/js/0.13/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/colors_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link href="developer.css" rel="stylesheet" type="text/css">
</head>
<body class="pretext-book ignore-math has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\require{cancel}\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\T}{{\mathbb T}}
\newcommand{\F}{{\mathbb F}}
\newcommand{\HH}{{\mathbb H}}
\newcommand{\compose}{\circ}
\newcommand{\bolda}{{\mathbf a}}
\newcommand{\boldb}{{\mathbf b}}
\newcommand{\boldc}{{\mathbf c}}
\newcommand{\boldd}{{\mathbf d}}
\newcommand{\bolde}{{\mathbf e}}
\newcommand{\boldi}{{\mathbf i}}
\newcommand{\boldj}{{\mathbf j}}
\newcommand{\boldk}{{\mathbf k}}
\newcommand{\boldn}{{\mathbf n}}
\newcommand{\boldp}{{\mathbf p}}
\newcommand{\boldq}{{\mathbf q}}
\newcommand{\boldr}{{\mathbf r}}
\newcommand{\bolds}{{\mathbf s}}
\newcommand{\boldt}{{\mathbf t}}
\newcommand{\boldu}{{\mathbf u}}
\newcommand{\boldv}{{\mathbf v}}
\newcommand{\boldw}{{\mathbf w}}
\newcommand{\boldx}{{\mathbf x}}
\newcommand{\boldy}{{\mathbf y}}
\newcommand{\boldz}{{\mathbf z}}
\newcommand{\boldzero}{{\mathbf 0}}
\newcommand{\boldmod}{\boldsymbol{ \bmod }}
\newcommand{\boldT}{{\mathbf T}}
\newcommand{\boldN}{{\mathbf N}}
\newcommand{\boldB}{{\mathbf B}}
\newcommand{\boldF}{{\mathbf F}}
\newcommand{\boldS}{{\mathbf S}}
\newcommand{\boldG}{{\mathbf G}}
\newcommand{\boldK}{{\mathbf K}}
\newcommand{\boldL}{{\mathbf L}}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\NS}{null}
\DeclareMathOperator{\RS}{row}
\DeclareMathOperator{\CS}{col}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\nullity}{nullity}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Aff}{Aff}
\DeclareMathOperator{\Frac}{Frac}
\DeclareMathOperator{\Ann}{Ann}
\DeclareMathOperator{\Tor}{Tor}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\mdeg}{mdeg}
\DeclareMathOperator{\Lt}{Lt}
\DeclareMathOperator{\Lc}{Lc}
\DeclareMathOperator{\disc}{disc}
\DeclareMathOperator{\Frob}{Frob}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\curl}{curl}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\diver}{div}
\DeclareMathOperator{\flux}{flux}
\def\Gal{\operatorname{Gal}}
\def\ord{\operatorname{ord}}
\def\ML{\operatorname{M}}
\def\GL{\operatorname{GL}}
\def\PGL{\operatorname{PGL}}
\def\SL{\operatorname{SL}}
\def\PSL{\operatorname{PSL}}
\def\GSp{\operatorname{GSp}}
\def\PGSp{\operatorname{PGSp}}
\def\Sp{\operatorname{Sp}}
\def\PSp{\operatorname{PSp}}
\def\Aut{\operatorname{Aut}}
\def\Inn{\operatorname{Inn}}
\def\Hom{\operatorname{Hom}}
\def\End{\operatorname{End}}
\def\ch{\operatorname{char}}
\def\Zp{\Z/p\Z}
\def\Zm{\Z/m\Z}
\def\Zn{\Z/n\Z}
\def\Fp{\F_p}
\newcommand{\surjects}{\twoheadrightarrow}
\newcommand{\injects}{\hookrightarrow}
\newcommand{\bijects}{\leftrightarrow}
\newcommand{\isomto}{\overset{\sim}{\rightarrow}}
\newcommand{\floor}[1]{\lfloor#1\rfloor}
\newcommand{\ceiling}[1]{\left\lceil#1\right\rceil}
\newcommand{\mclass}[2][m]{[#2]_{#1}}
\newcommand{\val}[2][]{\left\lvert #2\right\rvert_{#1}}
\newcommand{\valuation}[2][]{\left\lvert #2\right\rvert_{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\anpoly}{a_nx^n+a_{n-1}x^{n-1}\cdots +a_1x+a_0}
\newcommand{\anmonic}{x^n+a_{n-1}x^{n-1}\cdots +a_1x+a_0}
\newcommand{\bmpoly}{b_mx^m+b_{m-1}x^{m-1}\cdots +b_1x+b_0}
\newcommand{\pder}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\normalin}{\trianglelefteq}
\newcommand{\angvec}[1]{\langle #1\rangle}
\newcommand{\varpoly}[2]{#1_{#2}x^{#2}+#1_{#2-1}x^{#2-1}\cdots +#1_1x+#1_0}
\newcommand{\varpower}[1][a]{#1_0+#1_1x+#1_1x^2+\cdots}
\newcommand{\limasto}[2][x]{\lim_{#1\rightarrow #2}}
\def\ntoinfty{\lim_{n\rightarrow\infty}}
\def\xtoinfty{\lim_{x\rightarrow\infty}}
\def\ii{\item}
\def\bb{\begin{enumerate}}
\def\ee{\end{enumerate}}
\def\ds{\displaystyle}
\def\p{\partial}
\newcommand{\abcdmatrix}[4]{\begin{bmatrix}#1\amp #2\\ #3\amp #4 \end{bmatrix}
}
\newenvironment{amatrix}[1][ccc|c]{\left[\begin{array}{#1}}{\end{array}\right]}
\newenvironment{linsys}[2][m]{
\begin{array}[#1]{@{}*{#2}{rc}r@{}}
}{
\end{array}}
\newcommand{\eqsys}{\begin{array}{rcrcrcr}
a_{11}x_{1}\amp +\amp a_{12}x_{2}\amp +\cdots+\amp  a_{1n}x_{n}\amp =\amp b_1\\
a_{21}x_{1}\amp +\amp a_{22}x_{2}\amp +\cdots+\amp a_{2n}x_{n}\amp =\amp b_2\\
\amp \vdots\amp   \amp \vdots \amp  \amp \vdots \amp  \vdots\\
a_{m1}x_{1}\amp +\amp a_{m2}x_{2}\amp +\cdots +\amp a_{mn}x_{n}\amp =\amp b_m
\end{array}
}
\newcommand{\numeqsys}{\begin{array}{rrcrcrcr}
e_1:\amp  a_{11}x_{1}\amp +\amp a_{12}x_{2}\amp +\cdots+\amp  a_{1n}x_{n}\amp =\amp b_1\\
e_2: \amp a_{21}x_{1}\amp +\amp a_{22}x_{2}\amp +\cdots+\amp a_{2n}x_{n}\amp =\amp b_2\\
\amp \vdots\amp   \amp \vdots \amp  \amp \vdots \amp  \vdots\\
e_m: \amp a_{m1}x_{1}\amp +\amp a_{m2}x_{2}\amp +\cdots +\amp a_{mn}x_{n}\amp =\amp b_m
\end{array}
}
\newcommand{\homsys}{\begin{array}{rcrcrcr}
a_{11}x_{1}\amp +\amp a_{12}x_{2}\amp +\cdots+\amp  a_{1n}x_{n}\amp =\amp 0\\
a_{21}x_{1}\amp +\amp a_{22}x_{2}\amp +\cdots+\amp a_{2n}x_{n}\amp =\amp 0\\
\amp \vdots\amp   \amp \vdots \amp  \amp \vdots \amp  \vdots\\
a_{m1}x_{1}\amp +\amp a_{m2}x_{2}\amp +\cdots +\amp a_{mn}x_{n}\amp =\amp 0
\end{array}
}
\newcommand{\vareqsys}[4]{
\begin{array}{ccccccc}
#3_{11}x_{1}\amp +\amp #3_{12}x_{2}\amp +\cdots+\amp  #3_{1#2}x_{#2}\amp =\amp #4_1\\
#3_{21}x_{1}\amp +\amp #3_{22}x_{2}\amp +\cdots+\amp #3_{2#2}x_{#2}\amp =\amp #4_2\\
\vdots \amp \amp \vdots \amp  \amp \vdots \amp =\amp  \vdots\\
#3_{#1 1}x_{1}\amp +\amp #3_{#1 2}x_{2}\amp +\cdots +\amp #3_{#1 #2}x_{#2}\amp =\amp #4_{#1}
\end{array}
}
\newcommand{\genmatrix}[1][a]{
\begin{bmatrix}
#1_{11} \amp  #1_{12} \amp  \cdots \amp  #1_{1n} \\
#1_{21} \amp  #1_{22} \amp  \cdots \amp  #1_{2n} \\
\vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \\
#1_{m1} \amp  #1_{m2} \amp  \cdots \amp  #1_{mn}
\end{bmatrix}
}
\newcommand{\varmatrix}[3]{
\begin{bmatrix}
#3_{11} \amp  #3_{12} \amp  \cdots \amp  #3_{1#2} \\
#3_{21} \amp  #3_{22} \amp  \cdots \amp  #3_{2#2} \\
\vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \\
#3_{#1 1} \amp  #3_{#1 2} \amp  \cdots \amp  #3_{#1 #2}
\end{bmatrix}
}
\newcommand{\augmatrix}{
\begin{amatrix}[cccc|c]
a_{11} \amp  a_{12} \amp  \cdots \amp  a_{1n} \amp b_{1}\\
a_{21} \amp  a_{22} \amp  \cdots \amp  a_{2n} \amp b_{2}\\
\vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \amp \vdots\\
a_{m1} \amp  a_{m2} \amp  \cdots \amp  a_{mn}\amp b_{m}
\end{amatrix}
}
\newcommand{\varaugmatrix}[4]{
\begin{amatrix}[cccc|c]
#3_{11} \amp  #3_{12} \amp  \cdots \amp  #3_{1#2} \amp #4_{1}\\
#3_{21} \amp  #3_{22} \amp  \cdots \amp  #3_{2#2} \amp #4_{2}\\
\vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \amp \vdots\\
#3_{#1 1} \amp  #3_{#1 2} \amp  \cdots \amp  #3_{#1 #2}\amp #4_{#1}
\end{amatrix}
}
\newcommand{\spaceforemptycolumn}{\makebox[\wd\boxofmathplus]{\ }}

\newcommand{\generalmatrix}[3]{
\left(
\begin{array}{cccc}
#1_{1,1}  \amp #1_{1,2}  \amp \ldots  \amp #1_{1,#2}  \\
#1_{2,1}  \amp #1_{2,2}  \amp \ldots  \amp #1_{2,#2}  \\
\amp \vdots                         \\
#1_{#3,1} \amp #1_{#3,2} \amp \ldots  \amp #1_{#3,#2}
\end{array}
\right)  }
\newcommand{\colvec}[2][c]{\begin{bmatrix}[#1] #2 \end{bmatrix}}
\DeclareMathOperator{\size}{size}
\DeclareMathOperator{\adjoint}{adj}
\DeclareMathOperator{\sgn}{sgn}
\newcommand{\restrictionmap}[2]{{#1}\mathpunct\upharpoonright\hbox{}_{#2}}
\renewcommand{\emptyset}{\varnothing}

\newcommand{\proj}[2]{\mbox{proj}_{#2}({#1}) }
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</span></div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href="http://linear-algebra.northwestern.pub/" target="_blank"><img src="external/images/im_holycomm.svg" alt="Logo image"></a><div class="title-container">
<h1 class="heading"><a href="book-1.html"><span class="title">Linear algebra: the theory of vector spaces and linear transformations</span></a></h1>
<p class="byline">Aaron Greicius</p>
</div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="ss_eigenvectors.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="c_transbasis.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="backmatter-1.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="ss_eigenvectors.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="c_transbasis.html" title="Up">Up</a><a class="next-button button toolbar-item" href="backmatter-1.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter"><a href="frontmatter-1.html" data-scroll="frontmatter-1" class="internal"><span class="title">Front Matter</span></a></li>
<li class="link">
<a href="c_foundations.html" data-scroll="c_foundations" class="internal"><span class="codenumber">1</span> <span class="title">Foundations</span></a><ul>
<li><a href="s_sets_functions.html" data-scroll="s_sets_functions" class="internal">Sets and functions</a></li>
<li><a href="s_logic.html" data-scroll="s_logic" class="internal">Logic</a></li>
<li><a href="s_proof_technique.html" data-scroll="s_proof_technique" class="internal">Proof techniques</a></li>
</ul>
</li>
<li class="link">
<a href="c_linear_systems.html" data-scroll="c_linear_systems" class="internal"><span class="codenumber">2</span> <span class="title">Systems of linear equations</span></a><ul>
<li><a href="s_systems.html" data-scroll="s_systems" class="internal">Systems of linear equations</a></li>
<li><a href="s_ge.html" data-scroll="s_ge" class="internal">Gaussian elimination</a></li>
<li><a href="s_solving.html" data-scroll="s_solving" class="internal">Solving linear systems</a></li>
</ul>
</li>
<li class="link">
<a href="c_matrices.html" data-scroll="c_matrices" class="internal"><span class="codenumber">3</span> <span class="title">Matrices, their arithmetic, and their algebra</span></a><ul>
<li><a href="s_matrix.html" data-scroll="s_matrix" class="internal">Matrices and their arithmetic</a></li>
<li><a href="s_algebraic.html" data-scroll="s_algebraic" class="internal">Algebra of matrices</a></li>
<li><a href="s_invertible_matrices.html" data-scroll="s_invertible_matrices" class="internal">Invertible matrices</a></li>
<li><a href="s_invertibility_theorem.html" data-scroll="s_invertibility_theorem" class="internal">The invertibility theorem</a></li>
<li><a href="s_det.html" data-scroll="s_det" class="internal">The determinant</a></li>
</ul>
</li>
<li class="link">
<a href="c_vectorspace.html" data-scroll="c_vectorspace" class="internal"><span class="codenumber">4</span> <span class="title">Vector spaces and linear transformations</span></a><ul>
<li><a href="s_vectorspace.html" data-scroll="s_vectorspace" class="internal">Real vector spaces</a></li>
<li><a href="s_transformation.html" data-scroll="s_transformation" class="internal">Linear transformations</a></li>
<li><a href="s_subspace.html" data-scroll="s_subspace" class="internal">Subspaces</a></li>
<li><a href="s_span_independence.html" data-scroll="s_span_independence" class="internal">Span and linear independence</a></li>
<li><a href="s_basis_dimension.html" data-scroll="s_basis_dimension" class="internal">Bases and dimension</a></li>
<li><a href="s_rank_nullity.html" data-scroll="s_rank_nullity" class="internal">Rank-nullity theorem and fundamental spaces</a></li>
<li><a href="s_isom.html" data-scroll="s_isom" class="internal">Isomorphisms</a></li>
</ul>
</li>
<li class="link">
<a href="c_innerproductspaces.html" data-scroll="c_innerproductspaces" class="internal"><span class="codenumber">5</span> <span class="title">Inner product spaces</span></a><ul>
<li><a href="s_innerproducts.html" data-scroll="s_innerproducts" class="internal">Inner product spaces</a></li>
<li><a href="s_orthogonality.html" data-scroll="s_orthogonality" class="internal">Orthogonal bases and orthogonal projection</a></li>
</ul>
</li>
<li class="link">
<a href="c_transbasis.html" data-scroll="c_transbasis" class="internal"><span class="codenumber">6</span> <span class="title">Eigenvectors and diagonalization</span></a><ul>
<li><a href="s_coordinatevectors_isomorphisms.html" data-scroll="s_coordinatevectors_isomorphisms" class="internal">Coordinate vectors and isomorphisms</a></li>
<li><a href="s_matrixreps.html" data-scroll="s_matrixreps" class="internal">Matrix representations of linear transformations</a></li>
<li><a href="s_changeofbasis.html" data-scroll="s_changeofbasis" class="internal">Change of basis</a></li>
<li><a href="ss_eigenvectors.html" data-scroll="ss_eigenvectors" class="internal">Eigenvectors and eigenvalues</a></li>
<li><a href="ss_diagonalization.html" data-scroll="ss_diagonalization" class="active">Diagonalization</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter-1.html" data-scroll="backmatter-1" class="internal"><span class="title">Back Matter</span></a></li>
<li class="link"><a href="appendix-notation.html" data-scroll="appendix-notation" class="internal"><span class="codenumber">A</span> <span class="title">Notation</span></a></li>
<li class="link"><a href="appendix-defs.html" data-scroll="appendix-defs" class="internal"><span class="codenumber">B</span> <span class="title">Definitions</span></a></li>
<li class="link"><a href="appendix-thms.html" data-scroll="appendix-thms" class="internal"><span class="codenumber">C</span> <span class="title">Theory and procedures</span></a></li>
<li class="link"><a href="appendix-egs.html" data-scroll="appendix-egs" class="internal"><span class="codenumber">D</span> <span class="title">Examples</span></a></li>
<li class="link"><a href="appendix-vids.html" data-scroll="appendix-vids" class="internal"><span class="codenumber">E</span> <span class="title">Video examples and figures</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1" class="internal"><span class="title">Index</span></a></li>
</ul></nav><div class="extras"><nav><a class="pretext-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section class="section" id="ss_diagonalization"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">6.5</span> <span class="title">Diagonalization</span>
</h2>
<article class="definition definition-like" id="definition-90"><h3 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">6.5.1</span><span class="period">.</span>
</h3>
<p id="p-2143">A linear transformation <span class="process-math">\(T\colon V\rightarrow V\)</span> is <dfn class="terminology">diagonalizable</dfn> if there exists a basis <span class="process-math">\(B\)</span> of <span class="process-math">\(V\)</span> for which <span class="process-math">\([T]_B\)</span> is a diagonal matrix.</p></article><p id="p-2144">At last we relate the property of being diagonalizable with the notion of eigenvectors. In the process we make clear what we mean when we say <span class="process-math">\(T\)</span> is diagonalizable if and only if it has “enough” linearly independent eigenvectors.</p>
<article class="theorem theorem-like" id="th_diagonalizability"><h3 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">6.5.2</span><span class="period">.</span><span class="space"> </span><span class="title">Diagonalizability theorem.</span>
</h3>
<p id="p-2145">Let <span class="process-math">\(T\colon V\rightarrow V\)</span> be linear, <span class="process-math">\(\dim(V)=n\text{.}\)</span></p>
<ol class="decimal">
<li id="li-728">
<span class="heading"><span class="title">Qualitative statement.</span></span><p id="p-2146">Given basis <span class="process-math">\(B\)</span> of <span class="process-math">\(V\text{,}\)</span> the matrix <span class="process-math">\([T]_B\)</span> is diagonal if and only if <span class="process-math">\(B\)</span> consists of eigenvectors of <span class="process-math">\(T\text{.}\)</span> Thus <span class="process-math">\(T\)</span> is diagonalizable if and only if there is a basis of <span class="process-math">\(V\)</span> consisting of eigenvectors of <span class="process-math">\(T\text{.}\)</span></p>
</li>
<li id="li-729">
<span class="heading"><span class="title">Quantitative statement.</span></span><p id="p-2147">Let <span class="process-math">\(\lambda_1, \lambda_2, \dots, \lambda_r\)</span> be the <em class="emphasis">distinct</em> eigenvalues of <span class="process-math">\(T\text{,}\)</span> let <span class="process-math">\(W_{\lambda_j}\)</span> be the corresponding eigenspaces, and let <span class="process-math">\(n_j=\dim W_{\lambda_j}\text{.}\)</span> Then</p>
<div class="displaymath process-math">
\begin{equation*}
T \text{ is diagonalizable } \iff n_1+n_2+\cdots +n_r=n\text{.}
\end{equation*}
</div>
</li>
</ol></article><article class="hiddenproof" id="proof-88"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-88"><h3 class="heading"><span class="type">Proof<span class="period">.</span></span></h3></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-88"><article class="hiddenproof"><p id="p-2148">The first statement was a homework exercise. The proof of the second statement is within our means, but somewhat lengthy. I will sketch its proof elsewhere. For now it is more important to understand how to use the result.</p></article></div>
<article class="algorithm theorem-like" id="proc_diagonalize"><h3 class="heading">
<span class="type">Procedure</span><span class="space"> </span><span class="codenumber">6.5.3</span><span class="period">.</span><span class="space"> </span><span class="title">Deciding whether a linear transformation is diagonalizable.</span>
</h3>
<p id="p-2149">Let <span class="process-math">\(T\colon V\rightarrow V\text{,}\)</span> where <span class="process-math">\(V\)</span> is an <span class="process-math">\(n\)</span>-dimensional vector space. To decide whether <span class="process-math">\(T\)</span> is diagonalizable proceed as follows.</p>
<ol class="decimal">
<li id="li-730"><p id="p-2150">Use <a href="" class="xref" data-knowl="./knowl/proc_eigenspaces.html" title="Procedure 6.4.5: Computing eigenspaces of a matrix">Procedure 6.4.5</a> to compute the distinct real eigenvalues <span class="process-math">\(\lambda_1, \lambda_2, \dots, \lambda_r\)</span> of <span class="process-math">\(T\text{,}\)</span> as well as bases of their corresponding eigenspaces.</p></li>
<li id="li-731">
<p id="p-2151">We have</p>
<div class="displaymath process-math">
\begin{equation*}
T \text{ diagonalizable }\iff \sum_{i=1}^r\dim W_{\lambda_i}=n\text{.}
\end{equation*}
</div>
</li>
<li id="li-732"><p id="p-2152">Let <span class="process-math">\(B_i\)</span> be an ordered basis for <span class="process-math">\(W_{\lambda_i}\text{,}\)</span> and let <span class="process-math">\(B\)</span> be the ordered basis obtained by concatenating the bases <span class="process-math">\(B_1, B_2, \dots, B_r\text{.}\)</span> Then <span class="process-math">\([T]_B\)</span> is a diagonal matrix representing <span class="process-math">\(T\text{.}\)</span></p></li>
</ol></article><article class="algorithm theorem-like" id="proc_diagonalize_matrix"><h3 class="heading">
<span class="type">Procedure</span><span class="space"> </span><span class="codenumber">6.5.4</span><span class="period">.</span>
</h3>
<p id="p-2153">Let <span class="process-math">\(A\)</span> be an <span class="process-math">\(n\times n\)</span> matrix. To decide whether <span class="process-math">\(A\)</span> is diagonalizable, proceed as follows.</p>
<ol class="decimal">
<li id="li-733"><p id="p-2154">Compute the distinct real eigenvalues <span class="process-math">\(\lambda_1, \lambda_2, \dots, \lambda_r\)</span> of <span class="process-math">\(A\)</span> as well as bases for their corresponding eigenspaces.</p></li>
<li id="li-734">
<p id="p-2155">We have</p>
<div class="displaymath process-math">
\begin{equation*}
A \text{ diagonalizable }\iff \sum_{i=1}^n\dim W_{\lambda_i}=n\text{.}
\end{equation*}
</div>
</li>
<li id="li-735">
<p id="p-2156">Let <span class="process-math">\(B_i\)</span> be an ordered basis for <span class="process-math">\(W_{\lambda_i}\text{,}\)</span> let <span class="process-math">\(B'\)</span> be the ordered basis obtained by concatenating the bases <span class="process-math">\(B_1, B_2, \dots, B_r\text{,}\)</span> and let <span class="process-math">\(P\)</span> be the matrix whose <span class="process-math">\(j\)</span>-th column is the <span class="process-math">\(j\)</span>-th element of <span class="process-math">\(B'\text{.}\)</span> Then</p>
<div class="displaymath process-math">
\begin{equation*}
D=P^{-1}AP
\end{equation*}
</div>
<p class="continuation">is diagonal. Furthermore, we have <span class="process-math">\(D=[T_A]_{B'}\text{:}\)</span> i.e., <span class="process-math">\(D\)</span> is the matrix representing <span class="process-math">\(T_A\)</span> with respect to the basis <span class="process-math">\(B'\)</span> .</p>
</li>
</ol></article><section class="subsection" id="subsection-113"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.5.1</span> <span class="title">Deciding whether $A_{n\times n}$ is diagonalizable</span>
</h3>
<ol class="decimal">
<li id="li-736"><p id="p-2157">Find the distinct eigenvalues, <span class="process-math">\(\lambda_1,\dots, \lambda_r\text{,}\)</span> of <span class="process-math">\(A\text{,}\)</span> let <span class="process-math">\(W_{\lambda_i}\)</span> be the corresponding eigenspaces, and let <span class="process-math">\(n_i=\dim W_{\lambda_i}\text{.}\)</span></p></li>
<li id="li-737"><p id="p-2158"><span class="process-math">\(A\)</span> is diagonalizable if and only if <span class="process-math">\(n_1+n_2+\cdots +n_r=n\text{.}\)</span></p></li>
<li id="li-738"><p id="p-2159">If the above equality is true, compute bases <span class="process-math">\(B_j\)</span> for each <span class="process-math">\(W_{\lambda_j}\text{.}\)</span></p></li>
<li id="li-739"><p id="p-2160">Place all the vectors from the bases <span class="process-math">\(B_j\)</span> as columns of a matrix <span class="process-math">\(P\text{.}\)</span> As these eigenvectors are linearly independent, <span class="process-math">\(P\)</span> is invertible.</p></li>
<li id="li-740"><p id="p-2161">The matrix <span class="process-math">\(D=P^{-1}AP\)</span> is diagonal. In more detail, the <span class="process-math">\(j\)</span>-th diagonal entry of <span class="process-math">\(D\)</span> is the eigenvalue associated to the eigenvector in the <span class="process-math">\(j\)</span>-th column of <span class="process-math">\(P\text{.}\)</span> This means the first <span class="process-math">\(n_1\)</span> diagonal entries of <span class="process-math">\(D\)</span> are equal to <span class="process-math">\(\lambda_1\text{,}\)</span> the next <span class="process-math">\(n_2\)</span> entries are equal to <span class="process-math">\(\lambda_2\text{,}\)</span> etc.</p></li>
</ol></section><section class="subsection" id="subsection-114"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.5.2</span> <span class="title">Example</span>
</h3>
<p id="p-2162">Take <span class="process-math">\(A=\begin{bmatrix}1\amp 1\\ 0\amp 1 \end{bmatrix}\text{.}\)</span> Earlier I claimed that this matrix is not diagonalizable. Let's see why.</p>
<p id="p-2163">The characteristic polynomial of <span class="process-math">\(A\)</span> is <span class="process-math">\(p(t)=(t-1)^2\text{.}\)</span> Thus <span class="process-math">\(\lambda=1\)</span> is the only eigenvalue of <span class="process-math">\(A\text{.}\)</span></p>
<p id="p-2164">We have <span class="process-math">\(W_1=\NS(I-A)=\NS\begin{bmatrix}0\amp -1\\ 0\amp 0 \end{bmatrix}\text{.}\)</span> We see clearly that <span class="process-math">\(\rank(I-A)=1\text{,}\)</span> and hence <span class="process-math">\(\dim W_1=\dim\NS(I-A)=2-1=1\text{.}\)</span></p>
<p id="p-2165">Since <span class="process-math">\(W_1\)</span> is the only eigenspace, and since <span class="process-math">\(\dim W_1=1\ne 2\text{,}\)</span> we conclude <span class="process-math">\(A\)</span> is not diagonalizable.</p></section><section class="subsection" id="subsection-115"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.5.3</span> <span class="title">Example</span>
</h3>
<p id="p-2166">Let <span class="process-math">\(A=\begin{bmatrix}14 \amp 21 \amp 3 \amp -39 \\ 12 \amp 25 \amp 3 \amp -41 \\ 12 \amp 24 \amp 5 \amp -42 \\ 12 \amp 22 \amp 3 \amp -38 \end{bmatrix}\text{.}\)</span></p>
<p id="p-2167">The characteristic polynomial of <span class="process-math">\(A\)</span> is <span class="process-math">\(p(t)=x^4 - 6x^3 + 9x^2 + 4x - 12\text{.}\)</span> (This is not obvious, but would be a pain to compute in detail. You may take this for granted.)</p>
<p id="p-2168">Our usual factoring tricks allow us to factor this as <span class="process-math">\(p(x)=(x-2)^2(x+1)(x-3)\text{.}\)</span></p>
<p id="p-2169">The eigenspaces are <span class="process-math">\(W_2=\NS(2I-A), W_{-1}=\NS(-I-A)\text{,}\)</span> and <span class="process-math">\(W_3=(3I-A)\text{.}\)</span> I'll leave it to you to verify that they have bases <span class="process-math">\(B=\{ (3,2,0,2), (1,1,2,1)\}\text{,}\)</span> <span class="process-math">\(B'=\{(1,1,1,1)\}\text{,}\)</span> and <span class="process-math">\(B''=\{(3,5,6,4)\}\text{,}\)</span> respectively.</p>
<p id="p-2170">It follows that the dimensions of the eigenspaces are 2, 1, and 1, respectively. Since <span class="process-math">\(2+1+1=4\text{,}\)</span> we conclude that <span class="process-math">\(A\)</span> is diagonalizable.</p>
<p id="p-2171">In more detail, we have <span class="process-math">\(D=P^{-1}AP\text{,}\)</span> where</p>
<div class="displaymath process-math">
\begin{equation*}
D=\begin{bmatrix}2\amp 0\amp 0\amp 0\\ 0\amp 2\amp 0\amp 0\\ 0\amp 0\amp -1\amp 0\\ 0\amp 0\amp 0\amp 3 \end{bmatrix} , \ \ P=\begin{bmatrix}3 \amp  1 \amp  1 \amp  3 \\ 2 \amp  1 \amp  1 \amp  5 \\ 0 \amp  2 \amp  1 \amp  6 \\ 2 \amp  1 \amp  1 \amp  4 \end{bmatrix}
\end{equation*}
</div></section><section class="subsection" id="subsection-116"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.5.4</span> <span class="title">Geometric and algebraic multiplicity</span>
</h3>
<p id="p-2172">Take <span class="process-math">\(A\)</span> (or <span class="process-math">\(T\)</span>) and suppose the characteristic polynomial <span class="process-math">\(p(t)\)</span> factors as</p>
<div class="displaymath process-math">
\begin{equation*}
p(t)=(t-\lambda_1)^{n_1}(t-\lambda_2)^{n_2}\cdots (t-\lambda_r)^{n_r}\text{,}
\end{equation*}
</div>
<p class="continuation">where the <span class="process-math">\(\lambda_i\)</span> are the <em class="emphasis">distinct</em> eigenvalues of <span class="process-math">\(A\)</span> (or <span class="process-math">\(T\)</span>). It turns out that the exponent <span class="process-math">\(n_i\text{,}\)</span> called the <dfn class="terminology">algebraic multiplicity</dfn> of the eigenvalue <span class="process-math">\(\lambda_i\text{,}\)</span> is an upper bound on <span class="process-math">\(m_i=\dim W_{\lambda_i}\text{,}\)</span> called the <dfn class="terminology">geometric multiplicity</dfn>.</p>
<article class="theorem theorem-like" id="theorem-76"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">6.5.5</span><span class="period">.</span><span class="space"> </span><span class="title">Algebraic and geometric multiplicity theorem.</span>
</h4>
<p id="p-2173">Let <span class="process-math">\(A\)</span> (or <span class="process-math">\(T\)</span>) have characterisitc polynomial</p>
<div class="displaymath process-math">
\begin{equation*}
p(t)=(t-\lambda_1)^{n_1}(t-\lambda_2)^{n_2}\cdots (t-\lambda_r)^{n_r}\text{,}
\end{equation*}
</div>
<p class="continuation">where the <span class="process-math">\(\lambda_i\)</span> are the <em class="emphasis">distinct</em> eigenvalues of <span class="process-math">\(A\)</span> (or <span class="process-math">\(T\)</span>). Then</p>
<div class="displaymath process-math">
\begin{equation*}
\dim W_{\lambda_i}\leq n_i:
\end{equation*}
</div>
<p class="continuation">i.e., the geometric multiplicity is less than or equal to the algebraic multiplicity.</p></article></section><section class="subsection" id="subsection-117"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.5.5</span> <span class="title">Linear independence and eigenvectors</span>
</h3>
<p id="p-2174">The following result is used to prove the diagonalizability theorem (<a href="" class="xref" data-knowl="./knowl/th_diagonalizability.html" title="Theorem 6.5.2: Diagonalizability theorem">Theorem 6.5.2</a>), but is also very useful in its own right.</p>
<article class="theorem theorem-like" id="th_independentteigenvectors"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">6.5.6</span><span class="period">.</span>
</h4>
<p id="p-2175">Let <span class="process-math">\(T\colon V\rightarrow V\)</span> be a linear transformation, and let <span class="process-math">\(S=\{\boldv_1,\dots, \boldv_r\}\)</span> be a set of eigenvectors of <span class="process-math">\(T\)</span> with <span class="process-math">\(T\boldv_i=\lambda_i\boldv_i\text{.}\)</span></p>
<p id="p-2176">If the <span class="process-math">\(\lambda_i\)</span> are all distinct, then <span class="process-math">\(S\)</span> is linearly independent.</p></article><article class="hiddenproof" id="proof-89"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-89"><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-89"><article class="hiddenproof"><p id="p-2177">Proved elsewhere.</p></article></div>
<article class="corollary theorem-like" id="corollary-18"><h4 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">6.5.7</span><span class="period">.</span>
</h4>
<p id="p-2178">Let <span class="process-math">\(T\colon V\rightarrow V\)</span> be a linear transformation, and suppose <span class="process-math">\(\dim V=n\text{.}\)</span></p>
<p id="p-2179">If <span class="process-math">\(T\)</span> has <span class="process-math">\(n\)</span> \alert{distinct} eigenvalues, then <span class="process-math">\(T\)</span> is diagonalizable.</p></article><article class="hiddenproof" id="proof-90"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-90"><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-90"><article class="hiddenproof"><p id="p-2180">Let <span class="process-math">\(\boldv_1, \boldv_2,\dots, \boldv_n\)</span> be eigenvectors corresponding to these <span class="process-math">\(n\)</span> distinct eigenvalues. The theorem tells us they form a linearly independent set. Since <span class="process-math">\(\dim V=n\text{,}\)</span> they form a basis for <span class="process-math">\(V\)</span> by the dimension theorem compendium. Since <span class="process-math">\(T\)</span> has a basis of eigenvectors, it is diagonalizable.</p></article></div></section><p id="p-2181"><a href="" class="xref" data-knowl="./knowl/th_independentteigenvectors.html" title="Theorem 6.5.6">Theorem 6.5.6</a> makes no assumption about the dimension of <span class="process-math">\(V\text{.}\)</span> It can thus be applied to interesting infinite-dimensional examples.</p>
<article class="example example-like" id="example-61"><a href="" data-knowl="" class="id-ref example-knowl original" data-refid="hk-example-61"><h3 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">6.5.8</span><span class="period">.</span>
</h3></a></article><div class="hidden-content tex2jax_ignore" id="hk-example-61"><article class="example example-like"><p id="p-2182">Let <span class="process-math">\(V=C^\infty(\R)\text{,}\)</span> and let <span class="process-math">\(T\colon V\rightarrow V\)</span> be defined as <span class="process-math">\(T(f)=f'\text{.}\)</span></p>
<p id="p-2183">Let <span class="process-math">\(f_i(x)=e^{k_ix}\text{,}\)</span> where the <span class="process-math">\(k_i\)</span> are all distinct constants. I claim <span class="process-math">\(S=\{f_1,f_2,\dots , f_r\}\)</span> is linearly independent.</p>
<p id="p-2184">Indeed, each <span class="process-math">\(f_i\)</span> is an eigenvector of <span class="process-math">\(T\text{,}\)</span> since <span class="process-math">\(T(f_i)=(e^{k_ix})'=k_ie^{k_ix}=k_if_i\text{.}\)</span></p>
<p id="p-2185">Since the <span class="process-math">\(k_i\)</span>'s are all distinct, it follows that the <span class="process-math">\(f_i\)</span> are eigenvectors with distinct eigenvalues, hence linearly independent!</p>
<p id="p-2186">Note: try proving that <span class="process-math">\(S\)</span> is linearly independent using the the Wronskian! You get a very interesting determinant computation.</p></article></div>
<section class="subsection" id="subsection-118"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.5.6</span> <span class="title">Final extension of the invertibility theorem</span>
</h3>
<p id="p-2187">Lastly, we can add one final statement to the invertibility theorem: <span class="process-math">\(A\)</span> is invertible if and only if <span class="process-math">\(0\)</span> is not an eigenvalue of <span class="process-math">\(A\text{.}\)</span></p>
<p id="p-2188">Indeed <span class="process-math">\(0\)</span> is an eigenvalue of <span class="process-math">\(A\)</span> if and only if <span class="process-math">\(p(0)=\det(0I-A)=\det(-A)=0\)</span> if and only if <span class="process-math">\(\det A=0\text{,}\)</span> since <span class="process-math">\(\det(-A)=(-1)^n\det A\text{.}\)</span></p>
<p id="p-2189">Since <span class="process-math">\(\det A=0\)</span> if and only if <span class="process-math">\(A\)</span> is not invertible, we conclude that <span class="process-math">\(0\)</span> is an eigenvalue of <span class="process-math">\(A\)</span> if and only if <span class="process-math">\(A\)</span> is not invertible.</p>
<p id="p-2190">You find the final version of the invertibility theorem on the next slide.</p></section><section class="subsection" id="subsection-119"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.5.7</span> <span class="title">Diagonalizable matrices</span>
</h3>
<article class="definition definition-like" id="definition-91"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">6.5.9</span><span class="period">.</span>
</h4>
<p id="p-2191">An <span class="process-math">\(n\times n\)</span> matrix <span class="process-math">\(A\)</span> is <dfn class="terminology">diagonalizable</dfn> if there is an invertible matrix <span class="process-math">\(P\)</span> such that <span class="process-math">\(D=P^{-1}AP\)</span> is diagonal.</p>
<p id="p-2192">In other words, <span class="process-math">\(A\)</span> is diagonalizable if it is <dfn class="terminology">similar</dfn> to a diagonal matrix <span class="process-math">\(D\text{.}\)</span></p></article><article class="remark remark-like" id="remark-75"><h4 class="heading">
<span class="type">Remark</span><span class="space"> </span><span class="codenumber">6.5.10</span><span class="period">.</span>
</h4>
<ol class="decimal">
<li id="li-741"><p id="p-2193">If <span class="process-math">\(A\)</span> is itself diagonal, then it is diagonalizable: we may choose <span class="process-math">\(P=I_n\)</span> in the definition.</p></li>
<li id="li-742"><p id="p-2194">In this section we will develop a systematic procedure for determining whether a matrix is diagonalizable. As we will see, the answer is yes if and only if <span class="process-math">\(A\)</span> has “enough” linearly independent eigenvectors. Of course we will spell out precisely what we mean by “enough”.</p></li>
<li id="li-743"><p id="p-2195">Not all matrices are diagonalizable. For example, <span class="process-math">\(A=\begin{bmatrix}0\amp 1\\ 0\amp 0 \end{bmatrix}\)</span> is not diagonalizable, as the aforementioned procedure will show.</p></li>
<li id="li-744"><p id="p-2196">Roughly speaking, you should interpret being diagonalizable as meaning “as good as diagonal”. To elaborate: doing arithmetic with diagonal matrices <span class="process-math">\(D\)</span> is extremely easy; if we know <span class="process-math">\(A\)</span> is diagonalizable, meaning it is similar to a diagonal matrix <span class="process-math">\(D\text{,}\)</span> then it shares many essential properties of <span class="process-math">\(D\text{,}\)</span> and we can use the relation <span class="process-math">\(D=P^{-1}AP\)</span> to help ease arithmetic computations involving <span class="process-math">\(A\text{.}\)</span></p></li>
</ol></article></section><section class="subsection" id="subsection-120"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.5.8</span> <span class="title">Properties of conjugation</span>
</h3>
<p id="p-2197">If we have, <span class="process-math">\(D=P^{-1}AP\text{,}\)</span> why exactly is there such a close connection between <span class="process-math">\(D\)</span> and <span class="process-math">\(A\text{?}\)</span> One explanation has to do with the underlying operation</p>
<div class="displaymath process-math">
\begin{equation*}
A\longmapsto P^{-1}AP\text{,}
\end{equation*}
</div>
<p class="continuation">which we call <em class="emphasis">conjugation by <span class="process-math">\(P\)</span></em>. As the following theorem outlines, conjugation by an invertible <span class="process-math">\(P\)</span> satisfies many useful properties, and we use these to relate the matrix <span class="process-math">\(A\)</span> with the matrix <span class="process-math">\(P^{-1}AP\text{.}\)</span></p>
<article class="theorem theorem-like" id="th_conjugation"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">6.5.11</span><span class="period">.</span><span class="space"> </span><span class="title">Properties of conjugation.</span>
</h4>
<p id="p-2198">Let <span class="process-math">\(P\)</span> be any invertible <span class="process-math">\(n\times n\)</span> matrix.</p>
<ol class="decimal">
<li id="li-745"><p id="p-2199"><span class="process-math">\(P^{-1}(c_1A_1+c_2A_2)P=c_1P^{-1}A_1P+c_2P^{-1}A_2P\text{.}\)</span> (\alert{Conjugation by <span class="process-math">\(P\)</span> is a linear transformation}.)</p></li>
<li id="li-746"><p id="p-2200"><span class="process-math">\(P^{-1}A^nP=(P^{-1}AP)^n\)</span> for any integer <span class="process-math">\(n\geq 0\text{.}\)</span> If <span class="process-math">\(A\)</span> is invertible, the equality holds for <em class="emphasis">all</em> integers <span class="process-math">\(n\text{.}\)</span> (\alert{Conjugation preserves powers}.)</p></li>
<li id="li-747"><p id="p-2201">Recall that given any polynomial <span class="process-math">\(f(x)=\anpoly\)</span> and any <span class="process-math">\(n\times n\)</span> matrix <span class="process-math">\(A\)</span> we define <span class="process-math">\(f(A)=a_nA^n+a_{n-1}A^{n-1}+a_1A+a_0I_n\text{.}\)</span> We have <span class="process-math">\(f(P^{-1}AP)=P^{-1}f(A)P\)</span> for any polynomial <span class="process-math">\(f(x)\text{,}\)</span> and any <span class="process-math">\(n\times n\)</span> matrix <span class="process-math">\(A\text{.}\)</span> (\alert{Conjugation preserves polynomial evaluation}.)</p></li>
</ol></article><article class="hiddenproof" id="proof-91"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-91"><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-91"><article class="hiddenproof"><p id="p-2202">Exercise.</p></article></div></section><section class="subsection" id="subsection-121"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.5.9</span> <span class="title">Examples: utility of diagonalizability</span>
</h3>
<p id="p-2203">Suppose <span class="process-math">\(A\)</span> is diagonalizable, so that <span class="process-math">\(D=P^{-1}AP\text{,}\)</span> where <span class="process-math">\(D\)</span> is diagonal with diagonal entries <span class="process-math">\(d_i\text{.}\)</span> Using <a href="" class="xref" data-knowl="./knowl/th_conjugation.html" title="Theorem 6.5.11: Properties of conjugation">Theorem 6.5.11</a>, we can now see how to translate statements about <span class="process-math">\(D\)</span> (which are generally easy to prove), to statements about <span class="process-math">\(A\)</span> (which otherwise might have been difficult to show).</p>
<article class="example example-like" id="eg_diagpowers"><a href="" data-knowl="" class="id-ref example-knowl original" data-refid="hk-eg_diagpowers"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">6.5.12</span><span class="period">.</span>
</h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-eg_diagpowers"><article class="example example-like"><p id="p-2204">To compute <span class="process-math">\(A^{n}\)</span> (hard) we can just compute <span class="process-math">\(D^{n}\)</span> (easy) and then observe that <span class="process-math">\(A=PDP^{-1}\text{,}\)</span> and thus</p>
<div class="displaymath process-math">
\begin{equation*}
A^{n}=(PDP^{-1})^{n}=PD^{n}P^{-1}\text{,}
\end{equation*}
</div>
<p class="continuation">where the last equality follows from part (b) of <a href="" class="xref" data-knowl="./knowl/th_conjugation.html" title="Theorem 6.5.11: Properties of conjugation">Theorem 6.5.11</a>; here we let <span class="process-math">\(P^{-1}\)</span> assume the role of <span class="process-math">\(P\)</span> in the theorem statement.</p>
<p id="p-2205">For example, let <span class="process-math">\(A=\begin{bmatrix}1\amp 3\\ 1\amp -1 \end{bmatrix}\text{.}\)</span> Let's compute <span class="process-math">\(A^n\)</span> for arbitrary <span class="process-math">\(n\text{.}\)</span></p>
<p id="p-2206">We have <span class="process-math">\(D=P^{-1}AP\text{,}\)</span> where <span class="process-math">\(D=\begin{bmatrix}2\amp 0\\ 0\amp -2 \end{bmatrix}\text{,}\)</span> and <span class="process-math">\(P=\begin{bmatrix}3\amp 1\\ 1\amp -1 \end{bmatrix}\text{.}\)</span> (This is not obvious yet. Soon we will have the tools to see why this is so.)</p>
<p id="p-2207">Then <span class="process-math">\(A=PDP^{-1}\text{,}\)</span> and thus</p>
<div class="displaymath process-math">
\begin{equation*}
A^{n}=PD^{n}P^{-1}=P\begin{bmatrix}2^{100}\amp 0\\ 0\amp (-2)^{100} \end{bmatrix} P^{-1}=\frac{1}{4}\begin{bmatrix}3\cdot2^n+(-2)^n\amp 3\cdot 2^n-3(-2)^{n}\\ 2^{n}-(-2)^n\amp 2^n+3(-2)^{n} \end{bmatrix}\text{.}
\end{equation*}
</div></article></div></section><section class="subsection" id="subsection-122"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.5.10</span> <span class="title">Examples: utility of diagonalizability</span>
</h3>
<p id="p-2208">Suppose <span class="process-math">\(A\)</span> is diagonalizable, so that <span class="process-math">\(D=P^{-1}AP\text{,}\)</span> where <span class="process-math">\(D\)</span> is diagonal with diagonal entries <span class="process-math">\(d_i\text{.}\)</span></p>
<article class="example example-like" id="example-63"><a href="" data-knowl="" class="id-ref example-knowl original" data-refid="hk-example-63"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">6.5.13</span><span class="period">.</span>
</h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-example-63"><article class="example example-like"><p id="p-2209">More generally, we have <span class="process-math">\(f(A)=f(PDP^{-1})=Pf(D)P^{-1}\)</span> for any polynomial <span class="process-math">\(f(x)=\anpoly\text{.}\)</span> Since <span class="process-math">\(D\)</span> is diagonal, with diagonal entries <span class="process-math">\(d_i\text{,}\)</span> it is easy to see that <span class="process-math">\(f(D)\)</span> is also diagonal, with diagonal entries <span class="process-math">\(f(d_i)\text{.}\)</span></p>
<p id="p-2210">In particular we see that <span class="process-math">\(f(A)=\underset{n\times n}{\boldzero}\)</span> if and only if <span class="process-math">\(f(D)=\underset{n\times n}{\boldzero}\text{,}\)</span> and this holds if and only if <span class="process-math">\(f(d_i)=0\)</span> for each diagonal entry <span class="process-math">\(d_i\)</span> of <span class="process-math">\(D\text{.}\)</span></p>
<p id="p-2211">Take the matrix <span class="process-math">\(A\)</span> from <a href="" class="xref" data-knowl="./knowl/eg_diagpowers.html" title="Example 6.5.12">Example 6.5.12</a>, and let <span class="process-math">\(f(x)=(x-2)(x+2)=x^2-4\text{.}\)</span> Since <span class="process-math">\(f(2)=f(-2)=0\text{,}\)</span> it follows that <span class="process-math">\(f(D)=f(A)=\underset{2\times 2}{\boldzero}\text{.}\)</span> In other words, <span class="process-math">\(A^2-4I=\underset{2\times 2}{\boldzero}\text{,}\)</span> as you can easily check.</p></article></div></section><section class="subsection" id="subsection-123"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.5.11</span> <span class="title">Examples: utility of diagonalizability</span>
</h3>
<p id="p-2212">Suppose <span class="process-math">\(A\)</span> is diagonalizable, so that <span class="process-math">\(D=P^{-1}AP\text{,}\)</span> where <span class="process-math">\(D\)</span> is diagonal with diagonal entries <span class="process-math">\(d_i\text{.}\)</span></p>
<article class="example example-like" id="example-64"><a href="" data-knowl="" class="id-ref example-knowl original" data-refid="hk-example-64"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">6.5.14</span><span class="period">.</span>
</h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-example-64"><article class="example example-like"><p id="p-2213"><span class="process-math">\(A\)</span> has a <em class="emphasis">square-root</em> (i.e., a matrix <span class="process-math">\(B\)</span> such that <span class="process-math">\(B^2=A\)</span>) iff <span class="process-math">\(D\)</span> has a square-root.</p>
<p id="p-2214">Indeed, suppose <span class="process-math">\(B^2=A\text{.}\)</span> Set <span class="process-math">\(C=P^{-1}BP\text{.}\)</span> Then <span class="process-math">\(C^2=P^{-1}B^2P=P^{-1}AP=D\text{.}\)</span> Similarly, if <span class="process-math">\(C^2=D\text{,}\)</span> then <span class="process-math">\(B^2=A\text{,}\)</span> where <span class="process-math">\(B=PCP^{-1}\text{.}\)</span></p>
<p id="p-2215">As an example, the matrix <span class="process-math">\(A=\begin{bmatrix}0\amp -2\\ 1 \amp 3 \end{bmatrix}\text{,}\)</span> satisfies <span class="process-math">\(D=P^{-1}AP\text{,}\)</span> where <span class="process-math">\(D=\begin{bmatrix}1\amp 0\\ 0\amp 2 \end{bmatrix}\text{,}\)</span> and <span class="process-math">\(P=\begin{bmatrix}2\amp 1\\ -1\amp -1 \end{bmatrix}\text{.}\)</span> Since <span class="process-math">\(C=\begin{bmatrix}1\amp 0\\ 0\amp \sqrt{2} \end{bmatrix}\)</span> is a square-root of <span class="process-math">\(D\text{,}\)</span> <span class="process-math">\(B=PCP^{-1}=\begin{bmatrix}2-\sqrt{2}\amp 2-2\sqrt{2}\\ -1+\sqrt{2}\amp -1+2\sqrt{2} \end{bmatrix}\)</span> is a square-root of <span class="process-math">\(A\text{,}\)</span> as you can easily check.</p>
<p id="p-2216">So when exactly does a diagonal matrix <span class="process-math">\(D\)</span> have a square-root? Clearly, it is sufficient that <span class="process-math">\(d_i\geq 0\)</span> for all <span class="process-math">\(i\text{,}\)</span> as in the example above. Interestingly, this is not a necessary condition! Indeed, consider the following example:</p>
<p id="p-2217"><span class="process-math">\(\begin{bmatrix}-1\amp 0\\ 0\amp -1 \end{bmatrix} =\begin{bmatrix}0\amp -1\\ 1\amp 0 \end{bmatrix} ^2\text{.}\)</span></p></article></div></section><section class="subsection" id="subsection-124"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.5.12</span> <span class="title">Properties of similarity</span>
</h3>
<p id="p-2218">Before investigating the question of when a matrix is diagonalizable, we record a few more properties illustrating the close connection between similar matrices.</p>
<article class="theorem theorem-like" id="th_similarity"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">6.5.15</span><span class="period">.</span><span class="space"> </span><span class="title">Properties of similarity.</span>
</h4>
<p id="p-2219">Suppose <span class="process-math">\(A\)</span> is similar to <span class="process-math">\(B\text{:}\)</span> i.e., there is an invertible matrix <span class="process-math">\(P\)</span> such that <span class="process-math">\(B=P^{-1}AP\text{.}\)</span> Then:</p>
<ol class="decimal">
<li id="li-748"><p id="p-2220"><span class="process-math">\(B\)</span> is similar to <span class="process-math">\(A\text{.}\)</span> (\alert{Similarity is symmetric}.)</p></li>
<li id="li-749"><p id="p-2221"><span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> have the same trace and determinant.</p></li>
<li id="li-750"><p id="p-2222"><span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> have the same rank.</p></li>
<li id="li-751"><p id="p-2223"><span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> have the same characteristic polynomial.</p></li>
<li id="li-752"><p id="p-2224"><span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> have the same eigenvalues.</p></li>
<li id="li-753"><p id="p-2225">Given any <span class="process-math">\(\lambda\in\R\text{,}\)</span> let <span class="process-math">\(W_\lambda\)</span> be the corresponding eigenspace for <span class="process-math">\(A\text{,}\)</span> and <span class="process-math">\(W_\lambda'\)</span> the corresponding eigenspace for <span class="process-math">\(B\text{.}\)</span> Then <span class="process-math">\(\dim W_{\lambda}=\dim W_{\lambda}'\text{.}\)</span></p></li>
</ol></article><p id="p-2226">The proof of (d) follows. I leave the rest as an exercise.</p></section><article class="hiddenproof" id="proof-92"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-92"><h3 class="heading"><span class="type">Proof<span class="period">.</span></span></h3></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-92"><article class="hiddenproof"><p id="p-2227">By definition we have <span class="process-math">\(B=P^{-1}AP\)</span> for some matrix <span class="process-math">\(P\text{.}\)</span> We wish to show the characteristic polynomials <span class="process-math">\(p_A(t)\)</span> and <span class="process-math">\(p_B(t)\)</span> of the two matrices are equal. Compute:</p>
<div class="displaymath process-math">
\begin{align*}
p_B(t)\amp =\amp \det(tI-B)\\
\amp =\amp \det(tI-P^{-1}AP)\\
\amp =\amp \det(tP^{-1}IP-P^{-1}AP) \ \text{ (since \(P^{-1}IP=I\))}\\
\amp =\amp \det(P^{-1}tIP-P^{-1}AP) \ \text{ (\(t\) behaves as scalar) }\\
\amp =\amp \det(P^{-1}(tI-A)P)\\
\amp =\amp \det(P^{-1})\det(tI-A)\det(P)\\
\amp =\amp (\det(P))^{-1}\det(P)\det(tI-A)\\
\amp =\amp \det(tI-A)=p_A(t)\text{.}
\end{align*}
</div></article></div>
<section class="subsection" id="subsection-125"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.5.13</span> <span class="title">The true meaning of similarity</span>
</h3>
<p id="p-2228">Hopefully <a href="" class="xref" data-knowl="./knowl/th_conjugation.html" title="Theorem 6.5.11: Properties of conjugation">Theorems 6.5.11</a> and <a href="" class="xref" data-knowl="./knowl/th_similarity.html" title="Theorem 6.5.15: Properties of similarity">Theorem 6.5.15</a> convince you that similar matrices (in the linear algebraic sence) are truly similar (in the usual sense).</p>
<p id="p-2229">There is, however, a deeper explanation for this. Namely, if <span class="process-math">\(A\)</span> and <span class="process-math">\(A'\)</span> are similar, then they are simply two different matrix representations of a common linear transformation!</p>
<p id="p-2230">In more detail: suppose we have <span class="process-math">\(A'=P^{-1}AP\text{.}\)</span></p>
<ul class="disc">
<li id="li-754"><p id="p-2231">Let <span class="process-math">\(B\)</span> be the standard basis of <span class="process-math">\(\R^n\text{,}\)</span> and let <span class="process-math">\(B'\)</span> be the basis of <span class="process-math">\(\R^n\)</span> obtained by taking the columns of the invertible matrix <span class="process-math">\(P\text{.}\)</span> Finally, let <span class="process-math">\(T=T_A\)</span> be the matrix transformation associated to <span class="process-math">\(A\text{.}\)</span></p></li>
<li id="li-755"><p id="p-2232">Then <span class="process-math">\(A=[T]_B\text{,}\)</span> <span class="process-math">\(P=\underset{B'\rightarrow B}{P}\text{,}\)</span> and <span class="process-math">\(P^{-1}=\underset{B\rightarrow B'}{P}\text{.}\)</span></p></li>
<li id="li-756">
<p id="p-2233">From the change of basis formula it follows that</p>
<div class="displaymath process-math">
\begin{equation*}
A'=P^{-1}AP=\underset{B\rightarrow B'}{P}[T]_B\underset{B'\rightarrow B}{P}=[T]_{B'}
\end{equation*}
</div>
</li>
</ul>
<p id="p-2234">In other words to say <span class="process-math">\(A\)</span> and <span class="process-math">\(A'\)</span> are similar is simply to say that they are different matrix representations of the same overlying linear transformation <span class="process-math">\(T\)</span> (see Holy Commutative Tent of Linear Algebra on next slide). All their shared properties (same eigenvalues, same determinant, same trace, etc.) are simply the properties they inherit from this one overlying <span class="process-math">\(T\text{,}\)</span> of which they are but earthly shadows.</p>
<p id="p-2235">There is one true <span class="process-math">\(T\text{!}\)</span></p></section><p id="p-2236"><div class="image-box" style="width: 75%; margin-left: 12.5%; margin-right: 12.5%;"><img src="external/images/HolyCommutativeTent.png" class="contained"></div></p>
<p id="p-2237">The previous theorem allows us to extend some of our matrix definitions to linear transformations.</p>
<article class="definition definition-like" id="definition-92"><h3 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">6.5.16</span><span class="period">.</span>
</h3>
<p id="p-2238">Let <span class="process-math">\(T\colon V\rightarrow V\)</span> be linear, let <span class="process-math">\(B\)</span> be <dfn class="terminology">any basis</dfn> of <span class="process-math">\(V\text{,}\)</span> and let <span class="process-math">\(A=[T]_B\text{.}\)</span> We define the <dfn class="terminology">characteristic polynomial</dfn> of <span class="process-math">\(T\)</span> to be <span class="process-math">\(p(t)=\det(tI-A)\text{.}\)</span></p></article><article class="theorem theorem-like" id="theorem-80"><h3 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">6.5.17</span><span class="period">.</span><span class="space"> </span><span class="title">Invertibility theorem (final version).</span>
</h3>
<p id="p-2239">Let <span class="process-math">\(A\)</span> be <span class="process-math">\(n\times n\text{.}\)</span> The following statements are equivalent.</p>
<ol class="decimal">
<li id="li-757"><p id="p-2240"><span class="process-math">\(A\)</span> is invertible.</p></li>
<li id="li-758"><p id="p-2241"><span class="process-math">\(A\boldx=\boldzero\)</span> has a unique solution (the trivial one).</p></li>
<li id="li-759"><p id="p-2242"><span class="process-math">\(A\)</span> is row equivalent to <span class="process-math">\(I_n\text{,}\)</span> the <span class="process-math">\(n\times n\)</span> identity matrix.</p></li>
<li id="li-760"><p id="p-2243"><span class="process-math">\(A\)</span> is a product of elementary matrices.</p></li>
<li id="li-761"><p id="p-2244"><span class="process-math">\(A\boldx=\boldb\)</span> has a solution for every <span class="process-math">\(n\times 1\)</span> column vector <span class="process-math">\(\boldb\text{.}\)</span></p></li>
<li id="li-762"><p id="p-2245"><span class="process-math">\(A\boldx=\boldb\)</span> has a <em class="emphasis">unique</em> solution for every <span class="process-math">\(n\times 1\)</span> column vector <span class="process-math">\(\boldb\text{.}\)</span></p></li>
<li id="li-763"><p id="p-2246"><span class="process-math">\(\det(A)\ne 0\text{.}\)</span></p></li>
<li id="li-764"><p id="p-2247"><span class="process-math">\(\NS(A)=\{\boldzero\}\text{.}\)</span></p></li>
<li id="li-765"><p id="p-2248"><span class="process-math">\(\nullity(A)=0\text{.}\)</span></p></li>
<li id="li-766"><p id="p-2249"><span class="process-math">\(\rank(A)=n\text{.}\)</span></p></li>
<li id="li-767"><p id="p-2250"><span class="process-math">\(\CS(A)=\R^n\text{.}\)</span></p></li>
<li id="li-768"><p id="p-2251"><span class="process-math">\(\RS(A)=\R^n\text{.}\)</span></p></li>
<li id="li-769"><p id="p-2252">The columns of <span class="process-math">\(A\)</span> are linearly independent (or span <span class="process-math">\(\R^n\text{,}\)</span> or are a basis of <span class="process-math">\(\R^n\)</span>).</p></li>
<li id="li-770"><p id="p-2253">The rows of <span class="process-math">\(A\)</span> are linearly independent (or span <span class="process-math">\(\R^n\text{,}\)</span> or are a basis of <span class="process-math">\(\R^n\)</span>).</p></li>
<li id="li-771"><p id="p-2254"><span class="process-math">\(A\)</span> does not have 0 as an eigenvalue.</p></li>
</ol></article></section></div></main>
</div>
</body>
</html>
