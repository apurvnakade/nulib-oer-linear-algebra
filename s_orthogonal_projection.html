<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2022-09-07T19:56:25+05:30       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Orthogonal projection</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="Linear algebra: the theory of vector spaces and linear transformations">
<meta property="book:author" content="Aaron Greicius">
<script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script><script>window.MathJax = {
  tex: {
    inlineMath: [['\\(','\\)']],
    tags: "none",
    tagSide: "right",
    tagIndent: ".8em",
    packages: {'[+]': ['base', 'extpfeil', 'ams', 'amscd', 'newcommand', 'knowl']}
  },
  options: {
    ignoreHtmlClass: "tex2jax_ignore|ignore-math",
    processHtmlClass: "process-math",
    renderActions: {
        findScript: [10, function (doc) {
            document.querySelectorAll('script[type^="math/tex"]').forEach(function(node) {
                var display = !!node.type.match(/; *mode=display/);
                var math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                var text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = {node: text, delim: '', n: 0};
                math.end = {node: text, delim: '', n: 0};
                doc.math.push(math);
            });
        }, '']
    },
  },
  chtml: {
    scale: 0.88,
    mtextInheritFont: true
  },
  loader: {
    load: ['input/asciimath', '[tex]/extpfeil', '[tex]/amscd', '[tex]/newcommand', '[pretext]/mathjaxknowl3.js'],
    paths: {pretext: "https://pretextbook.org/js/lib"},
  },
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><link href="https://webwork-ptx.aimath.org/webwork2_files/js/apps/MathView/mathview.css" rel="stylesheet">
<script src="https://pretextbook.org/js/0.13/pretext-webwork.js"></script><script src="https://webwork-ptx.aimath.org/webwork2_files/node_modules/iframe-resizer/js/iframeResizer.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.13/pretext.js"></script><script>miniversion=0.674</script><script src="https://pretextbook.org/js/0.13/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/colors_blue_red.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/setcolors.css" rel="stylesheet" type="text/css">
<!--** eBookCongig is necessary to configure interactive       **-->
<!--** Runestone components to run locally in reader's browser **-->
<!--** No external communication:                              **-->
<!--**     log level is 0, Runestone Services are disabled     **-->
<script type="text/javascript">
eBookConfig = {};
eBookConfig.useRunestoneServices = false;
eBookConfig.host = 'http://127.0.0.1:8000';
eBookConfig.course = 'PTX Course: Title Here';
eBookConfig.basecourse = 'PTX Base Course';
eBookConfig.isLoggedIn = false;
eBookConfig.email = '';
eBookConfig.isInstructor = false;
eBookConfig.logLevel = 0;
eBookConfig.username = '';
eBookConfig.readings = null;
eBookConfig.activities = null;
eBookConfig.downloadsEnabled = false;
eBookConfig.allow_pairs = false;
eBookConfig.enableScratchAC = false;
eBookConfig.build_info = "";
eBookConfig.python3 = null;
eBookConfig.acDefaultLanguage = 'python';
eBookConfig.runestone_version = '5.0.1';
eBookConfig.jobehost = '';
eBookConfig.proxyuri_runs = '';
eBookConfig.proxyuri_files = '';
eBookConfig.enable_chatcodes =  false;
</script>
<!--*** Runestone Services ***-->
<script type="text/javascript" src="https://runestone.academy/cdn/runestone/6.2.1/runtime.b0f8547c48f16a9f.bundle.js"></script><script type="text/javascript" src="https://runestone.academy/cdn/runestone/6.2.1/637.d54be67956c5c660.bundle.js"></script><script type="text/javascript" src="https://runestone.academy/cdn/runestone/6.2.1/runestone.0e9550fe42760516.bundle.js"></script><link rel="stylesheet" type="text/css" href="https://runestone.academy/cdn/runestone/6.2.1/637.fafafbd97df8a0d1.css">
<link rel="stylesheet" type="text/css" href="https://runestone.academy/cdn/runestone/6.2.1/runestone.e4d5592da655219f.css">
</head>
<body class="pretext-book ignore-math has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\require{cancel}\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\T}{{\mathbb T}}
\newcommand{\F}{{\mathbb F}}
\newcommand{\HH}{{\mathbb H}}
\newcommand{\compose}{\circ}
\newcommand{\bolda}{{\mathbf a}}
\newcommand{\boldb}{{\mathbf b}}
\newcommand{\boldc}{{\mathbf c}}
\newcommand{\boldd}{{\mathbf d}}
\newcommand{\bolde}{{\mathbf e}}
\newcommand{\boldi}{{\mathbf i}}
\newcommand{\boldj}{{\mathbf j}}
\newcommand{\boldk}{{\mathbf k}}
\newcommand{\boldn}{{\mathbf n}}
\newcommand{\boldp}{{\mathbf p}}
\newcommand{\boldq}{{\mathbf q}}
\newcommand{\boldr}{{\mathbf r}}
\newcommand{\bolds}{{\mathbf s}}
\newcommand{\boldt}{{\mathbf t}}
\newcommand{\boldu}{{\mathbf u}}
\newcommand{\boldv}{{\mathbf v}}
\newcommand{\boldw}{{\mathbf w}}
\newcommand{\boldx}{{\mathbf x}}
\newcommand{\boldy}{{\mathbf y}}
\newcommand{\boldz}{{\mathbf z}}
\newcommand{\boldzero}{{\mathbf 0}}
\newcommand{\boldmod}{\boldsymbol{ \bmod }}
\newcommand{\boldT}{{\mathbf T}}
\newcommand{\boldN}{{\mathbf N}}
\newcommand{\boldB}{{\mathbf B}}
\newcommand{\boldF}{{\mathbf F}}
\newcommand{\boldS}{{\mathbf S}}
\newcommand{\boldG}{{\mathbf G}}
\newcommand{\boldK}{{\mathbf K}}
\newcommand{\boldL}{{\mathbf L}}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\NS}{null}
\DeclareMathOperator{\RS}{row}
\DeclareMathOperator{\CS}{col}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\nullity}{nullity}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Aff}{Aff}
\DeclareMathOperator{\Frac}{Frac}
\DeclareMathOperator{\Ann}{Ann}
\DeclareMathOperator{\Tor}{Tor}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\mdeg}{mdeg}
\DeclareMathOperator{\Lt}{Lt}
\DeclareMathOperator{\Lc}{Lc}
\DeclareMathOperator{\disc}{disc}
\DeclareMathOperator{\Frob}{Frob}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\curl}{curl}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\diver}{div}
\DeclareMathOperator{\flux}{flux}
\def\Gal{\operatorname{Gal}}
\def\ord{\operatorname{ord}}
\def\ML{\operatorname{M}}
\def\GL{\operatorname{GL}}
\def\PGL{\operatorname{PGL}}
\def\SL{\operatorname{SL}}
\def\PSL{\operatorname{PSL}}
\def\GSp{\operatorname{GSp}}
\def\PGSp{\operatorname{PGSp}}
\def\Sp{\operatorname{Sp}}
\def\PSp{\operatorname{PSp}}
\def\Aut{\operatorname{Aut}}
\def\Inn{\operatorname{Inn}}
\def\Hom{\operatorname{Hom}}
\def\End{\operatorname{End}}
\def\ch{\operatorname{char}}
\def\Zp{\Z/p\Z}
\def\Zm{\Z/m\Z}
\def\Zn{\Z/n\Z}
\def\Fp{\F_p}
\newcommand{\surjects}{\twoheadrightarrow}
\newcommand{\injects}{\hookrightarrow}
\newcommand{\bijects}{\leftrightarrow}
\newcommand{\isomto}{\overset{\sim}{\rightarrow}}
\newcommand{\floor}[1]{\lfloor#1\rfloor}
\newcommand{\ceiling}[1]{\left\lceil#1\right\rceil}
\newcommand{\mclass}[2][m]{[#2]_{#1}}
\newcommand{\val}[2][]{\left\lvert #2\right\rvert_{#1}}
\newcommand{\abs}[2][]{\left\lvert #2\right\rvert_{#1}}
\newcommand{\valuation}[2][]{\left\lvert #2\right\rvert_{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\anpoly}{a_nx^n+a_{n-1}x^{n-1}\cdots +a_1x+a_0}
\newcommand{\anmonic}{x^n+a_{n-1}x^{n-1}\cdots +a_1x+a_0}
\newcommand{\bmpoly}{b_mx^m+b_{m-1}x^{m-1}\cdots +b_1x+b_0}
\newcommand{\pder}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\normalin}{\trianglelefteq}
\newcommand{\angvec}[1]{\langle #1\rangle}
\newcommand{\varpoly}[2]{#1_{#2}x^{#2}+#1_{#2-1}x^{#2-1}\cdots +#1_1x+#1_0}
\newcommand{\varpower}[1][a]{#1_0+#1_1x+#1_1x^2+\cdots}
\newcommand{\limasto}[2][x]{\lim_{#1\rightarrow #2}}
\def\ntoinfty{\lim_{n\rightarrow\infty}}
\def\xtoinfty{\lim_{x\rightarrow\infty}}
\def\ii{\item}
\def\bb{\begin{enumerate}}
\def\ee{\end{enumerate}}
\def\ds{\displaystyle}
\def\p{\partial}
\newcommand{\abcdmatrix}[4]{\begin{bmatrix}#1\amp #2\\ #3\amp #4 \end{bmatrix}
}
\newenvironment{amatrix}[1][ccc|c]{\left[\begin{array}{#1}}{\end{array}\right]}
\newenvironment{linsys}[2][m]{
\begin{array}[#1]{@{}*{#2}{rc}r@{}}
}{
\end{array}}
\newcommand{\eqsys}{\begin{array}{rcrcrcr}
a_{11}x_{1}\amp +\amp a_{12}x_{2}\amp +\cdots+\amp  a_{1n}x_{n}\amp =\amp b_1\\
a_{21}x_{1}\amp +\amp a_{22}x_{2}\amp +\cdots+\amp a_{2n}x_{n}\amp =\amp b_2\\
\amp \vdots\amp   \amp \vdots \amp  \amp \vdots \amp  \vdots\\
a_{m1}x_{1}\amp +\amp a_{m2}x_{2}\amp +\cdots +\amp a_{mn}x_{n}\amp =\amp b_m
\end{array}
}
\newcommand{\numeqsys}{\begin{array}{rrcrcrcr}
e_1:\amp  a_{11}x_{1}\amp +\amp a_{12}x_{2}\amp +\cdots+\amp  a_{1n}x_{n}\amp =\amp b_1\\
e_2: \amp a_{21}x_{1}\amp +\amp a_{22}x_{2}\amp +\cdots+\amp a_{2n}x_{n}\amp =\amp b_2\\
\amp \vdots\amp   \amp \vdots \amp  \amp \vdots \amp  \vdots\\
e_m: \amp a_{m1}x_{1}\amp +\amp a_{m2}x_{2}\amp +\cdots +\amp a_{mn}x_{n}\amp =\amp b_m
\end{array}
}
\newcommand{\homsys}{\begin{array}{rcrcrcr}
a_{11}x_{1}\amp +\amp a_{12}x_{2}\amp +\cdots+\amp  a_{1n}x_{n}\amp =\amp 0\\
a_{21}x_{1}\amp +\amp a_{22}x_{2}\amp +\cdots+\amp a_{2n}x_{n}\amp =\amp 0\\
\amp \vdots\amp   \amp \vdots \amp  \amp \vdots \amp  \vdots\\
a_{m1}x_{1}\amp +\amp a_{m2}x_{2}\amp +\cdots +\amp a_{mn}x_{n}\amp =\amp 0
\end{array}
}
\newcommand{\vareqsys}[4]{
\begin{array}{ccccccc}
#3_{11}x_{1}\amp +\amp #3_{12}x_{2}\amp +\cdots+\amp  #3_{1#2}x_{#2}\amp =\amp #4_1\\
#3_{21}x_{1}\amp +\amp #3_{22}x_{2}\amp +\cdots+\amp #3_{2#2}x_{#2}\amp =\amp #4_2\\
\vdots \amp \amp \vdots \amp  \amp \vdots \amp =\amp  \vdots\\
#3_{#1 1}x_{1}\amp +\amp #3_{#1 2}x_{2}\amp +\cdots +\amp #3_{#1 #2}x_{#2}\amp =\amp #4_{#1}
\end{array}
}
\newcommand{\genmatrix}[1][a]{
\begin{bmatrix}
#1_{11} \amp  #1_{12} \amp  \cdots \amp  #1_{1n} \\
#1_{21} \amp  #1_{22} \amp  \cdots \amp  #1_{2n} \\
\vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \\
#1_{m1} \amp  #1_{m2} \amp  \cdots \amp  #1_{mn}
\end{bmatrix}
}
\newcommand{\varmatrix}[3]{
\begin{bmatrix}
#3_{11} \amp  #3_{12} \amp  \cdots \amp  #3_{1#2} \\
#3_{21} \amp  #3_{22} \amp  \cdots \amp  #3_{2#2} \\
\vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \\
#3_{#1 1} \amp  #3_{#1 2} \amp  \cdots \amp  #3_{#1 #2}
\end{bmatrix}
}
\newcommand{\augmatrix}{
\begin{amatrix}[cccc|c]
a_{11} \amp  a_{12} \amp  \cdots \amp  a_{1n} \amp b_{1}\\
a_{21} \amp  a_{22} \amp  \cdots \amp  a_{2n} \amp b_{2}\\
\vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \amp \vdots\\
a_{m1} \amp  a_{m2} \amp  \cdots \amp  a_{mn}\amp b_{m}
\end{amatrix}
}
\newcommand{\varaugmatrix}[4]{
\begin{amatrix}[cccc|c]
#3_{11} \amp  #3_{12} \amp  \cdots \amp  #3_{1#2} \amp #4_{1}\\
#3_{21} \amp  #3_{22} \amp  \cdots \amp  #3_{2#2} \amp #4_{2}\\
\vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \amp \vdots\\
#3_{#1 1} \amp  #3_{#1 2} \amp  \cdots \amp  #3_{#1 #2}\amp #4_{#1}
\end{amatrix}
}
\newcommand{\spaceforemptycolumn}{\makebox[\wd\boxofmathplus]{\ }}

\newcommand{\generalmatrix}[3]{
\left(
\begin{array}{cccc}
#1_{1,1}  \amp #1_{1,2}  \amp \ldots  \amp #1_{1,#2}  \\
#1_{2,1}  \amp #1_{2,2}  \amp \ldots  \amp #1_{2,#2}  \\
\amp \vdots                         \\
#1_{#3,1} \amp #1_{#3,2} \amp \ldots  \amp #1_{#3,#2}
\end{array}
\right)  }
\newcommand{\colvec}[2][c]{\begin{amatrix}[#1] #2 \end{amatrix}}
\DeclareMathOperator{\size}{size}
\DeclareMathOperator{\adjoint}{adj}
\DeclareMathOperator{\sgn}{sgn}
\newcommand{\restrictionmap}[2]{{#1}\mathpunct\upharpoonright\hbox{}_{#2}}
\renewcommand{\emptyset}{\varnothing}

\newcommand{\proj}[2]{\mbox{proj}_{#2}({#1}) }
\renewcommand{\Re}{\operatorname{Re}}
 \renewcommand{\Im}{\operatorname{Im}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href="http://linear-algebra.northwestern.pub/" target="_blank"><img src="external/images/im_holycomm.svg" alt="Logo image"></a><div class="title-container">
<h1 class="heading"><a href="book-1.html"><span class="title">Linear algebra: the theory of vector spaces and linear transformations</span></a></h1>
<p class="byline">Aaron Greicius</p>
</div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="s_orthogonality.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="c_innerproductspaces.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="c_transbasis.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="s_orthogonality.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="c_innerproductspaces.html" title="Up">Up</a><a class="next-button button toolbar-item" href="c_transbasis.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter">
<a href="frontmatter-1.html" data-scroll="frontmatter-1" class="internal"><span class="title">Front Matter</span></a><ul>
<li><a href="colophon-1.html" data-scroll="colophon-1" class="internal">Colophon</a></li>
<li><a href="acknowledgement-1.html" data-scroll="acknowledgement-1" class="internal">Acknowledgements</a></li>
</ul>
</li>
<li class="link">
<a href="c_foundations.html" data-scroll="c_foundations" class="internal"><span class="codenumber">0</span> <span class="title">Foundations</span></a><ul>
<li><a href="s_sets_functions.html" data-scroll="s_sets_functions" class="internal">Sets and functions</a></li>
<li><a href="s_logic.html" data-scroll="s_logic" class="internal">Logic</a></li>
<li><a href="s_proof_technique.html" data-scroll="s_proof_technique" class="internal">Proof techniques</a></li>
<li><a href="s_complex_numbers.html" data-scroll="s_complex_numbers" class="internal">Complex numbers</a></li>
<li><a href="s_polynomials.html" data-scroll="s_polynomials" class="internal">Polynomials</a></li>
</ul>
</li>
<li class="link">
<a href="c_linear_systems.html" data-scroll="c_linear_systems" class="internal"><span class="codenumber">1</span> <span class="title">Systems of linear equations</span></a><ul>
<li><a href="s_systems.html" data-scroll="s_systems" class="internal">Systems of linear equations</a></li>
<li><a href="s_ge.html" data-scroll="s_ge" class="internal">Gaussian elimination</a></li>
<li><a href="s_solving.html" data-scroll="s_solving" class="internal">Solving linear systems</a></li>
</ul>
</li>
<li class="link">
<a href="c_matrices.html" data-scroll="c_matrices" class="internal"><span class="codenumber">2</span> <span class="title">Matrices, their arithmetic, and their algebra</span></a><ul>
<li><a href="s_matrix.html" data-scroll="s_matrix" class="internal">Matrices and their arithmetic</a></li>
<li><a href="s_algebraic.html" data-scroll="s_algebraic" class="internal">Algebra of matrices</a></li>
<li><a href="s_invertible_matrices.html" data-scroll="s_invertible_matrices" class="internal">Invertible matrices</a></li>
<li><a href="s_invertibility_theorem.html" data-scroll="s_invertibility_theorem" class="internal">The invertibility theorem</a></li>
<li><a href="s_det.html" data-scroll="s_det" class="internal">The determinant</a></li>
</ul>
</li>
<li class="link">
<a href="c_vectorspace.html" data-scroll="c_vectorspace" class="internal"><span class="codenumber">3</span> <span class="title">Vector spaces and linear transformations</span></a><ul>
<li><a href="s_vectorspace.html" data-scroll="s_vectorspace" class="internal">Real vector spaces</a></li>
<li><a href="s_transformation.html" data-scroll="s_transformation" class="internal">Linear transformations</a></li>
<li><a href="s_subspace.html" data-scroll="s_subspace" class="internal">Subspaces</a></li>
<li><a href="s_nullspace_image.html" data-scroll="s_nullspace_image" class="internal">Null space and image</a></li>
<li><a href="s_span_independence.html" data-scroll="s_span_independence" class="internal">Span and linear independence</a></li>
<li><a href="s_basis.html" data-scroll="s_basis" class="internal">Bases</a></li>
<li><a href="s_dimension.html" data-scroll="s_dimension" class="internal">Dimension</a></li>
<li><a href="s_rank_nullity.html" data-scroll="s_rank_nullity" class="internal">Rank-nullity theorem and fundamental spaces</a></li>
<li><a href="s_isom.html" data-scroll="s_isom" class="internal">Isomorphisms</a></li>
</ul>
</li>
<li class="link">
<a href="c_innerproductspaces.html" data-scroll="c_innerproductspaces" class="internal"><span class="codenumber">4</span> <span class="title">Inner product spaces</span></a><ul>
<li><a href="s_innerproducts.html" data-scroll="s_innerproducts" class="internal">Inner product spaces</a></li>
<li><a href="s_orthogonality.html" data-scroll="s_orthogonality" class="internal">Orthogonal bases</a></li>
<li><a href="s_orthogonal_projection.html" data-scroll="s_orthogonal_projection" class="active">Orthogonal projection</a></li>
</ul>
</li>
<li class="link">
<a href="c_transbasis.html" data-scroll="c_transbasis" class="internal"><span class="codenumber">5</span> <span class="title">Eigenvectors and diagonalization</span></a><ul>
<li><a href="s_coordinatevectors.html" data-scroll="s_coordinatevectors" class="internal">Coordinate vectors</a></li>
<li><a href="s_matrixreps.html" data-scroll="s_matrixreps" class="internal">Matrix representations of linear transformations</a></li>
<li><a href="s_changeofbasis.html" data-scroll="s_changeofbasis" class="internal">Change of basis</a></li>
<li><a href="s_eigenvectors.html" data-scroll="s_eigenvectors" class="internal">Eigenvectors and eigenvalues</a></li>
<li><a href="s_diagonalization.html" data-scroll="s_diagonalization" class="internal">Diagonalization</a></li>
<li><a href="s_spectral_theorem.html" data-scroll="s_spectral_theorem" class="internal">The spectral theorem</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter-1.html" data-scroll="backmatter-1" class="internal"><span class="title">Back Matter</span></a></li>
<li class="link"><a href="appendix-notation.html" data-scroll="appendix-notation" class="internal"><span class="codenumber">A</span> <span class="title">Notation</span></a></li>
<li class="link"><a href="appendix-defs.html" data-scroll="appendix-defs" class="internal"><span class="codenumber">B</span> <span class="title">Definitions</span></a></li>
<li class="link"><a href="appendix-thms.html" data-scroll="appendix-thms" class="internal"><span class="codenumber">C</span> <span class="title">Theory and procedures</span></a></li>
<li class="link"><a href="appendix-egs.html" data-scroll="appendix-egs" class="internal"><span class="codenumber">D</span> <span class="title">Examples</span></a></li>
<li class="link"><a href="appendix-sage.html" data-scroll="appendix-sage" class="internal"><span class="codenumber">E</span> <span class="title">Sage examples</span></a></li>
<li class="link"><a href="appendix-mantras.html" data-scroll="appendix-mantras" class="internal"><span class="codenumber">F</span> <span class="title">Video examples and figures</span></a></li>
<li class="link"><a href="appendix-vids.html" data-scroll="appendix-vids" class="internal"><span class="codenumber">G</span> <span class="title">Mantras and fiats</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1" class="internal"><span class="title">Index</span></a></li>
</ul></nav><div class="extras"><nav><a class="pretext-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section class="section" id="s_orthogonal_projection"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">4.3</span> <span class="title">Orthogonal projection</span>
</h2>
<section class="introduction" id="introduction-63"><p id="p-2476">A trick we learn early on in physics-- specifically, in dynamics problems in <span class="process-math">\(\R^2\)</span>-- is to pick a convenient axis and then decompose any relevant vectors (force, acceleration, velocity, position, etc.) into a sum of two components: one that points along the chosen axis, and one that points perpendicularly to it. As we will see in this section, this technique can be vastly generalized. Namely, instead of <span class="process-math">\(\R^2\)</span> we  can take any inner product space <span class="process-math">\((V, \langle\, , \rangle)\text{;}\)</span> and instead of a chosen axis in <span class="process-math">\(\R^2\text{,}\)</span> we can choose any finite-dimensional subspace <span class="process-math">\(W\subseteq V\text{;}\)</span> then any <span class="process-math">\(\boldv\in V\)</span> can be decomposed in the form</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\boldv=\boldw+\boldw^\perp,
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(\boldw\in W\)</span> and <span class="process-math">\(\boldw^\perp\)</span> is a vector <em class="emphasis">orthogonal</em> to <span class="process-math">\(W\text{,}\)</span> in a sense we will make precise below. Just as in our toy physics example, this manner of decomposing vectors helps simplify computations in problems where the subspace <span class="process-math">\(W\)</span> chosen is of central importance.</p></section><section class="subsection" id="ss_ortho_complement"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.3.1</span> <span class="title">Orthogonal complement</span>
</h3>
<section class="introduction" id="introduction-64"><p id="p-2477">We begin by making sense of what it means for a vector to be orthogonal to a subspace.</p></section><article class="definition definition-like" id="d_orthogonal_complement"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.3.1</span><span class="period">.</span><span class="space"> </span><span class="title">Orthogonal complement.</span>
</h4>
<p id="p-2478">. Let <span class="process-math">\((V,\langle \ , \rangle)\)</span> be an inner product vector space, and let <span class="process-math">\(W\subseteq V\)</span> be a subspace.</p>
<p id="p-2479">A vector <span class="process-math">\(\boldv\)</span> is <dfn class="terminology">orthogonal</dfn> to <span class="process-math">\(W\)</span> if it is orthogonal to every element of <span class="process-math">\(W\text{:}\)</span>i.e., if <span class="process-math">\(\langle \boldv, \boldw\rangle=0\)</span> for all <span class="process-math">\(\boldw\in W\text{.}\)</span></p>
<p id="p-2480">The <dfn class="terminology">orthogonal complement</dfn> of <span class="process-math">\(W\text{,}\)</span> denoted <span class="process-math">\(W^\perp\text{,}\)</span> is the set of all elements of <span class="process-math">\(V\)</span> orthogonal to <span class="process-math">\(W\text{:}\)</span> i.e.,</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
W^\perp=\{\boldv\in V\colon \langle \boldv, \boldw\rangle=0 \text{ for all } \boldw\in W\}\text{.}
\end{equation*}
</div></article><article class="remark remark-like" id="rm_computing_ortho_comp"><h4 class="heading">
<span class="type">Remark</span><span class="space"> </span><span class="codenumber">4.3.2</span><span class="period">.</span><span class="space"> </span><span class="title">Computing <span class="process-math">\(W^\perp\)</span>.</span>
</h4>
<p id="p-2481">According to <a href="" class="xref" data-knowl="./knowl/d_orthogonal_complement.html" title="Definition 4.3.1: Orthogonal complement">Definition 4.3.1</a>, to verify that a vector <span class="process-math">\(\boldv\)</span> lies in <span class="process-math">\(W^\perp\text{,}\)</span> we must show that <span class="process-math">\(\langle \boldv, \boldw\rangle=0\)</span> for all <span class="process-math">\(\boldw\in W\text{.}\)</span> The “for all” quantifier here can potentially make this an onerous task: there are in principle infinitely many <span class="process-math">\(\boldw\)</span> to check! In the special case where <span class="process-math">\(W\)</span> has a finite spanning set, so that <span class="process-math">\(W=\Span \{\boldw_1, \boldw_2,\dots, \boldw_r\}\)</span> for some vectors <span class="process-math">\(\boldw_i\text{,}\)</span> deciding whether <span class="process-math">\(\boldv\in W^\perp\)</span> reduces to checking whether <span class="process-math">\(\langle \boldv, \boldw_i\rangle=0\)</span> for all <span class="process-math">\(1\leq i\leq r\text{.}\)</span> In other words, we have</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/d_orthogonal_complement.html ./knowl/ex_ortho_comp.html">
\begin{equation*}
\boldv\in W^\perp\iff \langle \boldv, \boldw_i\rangle=0 \text{ for all } 1\leq i\leq r\text{.}
\end{equation*}
</div>
<p class="continuation">The forward implication of this equivalence is clear: if <span class="process-math">\(\boldv\)</span> is orthogonal to all elements of <span class="process-math">\(W\text{,}\)</span> then clearly it is orthogonal to each <span class="process-math">\(\boldw_i\text{.}\)</span> The reverse implication is left as an exercise. (See <a href="" class="xref" data-knowl="./knowl/ex_ortho_comp.html" title="Exercise 4.3.6.7">Exercise 4.3.6.7</a>.)</p>
<p id="p-2482">We illustrate this computational technique in the next examples.</p></article><article class="example example-like" id="eg_ortho_comp_line"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.3.3</span><span class="period">.</span>
</h4>
<p id="p-2483">Consider the inner product space <span class="process-math">\(\R^2\)</span> together with the dot product. Let <span class="process-math">\(W=\Span\{(1,1)\}=\{(t,t)\colon t\in \R\}\text{:}\)</span> the line <span class="process-math">\(\ell\subseteq \R^2\)</span> with equation <span class="process-math">\(y=x\text{.}\)</span> Compute <span class="process-math">\(W^\perp\)</span> and identify it as a familiar geometric object in <span class="process-math">\(\R^2\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-78" id="solution-78"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-78"><div class="solution solution-like">
<p id="p-2484">According to <a href="" class="xref" data-knowl="./knowl/rm_computing_ortho_comp.html" title="Remark 4.3.2: Computing W^\perp">Remark 4.3.2</a>, since <span class="process-math">\(W=\Span\{(1,1)\}\text{,}\)</span> we have</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/rm_computing_ortho_comp.html">
\begin{equation*}
\boldx\in W^\perp \iff \boldx\cdot (1,1)=0\text{.}
\end{equation*}
</div>
<p class="continuation">Letting <span class="process-math">\(\boldx=(x,y)\text{,}\)</span> we see that <span class="process-math">\(\boldx\cdot (1,1)=0\)</span> if and only if <span class="process-math">\(x+y=0\text{,}\)</span> if and only if <span class="process-math">\(y=-x\text{.}\)</span> Thus <span class="process-math">\(W^\perp=\{(x,y)\colon y=-x\}\)</span> is the line <span class="process-math">\(\ell'\subseteq \R^2\)</span> with equation <span class="process-math">\(y=-x\text{.}\)</span> Observe that the lines <span class="process-math">\(\ell\)</span> and <span class="process-math">\(\ell'\)</span> are indeed perpendicular to one another. (Graph them!)</p>
</div></div>
</div></article><article class="example example-like" id="eg_ortho_comp_plane"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.3.4</span><span class="period">.</span>
</h4>
<p id="p-2485">Consider the inner product space <span class="process-math">\(\R^3\)</span> together with the dot product. Let <span class="process-math">\(W\subseteq \R^3\)</span> be the plane with equation <span class="process-math">\(x-2y-z=0\text{.}\)</span>  Compute <span class="process-math">\(W^\perp\)</span> and identify this as a familiar geometric object in <span class="process-math">\(\R^3\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-79" id="solution-79"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-79"><div class="solution solution-like">
<p id="p-2486">First, solving <span class="process-math">\(x-2y-z=0\)</span> for <span class="process-math">\((x,y,z)\text{,}\)</span> we see that</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/rm_computing_ortho_comp.html">
\begin{equation*}
W=\{(2s+t,s,t)\colon s,t\in \R\}=\Span\{(2,1,0),(1,0,1)\}\text{.}
\end{equation*}
</div>
<p class="continuation">Next, according to <a href="" class="xref" data-knowl="./knowl/rm_computing_ortho_comp.html" title="Remark 4.3.2: Computing W^\perp">Remark 4.3.2</a> we have</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/rm_computing_ortho_comp.html">
\begin{equation*}
\boldx\in W^\perp\iff \boldx\cdot (2,1,0)=0 \text{ and } \boldx\cdot (1,0,1)=0\text{.}
\end{equation*}
</div>
<p class="continuation">It follows that <span class="process-math">\(W^\perp\)</span> is the set of vectors <span class="process-math">\(\boldx=(x,y,z)\)</span> satisfying the linear system</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/rm_computing_ortho_comp.html">
\begin{equation*}
\begin{linsys}{3}
2x\amp +\amp y \amp \amp  \amp = \amp 0\\
x\amp \amp  \amp +\amp z \amp = \amp 0
\end{linsys}.
\end{equation*}
</div>
<p class="continuation">Solving this system using Gaussian elimination we conclude that</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/rm_computing_ortho_comp.html">
\begin{equation*}
W^\perp=\{(t,-2t,-t)\colon t\in \R\}=\Span\{(1,-2,-1)\}\text{,}
\end{equation*}
</div>
<p class="continuation">which we recognize as the line <span class="process-math">\(\ell\subseteq \R^3\)</span> passing through the origin with direction vector <span class="process-math">\((1,-2,-1)\text{.}\)</span> This is none other than the normal line to the plane <span class="process-math">\(W\)</span> passing through the origin.</p>
</div></div>
</div></article><article class="theorem theorem-like" id="th_orthogonal_complement"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.3.5</span><span class="period">.</span><span class="space"> </span><span class="title">Orthogonal complement.</span>
</h4>
<p id="p-2487">Let <span class="process-math">\((V,\langle \ , \rangle)\)</span> be an inner product vector space, and let <span class="process-math">\(W\subseteq V\)</span> be a subspace.</p>
<ol class="decimal">
<li id="li-768"><p id="p-2488">The orthogonal complement <span class="process-math">\(W^\perp\)</span> is a subspace of <span class="process-math">\(V\text{.}\)</span></p></li>
<li id="li-769"><p id="p-2489">We have <span class="process-math">\(W\cap W^\perp=\{\boldzero\}\text{.}\)</span></p></li>
<li id="li-770">
<p id="p-2490">If <span class="process-math">\(\dim V=n\lt \infty\)</span> then</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\dim W+\dim W^\perp=n\text{.}
\end{equation*}
</div>
</li>
</ol></article><article class="hiddenproof" id="proof-80"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-80"><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-80"><article class="hiddenproof"><p id="p-2491">The proof is left as an exercise. (See <a href="" class="xref" data-knowl="./knowl/ex_orthocomp_subspace.html" title="Exercise 4.3.6.9">Exercise 4.3.6.9–4.3.6.10</a>.)</p></article></div>
<article class="example example-like" id="example-84"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.3.6</span><span class="period">.</span>
</h4>
<p id="p-2492">Consider the inner product space <span class="process-math">\(\R^3\)</span> with the dot product. Let <span class="process-math">\(W=\Span\{(1,1,1)\}\subset \R^3\text{,}\)</span> the line passing through the origin with direction vector <span class="process-math">\((1,1,1)\text{.}\)</span> The orthogonal complement <span class="process-math">\(W^\perp\)</span> is the set of vectors orthogonal to <span class="process-math">\((1,1,1)\text{.}\)</span> Using the definition of dot product, this is the set of solutions <span class="process-math">\((x,y,z)\)</span> to the equation</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/th_orthogonal_complement.html">
\begin{equation*}
x+y+z=0\text{,}
\end{equation*}
</div>
<p class="continuation">which we recognize as the plane passing through the origin with normal vector <span class="process-math">\((1,1,1)\text{.}\)</span> Note that we have</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/th_orthogonal_complement.html">
\begin{equation*}
\dim W+\dim W^\perp=1+2=3,
\end{equation*}
</div>
<p class="continuation">as predicted in <a href="" class="xref" data-knowl="./knowl/th_orthogonal_complement.html" title="Theorem 4.3.5: Orthogonal complement">Theorem 4.3.5</a>.</p></article><p id="p-2493">The notion of orthogonal complement gives us a more conceptual way of understanding the relationship between the various fundamental spaces of a matrix.</p>
<article class="theorem theorem-like" id="th_row_null_comp"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.3.7</span><span class="period">.</span><span class="space"> </span><span class="title">Fundamental spaces and orthogonal complements.</span>
</h4>
<p id="p-2494">Let <span class="process-math">\(A\)</span> be <span class="process-math">\(m\times n\text{,}\)</span> and consider <span class="process-math">\(\R^n\)</span> and <span class="process-math">\(\R^m\)</span> as inner product spaces with respect to the dot product. Then:</p>
<ol class="decimal">
<li id="li-771"><p id="p-2495"><span class="process-math">\(\NS(A)=\left(\RS(A)\right)^\perp\text{,}\)</span> and thus <span class="process-math">\(\RS(A)=\left(\NS(A)\right)^\perp\text{.}\)</span></p></li>
<li id="li-772"><p id="p-2496"><span class="process-math">\(\NS(A^T)=\left(\CS(A)\right)^\perp\text{,}\)</span> and thus <span class="process-math">\(\CS(A)=\left(\NS(A^T)\right)^\perp\text{.}\)</span></p></li>
</ol></article><article class="hiddenproof" id="proof-81"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-81"><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-81"><article class="hiddenproof"><ol class="decimal">
<li id="li-773">
<p id="p-2497">Using the dot product method of matrix multiplication, we see that a vector <span class="process-math">\(\boldx\in\NS(A)\)</span> if and only if <span class="process-math">\(\boldx\cdot\boldr_i=0\)</span> for each row <span class="process-math">\(\boldr_i\)</span> of <span class="process-math">\(A\text{,}\)</span> if and only if <span class="process-math">\(\boldx\cdot \boldw=0\)</span> for all <span class="process-math">\(\boldw\in \Span \{\boldr_1, \boldr_2, \dots, \boldr_m\}=\RS A\)</span> (see <a href="" class="xref" data-knowl="./knowl/rm_computing_ortho_comp.html" title="Remark 4.3.2: Computing W^\perp">Remark 4.3.2</a>), if and only if <span class="process-math">\(\boldx\in (\RS A)^\perp\text{.}\)</span> This shows <span class="process-math">\(\NS A=(\RS A)^\perp\text{.}\)</span></p>
<p id="p-2498">We can use <a href="" class="xref" data-knowl="./knowl/cor_orthocomp_selfdual.html" title="Corollary 4.3.13">Corollary 4.3.13</a> to conclude <span class="process-math">\(\RS A=(\NS A)^\perp\text{.}\)</span> Alternatively, and more directly, the argument above shows that <span class="process-math">\(\boldw\in \RS A\implies \boldw\in (\NS A)^\perp\text{,}\)</span> proving <span class="process-math">\(\RS A\subseteq (\NS A)^\perp\text{.}\)</span> Next, by the rank-nullity theorem we have <span class="process-math">\(\dim \RS A=n-\dim\NS A\text{;}\)</span> and by <a href="" class="xref" data-knowl="./knowl/th_orthogonal_complement.html" title="Theorem 4.3.5: Orthogonal complement">Theorem 4.3.5</a> we have <span class="process-math">\(\dim (\NS A)^\perp=n-\dim\NS A\text{.}\)</span> It follows that <span class="process-math">\(\dim\RS A=\dim (\NS A)^\perp\text{.}\)</span> Since <span class="process-math">\(\RS A\subseteq (\NS A)^\perp\)</span> and <span class="process-math">\(\dim \RS A=\dim (\NS A)^\perp\text{,}\)</span> we conclude by <a href="" class="xref" data-knowl="./knowl/cor_dimension_subspace.html" title="Corollary 3.7.10: Dimension of subspaces">Corollary 3.7.10</a> that <span class="process-math">\(\RS A=(\NS A)^\perp\text{.}\)</span></p>
</li>
<li id="li-774"><p id="p-2499">This follows from (1) and the fact that <span class="process-math">\(\CS(A)=\RS(A^T)\text{.}\)</span></p></li>
</ol></article></div>
<article class="example example-like" id="example-85"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.3.8</span><span class="period">.</span>
</h4>
<p id="p-2500">Understanding the orthogonal relationship between <span class="process-math">\(\NS A \)</span> and <span class="process-math">\(\RS A\)</span> allows us in many cases to quickly determine/visualize the one from the other. As an example, consider <span class="process-math">\(A=\begin{bmatrix}1\amp -1\amp 1\\ 1\amp -1\amp -1 \end{bmatrix}\text{.}\)</span> Looking at the columns, we see easily that <span class="process-math">\(\rank A =2\text{,}\)</span> which implies that <span class="process-math">\(\nullity A=3-2=1\text{.}\)</span> Since <span class="process-math">\((1,-1,0)\)</span> is an element of <span class="process-math">\(\NS(A)\)</span> and <span class="process-math">\(\dim\NS A=1\text{,}\)</span> we must have <span class="process-math">\(\NS A=\Span\{(1,-1,0)\}\text{,}\)</span> a line. By orthogonality, we conclude that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\RS A=(\NS A)^\perp\text{,}
\end{equation*}
</div>
<p class="continuation">which is the plane with normal vector <span class="process-math">\((1,-1,0)\)</span> passing through the origin.</p></article></section><section class="subsection" id="subsection-67"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.3.2</span> <span class="title">Orthogonal Projection</span>
</h3>
<article class="theorem theorem-like" id="th_orthogonal_projection"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.3.9</span><span class="period">.</span><span class="space"> </span><span class="title">Orthogonal projection theorem.</span>
</h4>
<p id="p-2501">Let <span class="process-math">\((V,\langle \ , \rangle)\)</span> be an inner product space, and let <span class="process-math">\(W\subseteq V\)</span> be a finite-dimensional subspace.</p>
<ol class="decimal">
<li id="li-775">
<span class="heading"><span class="title">Orthogonal decomposition.</span></span><p id="p-2502">For all <span class="process-math">\(\boldv\in V\)</span> there are vectors <span class="process-math">\(\boldw\)</span> and <span class="process-math">\(\boldw^\perp\)</span> satisfying</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_ortho_decomp.html" id="eq_ortho_decomp">
\begin{equation}
\boldv=\boldw+\boldw^\perp, \boldw\in W, \boldw^\perp\in W^\perp\text{.}\tag{4.3.1}
\end{equation}
</div>
<p class="continuation">Furthermore, the pair <span class="process-math">\(\boldw, \boldw^\perp\)</span> is unique in the following sense: if we have <span class="process-math">\(\boldv=\boldu+\boldu^\perp\)</span> for some <span class="process-math">\(\boldu\in W\)</span> and <span class="process-math">\(\boldu^\perp\in W^\perp\text{,}\)</span> then <span class="process-math">\(\boldu=\boldw\)</span> and <span class="process-math">\(\boldu^\perp=\boldw^\perp\text{.}\)</span> Accordingly, the vector equation <a href="" class="xref" data-knowl="./knowl/eq_ortho_decomp.html" title="Equation 4.3.1">(4.3.1)</a> is called the <dfn class="terminology">orthogonal decomposition</dfn> of <span class="process-math">\(\boldv\)</span> with respect to <span class="process-math">\(W\text{;}\)</span> and the vector <span class="process-math">\(\boldw\)</span> is called the <dfn class="terminology">orthogonal projection</dfn> of <span class="process-math">\(\boldv\)</span> onto <span class="process-math">\(W\text{,}\)</span> denoted <span class="process-math">\(\boldw=\proj{\boldv}{W}\text{.}\)</span></p>
</li>
<li id="li-776">
<span class="heading"><span class="title">Orthogonal projection formula.</span></span><p id="p-2503">Choose any orthogonal basis <span class="process-math">\(B=\{\boldw_1, \boldw_2, \dots, \boldw_r\}\)</span> of <span class="process-math">\(W\text{.}\)</span> We have</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_ortho_proj_formula">
\begin{equation}
\proj{\boldv}{W}=\sum_{i=1}^r\frac{\angvec{\boldv,\boldw_i}}{\angvec{\boldw_i, \boldw_i}}\boldw_i\text{.}\tag{4.3.2}
\end{equation}
</div>
</li>
<li id="li-777">
<span class="heading"><span class="title">Distance to <span class="process-math">\(W\)</span>.</span></span><p id="p-2504">The orthogonal projection <span class="process-math">\(\boldw=\proj{\boldv}{W}\)</span> is the element of <span class="process-math">\(W\)</span> that is closest to <span class="process-math">\(\boldv\)</span> in the following sense: for all <span class="process-math">\(\boldw'\in W\)</span> we have</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
d(\boldv, \proj{\boldv}{W})\leq d(\boldv, \boldw')\text{,}
\end{equation*}
</div>
<p class="continuation">or equivalently,</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_ortho_proj_distance">
\begin{equation}
\norm{\boldv-\proj{\boldv}{W}}\leq\norm{\boldv-\boldw'}\text{.}\tag{4.3.3}
\end{equation}
</div>
<p class="continuation">Accordingly, we define the <dfn class="terminology">distance</dfn> <span class="process-math">\(d(\boldv, W)\)</span> between <span class="process-math">\(\boldv\)</span> and <span class="process-math">\(W\)</span> to be</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
d(\boldv, W)=d(\boldv, \proj{\boldv}{W})=\norm{\boldv-\proj{\boldv}{W}}\text{.}
\end{equation*}
</div>
</li>
</ol></article><article class="hiddenproof" id="proof-82"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-82"><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-82"><article class="hiddenproof"><p id="p-2505">Let <span class="process-math">\(B=\{\boldw_1, \boldw_2, \dots, \boldw_r\}\text{.}\)</span> We first show that the vectors</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_ortho_decomp.html ./knowl/eq_ortho_proj_formula_proof.html" id="eq_ortho_proj_formula_proof">
\begin{equation}
\boldw=\sum_{i=1}^r\frac{\angvec{\boldv,\boldw_i}}{\angvec{\boldw_i, \boldw_i}}\boldw_i\tag{4.3.4}
\end{equation}
</div>
<p class="continuation">and <span class="process-math">\(\boldw^\perp=\boldv-\boldw\)</span> satisfy the conditions in <a href="" class="xref" data-knowl="./knowl/eq_ortho_decomp.html" title="Equation 4.3.1">(4.3.1)</a>. It is clear that the <span class="process-math">\(\boldw\)</span> defined in <a href="" class="xref" data-knowl="./knowl/eq_ortho_proj_formula_proof.html" title="Equation 4.3.4">(4.3.4)</a> is an element of <span class="process-math">\(W\text{,}\)</span> since it is a linear combination of the <span class="process-math">\(\boldw_i\text{.}\)</span> Furthermore, we see easily that our choice <span class="process-math">\(\boldw^\perp=\boldv-\boldw\)</span> satisfies</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_ortho_decomp.html ./knowl/eq_ortho_proj_formula_proof.html">
\begin{equation*}
\boldw+\boldw^\perp=\boldw+(\boldv-\boldw)=\boldv\text{.}
\end{equation*}
</div>
<p class="continuation">It remains only to show that <span class="process-math">\(\boldw^\perp=\boldv-\boldw\in W^\perp\text{.}\)</span> Since <span class="process-math">\(B\)</span> is a basis of <span class="process-math">\(W\text{,}\)</span> it suffices to show that <span class="process-math">\(\langle \boldw^\perp,\boldw_j\rangle=0\)</span> for all <span class="process-math">\(1\leq i\leq r\text{.}\)</span> We compute:</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_ortho_decomp.html ./knowl/eq_ortho_proj_formula_proof.html" id="md-194">
\begin{align*}
\langle \boldw^\perp, \boldw_j \rangle\amp = \langle \boldv-\proj{\boldv}{W}, \boldw_i \rangle\\
\amp =
\left\langle \boldv-\sum_{i=1}^r\frac{\angvec{\boldv,\boldw_i}}{\angvec{\boldw_i, \boldw_i}}\boldw_i, \boldw_j\right\rangle \\
\amp =
\langle \boldv, \boldw_j\rangle -\sum_{i=1}^r\frac{\angvec{\boldv,\boldw_i}}{\angvec{\boldw_i, \boldw_i}}\langle \boldw_i, \boldw_j\rangle \\
\amp =
\langle \boldv, \boldw_j\rangle -\frac{\langle \boldv, \boldw_j\rangle}{\cancel{\langle \boldw_j, \boldw_j\rangle}}\cancel{\langle \boldw_j, \boldw_j\rangle}\\
\amp = 0 \text{,}
\end{align*}
</div>
<p class="continuation">as desired.</p>
<p id="p-2506">Having shown that a decomposition of <span class="process-math">\(\boldv\)</span> of the form <a href="" class="xref" data-knowl="./knowl/eq_ortho_decomp.html" title="Equation 4.3.1">(4.3.1)</a> exists, we now show it is unique in the sense specified. Suppose we have</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_ortho_decomp.html ./knowl/th_orthogonal_complement.html">
\begin{equation*}
\boldv=\boldw+\boldw^\perp=\boldu+\boldu^\perp\text{,}
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(\boldw, \boldu\in W\)</span> and <span class="process-math">\(\boldw^\perp, \boldu^\perp\in W^\perp\text{.}\)</span> Rearranging, we see that</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_ortho_decomp.html ./knowl/th_orthogonal_complement.html">
\begin{equation*}
\boldw-\boldu=\boldu^\perp-\boldw^\perp\text{.}
\end{equation*}
</div>
<p class="continuation">We now claim that <span class="process-math">\(\boldw-\boldu=\boldu^\perp-\boldw^\perp=\boldzero\text{,}\)</span> in which case <span class="process-math">\(\boldw=\boldu\)</span> and <span class="process-math">\(\boldw^\perp=\boldu^\perp\text{,}\)</span> as desired. To see why the claim is true, consider the vector <span class="process-math">\(\boldv'=\boldw-\boldu=\boldu^\perp-\boldw^\perp\text{.}\)</span> Since <span class="process-math">\(\boldv'=\boldw-\boldu\text{,}\)</span> and <span class="process-math">\(\boldw, \boldu\in W\text{,}\)</span> we have <span class="process-math">\(\boldv'\in W\text{.}\)</span> On the other hand, since <span class="process-math">\(\boldv'=\boldu^\perp-\boldw^\perp\text{,}\)</span> and <span class="process-math">\(\boldu^\perp, \boldw^\perp\in W^\perp\text{,}\)</span> we have <span class="process-math">\(\boldv'\in W^\perp\text{.}\)</span> Thus <span class="process-math">\(\boldv'\in W\cap W^\perp\text{.}\)</span> Since <span class="process-math">\(W\cap W^\perp=\{\boldzero\}\)</span> (<a href="" class="xref" data-knowl="./knowl/th_orthogonal_complement.html" title="Theorem 4.3.5: Orthogonal complement">Theorem 4.3.5</a>), we conclude <span class="process-math">\(\boldv'=\boldw-\boldu=\boldu^\perp-\boldw^\perp=\boldzero\text{,}\)</span> as claimed.</p>
<p id="p-2507">At this point we have proved both (1) and (2), and it remains only to show that <a href="" class="xref" data-knowl="./knowl/eq_ortho_proj_distance.html" title="Equation 4.3.3">(4.3.3)</a> holds for all <span class="process-math">\(\boldw'\in W\text{.}\)</span> To this end we compute:</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_ortho_proj_distance.html ./knowl/ex_ortho_pythag.html" id="md-195">
\begin{align*}
\norm{\boldv-\boldw'}^2\amp = \norm{\boldw^\perp+(\boldw-\boldw')}^2\\
\amp =\norm{\boldw^\perp}^2+\norm{\boldw-\boldw'}^2 \amp (\knowl{./knowl/ex_ortho_pythag.html}{\text{Exercise 4.2.3.8}})\\
\amp \geq \norm{\boldw^\perp}^2\\
\amp =\norm{\boldv-\boldw}^2\text{.}
\end{align*}
</div>
<p class="continuation">This shows <span class="process-math">\(\norm{\boldv-\boldw'}^2\geq \norm{\boldv-\boldw}^2\text{.}\)</span> Taking square-roots now proves the desired inequality.</p></article></div>
<article class="remark remark-like" id="rm_ortho_proj_formula"><h4 class="heading">
<span class="type">Remark</span><span class="space"> </span><span class="codenumber">4.3.10</span><span class="period">.</span><span class="space"> </span><span class="title">Orthogonal projection formula.</span>
</h4>
<p id="p-2508">The formula <a href="" class="xref" data-knowl="./knowl/eq_ortho_proj_formula.html" title="Equation 4.3.2">(4.3.2)</a> is very convenient for computing an orthogonal projection <span class="process-math">\(\proj{\boldv}{W}\text{,}\)</span> but mark well this important detail: to apply the formula we must first provide an <em class="emphasis">orthogonal</em> basis of <span class="process-math">\(W\text{.}\)</span> Thus unless one is provided, our first step in an orthogonal projection computation is to produce an orthogonal basis of <span class="process-math">\(W\text{.}\)</span> In some simple cases (e.g., when <span class="process-math">\(W\)</span> is 1- or 2-dimensional) this can be done by inspection. Otherwise, we use the Gram-Schmidt procedure.</p></article><article class="example example-like" id="example-86"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.3.11</span><span class="period">.</span>
</h4>
<p id="p-2509">Consider the inner product space <span class="process-math">\(\R^3\)</span> with the dot product. Let <span class="process-math">\(W\subseteq\R^3\)</span> be the plane with equation <span class="process-math">\(x+y+z=0\text{.}\)</span> Compute <span class="process-math">\(\proj{\boldv}{W}\)</span> for each <span class="process-math">\(\boldv\)</span> below.</p>
<ol class="decimal">
<li id="li-778"><p id="p-2510"><span class="process-math">\(\displaystyle \boldv=(3,-2,2)\)</span></p></li>
<li id="li-779"><p id="p-2511"><span class="process-math">\(\displaystyle \boldv=(2,1,-3)\)</span></p></li>
<li id="li-780"><p id="p-2512"><span class="process-math">\(\displaystyle \boldv=(-7,-7,-7)\)</span></p></li>
</ol>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-80" id="solution-80"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-80"><div class="solution solution-like">
<p id="p-2513">According to <a href="" class="xref" data-knowl="./knowl/rm_ortho_proj_formula.html" title="Remark 4.3.10: Orthogonal projection formula">Remark 4.3.10</a> our first step is to produce an orthogonal basis of <span class="process-math">\(W\text{.}\)</span> We do so by inspection. Since <span class="process-math">\(\dim W=2\text{,}\)</span> we simply need to find two solutions to <span class="process-math">\(x+y+z=0\)</span> that are orthogonal to one another: e.g., <span class="process-math">\(\boldw_1=(1,-1,0)\)</span> and <span class="process-math">\(\boldw_2=(1,1,-2)\text{.}\)</span> Thus we choose <span class="process-math">\(B=\{ (1,-1,0), (1,1,-2)\}\)</span> as our orthogonal basis, and our computations become a matter of applying <a href="" class="xref" data-knowl="./knowl/eq_ortho_proj_formula.html" title="Equation 4.3.2">(4.3.2)</a>, which in this case becomes</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/rm_ortho_proj_formula.html ./knowl/eq_ortho_proj_formula.html ./knowl/ex_orthoproj_props.html">
\begin{equation*}
\proj{\boldv}{W}=\frac{\boldv\cdot\boldw_1}{\boldw_1\cdot \boldw_1}\boldw_1+\frac{\boldv\cdot\boldw_2}{\boldw_2\cdot \boldw_2}\boldw_2=
\frac{\boldv\cdot\boldw_1}{2}\boldw_1+\frac{\boldv\cdot\boldw_2}{6}\boldw_2\text{.}
\end{equation*}
</div>
<p class="continuation">Now compute:</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/rm_ortho_proj_formula.html ./knowl/eq_ortho_proj_formula.html ./knowl/ex_orthoproj_props.html" id="md-196">
\begin{align*}
\proj{(3,-2,2)}{W} \amp =\frac{5}{2}(1,-1,0)+\frac{-3}{6}(1,1,-2)=(2,-3,1)\\
\proj{(2,1,-3)}{W} \amp =\frac{1}{2}(1,-1,0)+\frac{9}{6}(1,1,-2)=(2,1,-3)\\
\proj{(-7,-7,-7)}{W} \amp =\frac{0}{2}(1,-1,0)+\frac{0}{6}(1,1,-2)=(0,0,0)\text{.}
\end{align*}
</div>
<p class="continuation">The last two computations might give you pause. Why do we have <span class="process-math">\(\proj{(2,1,-3)}{W}=(2,1,-3)\)</span> and <span class="process-math">\(\proj{(-7,7-7,-7)}{W}=(0,0,0)\text{?}\)</span> The answer is that <span class="process-math">\((2,1,-3)\)</span> is already an element of <span class="process-math">\(W\text{,}\)</span> so it stands to reason that its projection is itself; and <span class="process-math">\((-7,-7,-7)\)</span> is already orthogonal to <span class="process-math">\(W\)</span> (it is a scalar multiple of <span class="process-math">\((1,1,1)\)</span>), so it stands to reason that its projection is equal to <span class="process-math">\(\boldzero\text{.}\)</span> See <a href="" class="xref" data-knowl="./knowl/ex_orthoproj_props.html" title="Exercise 4.3.6.12">Exercise 4.3.6.12</a> for a rigorous proof of these claims.</p>
</div></div>
</div></article><section class="paragraphs" id="ss_vid_eg_orthoproj_functions"><h4 class="heading"><span class="title">Video example: orthogonal projection in function space.</span></h4>
<figure class="figure figure-like" id="fig_vid_orthoproj_functions"><div class="video-box" style="width: 100%;padding-top: 56.25%; margin-left: 0%; margin-right: 0%;"><iframe id="vid_orthoproj_functions" class="video" allowfullscreen="" src="https://www.youtube-nocookie.com/embed/xWuzNdExSEk?&amp;modestbranding=1&amp;rel=0"></iframe></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">4.3.12<span class="period">.</span></span><span class="space"> </span>Video: orthogonal projection in function space</figcaption></figure></section><article class="corollary theorem-like" id="cor_orthocomp_selfdual"><h4 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">4.3.13</span><span class="period">.</span>
</h4>
<p id="p-2514">Let <span class="process-math">\((V,\angvec{\ , \ })\)</span> be an inner product space, and let <span class="process-math">\(W\subseteq V\)</span> be a finite-dimensional subspace. Then <span class="process-math">\((W^\perp)^\perp=W\text{.}\)</span></p></article><article class="hiddenproof" id="proof-83"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-83"><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-83"><article class="hiddenproof"><p id="p-2515">Clearly <span class="process-math">\(W\subseteq (W^\perp)^\perp\text{.}\)</span> For the other direction, take <span class="process-math">\(\boldv\in (W^\perp)^\perp\text{.}\)</span> Using the <em class="emphasis">orthogonal projection theorem</em>, we can write <span class="process-math">\(\boldv=\boldw+\boldw^\perp\)</span> with <span class="process-math">\(\boldw\in W\)</span> and <span class="process-math">\(\boldw^\perp\in W^\perp\text{.}\)</span> We will show <span class="process-math">\(\boldw^\perp=\boldzero\text{.}\)</span></p>
<p id="p-2516">Since <span class="process-math">\(\boldv\in (W^\perp)^\perp\)</span> we have <span class="process-math">\(\angvec{\boldv,\boldw^\perp}=0\text{.}\)</span> Then we have</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-197">
\begin{align*}
0\amp =\angvec{\boldv,\boldw^\perp}\\
\amp =\angvec{\boldw+\boldw^\perp,\boldw^\perp}\\
\amp =\angvec{\boldw,\boldw^\perp}+\angvec{\boldw^\perp,\boldw^\perp} \amp \text{ (since \(W\perp W^\perp\)) }\\
\amp =0+\angvec{\boldw^\perp,\boldw^\perp}
\end{align*}
</div>
<p id="p-2517">Thus <span class="process-math">\(\angvec{\boldw^\perp,\boldw^\perp}=0\text{.}\)</span> It follows that <span class="process-math">\(\boldw^\perp=\boldzero\text{,}\)</span> and hence <span class="process-math">\(\boldv=\boldw+\boldzero=\boldw\in W\text{.}\)</span></p></article></div>
<article class="corollary theorem-like" id="cor_orthoproj_linear"><h4 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">4.3.14</span><span class="period">.</span><span class="space"> </span><span class="title">Orthgonal projection is linear.</span>
</h4>
<p id="p-2518">Let <span class="process-math">\((V,\angvec{\ , \ })\)</span> be an inner product space, and let <span class="process-math">\(W\subseteq V\)</span> be a finite-dimensional subspace.</p>
<ol class="decimal">
<li id="li-781">
<p id="p-2519">The function</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-198">
\begin{align*}
\operatorname{proj}_W\colon V \amp\rightarrow V\\
\boldv \amp\mapsto \proj{\boldv}{W} 
\end{align*}
</div>
<p class="continuation">is a linear transformation.</p>
</li>
<li id="li-782"><p id="p-2520">We have <span class="process-math">\(\im \operatorname{proj}_W=W\)</span> and <span class="process-math">\(\NS \operatorname{proj}_W=W^\perp\text{.}\)</span></p></li>
</ol></article><article class="hiddenproof" id="proof-84"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-84"><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-84"><article class="hiddenproof"><ol class="decimal">
<li id="li-783">
<p id="p-2521">We must show that <span class="process-math">\(\proj{c\boldv+d\boldw}{W}=c\,\proj{\boldv}{W}+d\,\proj{\boldw}{W}\)</span> for all <span class="process-math">\(c,d\in\R\)</span> and <span class="process-math">\(\boldv,\boldw\in V\text{.}\)</span> We pick an orthogonal basis <span class="process-math">\(B=\{\boldv_1,\boldv_2, \dots, \boldv_r\}\)</span> of <span class="process-math">\(W\)</span> and compute, using formula <a href="" class="xref" data-knowl="./knowl/eq_ortho_proj_formula.html" title="Equation 4.3.2">(4.3.2)</a>:</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_ortho_proj_formula.html" id="md-199">
\begin{align*}
\proj{c\boldv+d\boldw}{W} \amp=\sum_{i=1}^{r}\frac{\langle c\boldv+d\boldw, \boldv_i\rangle}{\langle\boldv_i, \boldv_i\rangle }\boldv_i \\
\amp=\sum_{i=1}^r\frac{c\langle \boldv,\boldv_i\rangle+d\langle \boldw, \boldv_i\rangle}{\langle \boldv_i, \boldv_i\rangle}\boldv_i \\
\amp =c\sum_{i=1}^r\frac{\angvec{\boldv, \boldv_i}}{\angvec{\boldv_i, \boldv_i}}\boldv_i+d\sum_{i=1}^r\frac{\angvec{\boldw, \boldv_i}}{\angvec{\boldv_i, \boldv_i}}\boldv_i\\
\amp = c\,\proj{\boldv}{W}+d\,\proj{\boldw}{W}\text{.}
\end{align*}
</div>
</li>
<li id="li-784">
<p id="p-2522">By definition we have <span class="process-math">\(\proj{\boldv}{W}\in W\)</span> for all <span class="process-math">\(\boldv\in W\text{,}\)</span> and thus <span class="process-math">\(\im \operatorname{proj}_W \subseteq W\text{.}\)</span> For the other direction, if <span class="process-math">\(\boldw\in W\text{,}\)</span> then <span class="process-math">\(\boldw=\proj{\boldw}{W}\)</span> (<a href="" class="xref" data-knowl="./knowl/ex_orthoproj_props.html" title="Exercise 4.3.6.12">Exercise 4.3.6.12</a>), and thus <span class="process-math">\(\boldw\in \im\operatorname{proj}\text{.}\)</span> This proves <span class="process-math">\(\im\operatorname{proj}=W\text{.}\)</span></p>
<p id="p-2523">The fact that <span class="process-math">\(\NS\operatorname{proj}=W^\perp\)</span> follows from the equivalence stated in (b) of <a href="" class="xref" data-knowl="./knowl/ex_orthoproj_props.html" title="Exercise 4.3.6.12">Exercise 4.3.6.12</a>.</p>
</li>
</ol></article></div></section><section class="subsection" id="subsection-68"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.3.3</span> <span class="title">Orthogonal projection in <span class="process-math">\(\R^2\)</span> and <span class="process-math">\(\R^3\)</span></span>
</h3>
<section class="introduction" id="introduction-65"><p id="p-2524">For this subsection we will always work within Euclidean space: i.e., <span class="process-math">\(V=\R^n\)</span> with the dot product. In applications we often want to compute the projection of a point onto a line (in <span class="process-math">\(\R^2\)</span> or <span class="process-math">\(\R^3\)</span>) or plane (in <span class="process-math">\(\R^3\)</span>). According to <a href="" class="xref" data-knowl="./knowl/cor_orthoproj_linear.html" title="Corollary 4.3.14: Orthgonal projection is linear">Corollary 4.3.14</a> the operation of projecting onto any subspace <span class="process-math">\(W\subseteq \R^n\)</span> is in fact a linear transformation <span class="process-math">\(\operatorname{proj}_W\colon \R^n\rightarrow \R^n\text{.}\)</span> By <a href="" class="xref" data-knowl="./knowl/cor_matrix_transformations.html" title="Corollary 3.6.16: Matrix transformations">Corollary 3.6.16</a> we have <span class="process-math">\(\operatorname{proj}_W=T_A\text{,}\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/cor_orthoproj_linear.html ./knowl/cor_matrix_transformations.html ./knowl/eq_ortho_proj_formula.html">
\begin{equation*}
A=\begin{bmatrix}
\vert \amp \vert \amp \amp \vert \\
\proj{\bolde_1}{W}\amp \proj{\bolde_2}{W}\amp \cdots \amp \proj{\bolde_n}{W} \\
\vert \amp \vert \amp \amp \vert
\end{bmatrix}\text{.}
\end{equation*}
</div>
<p class="continuation">Lastly, <a href="" class="xref" data-knowl="./knowl/eq_ortho_proj_formula.html" title="Equation 4.3.2">(4.3.2)</a> gives us an easy formula for computing <span class="process-math">\(\proj{\bolde_j}{W}\)</span> for all <span class="process-math">\(j\text{,}\)</span> once we have selected an orthogonal basis for <span class="process-math">\(W\text{.}\)</span> As a result we can easily derive matrix formulas for projection onto any subspace <span class="process-math">\(W\)</span> of any Euclidean space <span class="process-math">\(\R^n\text{.}\)</span> We illustrate this with some examples in <span class="process-math">\(\R^2\)</span> and <span class="process-math">\(\R^3\)</span> below.</p></section><article class="example example-like" id="eg_projection_line"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.3.15</span><span class="period">.</span><span class="space"> </span><span class="title">Projection onto a line <span class="process-math">\(\ell\subseteq \R^3\)</span>.</span>
</h4>
<p id="p-2525">Any line in <span class="process-math">\(\R^3\)</span> passing through the origin can be described as <span class="process-math">\(\ell=\Span\{\boldv\}\text{,}\)</span> for some <span class="process-math">\(\boldv=(a,b,c)\ne 0\text{.}\)</span> The set <span class="process-math">\(\{(a,b,c)\}\)</span> is trivially an orthogonal basis of <span class="process-math">\(\ell\text{.}\)</span> Using <a href="" class="xref" data-knowl="./knowl/eq_ortho_proj_formula.html" title="Equation 4.3.2">(4.3.2)</a>, we have</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_ortho_proj_formula.html">
\begin{equation*}
\proj{\boldx}{\ell}=\frac{\boldx\cdot \boldv}{\boldv\cdot\boldv}\boldv=\frac{ax+by+cz}{a^2+b^2+c^2}(a,b,c)\text{.}
\end{equation*}
</div>
<p class="continuation">It follows that <span class="process-math">\(\operatorname{proj}_\ell=T_A\text{,}\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_ortho_proj_formula.html">
\begin{equation*}
A=\begin{bmatrix}
\vert \amp \vert \amp \vert \\
\proj{(1,0,0)}{\ell}\amp \proj{(0,1,0)}{\ell}\amp \proj{(0,0,1)}{\ell} \\
\vert \amp \vert \amp \vert
\end{bmatrix}=\frac{1}{a^2+b^2+c}\begin{bmatrix}
a^2\amp ab\amp ac\\
ab\amp b^2\amp bc\\
ac\amp bc\amp c^2
\end{bmatrix}\text{.}
\end{equation*}
</div></article><article class="example example-like" id="example-88"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.3.16</span><span class="period">.</span>
</h4>
<p id="p-2526">Consider the line <span class="process-math">\(\ell=\Span\{(1,2,1)\}\subseteq \R^3\text{.}\)</span></p>
<ol class="decimal">
<li id="li-785"><p id="p-2527">Find the matrix <span class="process-math">\(A\)</span> such that <span class="process-math">\(\operatorname{proj}_\ell=T_A\text{.}\)</span></p></li>
<li id="li-786"><p id="p-2528">Use your matrix formula from (a) to compute <span class="process-math">\(\proj{(-2,3,1)}{\ell}\text{,}\)</span> <span class="process-math">\(\proj{(-2,-4,-2)}{\ell}\text{,}\)</span> and  <span class="process-math">\(\proj{(1,-1,1)}{\ell}\text{.}\)</span></p></li>
<li id="li-787"><p id="p-2529">Compute <span class="process-math">\(d((-2,3,1), \ell)\)</span> and <span class="process-math">\(d((-2,-4,-2), \ell)\text{.}\)</span></p></li>
</ol>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-81" id="solution-81"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-81"><div class="solution solution-like"><ol class="decimal">
<li id="li-788">
<p id="p-2530">Using the general formula described in <a href="" class="xref" data-knowl="./knowl/eg_projection_line.html" title="Example 4.3.15: Projection onto a line \ell\subseteq \R^3">Example 4.3.15</a>, we have</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eg_projection_line.html">
\begin{equation*}
A=\frac{1}{6}\begin{bmatrix}
1\amp 2\amp 1\\ 2\amp 4\amp 2\\ 1\amp 2\amp 1
\end{bmatrix}\text{.}
\end{equation*}
</div>
</li>
<li id="li-789">
<p id="p-2531">Now compute</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/ex_orthoproj_props.html" id="md-200">
\begin{align*}
\proj{(-2,3,1)}{\ell}\amp=  \frac{1}{6}\begin{bmatrix}
1\amp 2\amp 1\\ 2\amp 4\amp 2\\ 1\amp 2\amp 1
\end{bmatrix}
\begin{amatrix}[r]
-2 \\ 3\\ 1
\end{amatrix}=
\frac{1}{6}\begin{amatrix}[r]
5\\ 10\\ 5
\end{amatrix}\\
\proj{(-2,-4,-2)}{\ell}\amp=  \frac{1}{6}\begin{bmatrix}
1\amp 2\amp 1\\ 2\amp 4\amp 2\\ 1\amp 2\amp 1
\end{bmatrix}
\begin{amatrix}[r]
-2 \\ -4\\ -2
\end{amatrix}=
\frac{1}{6}\begin{amatrix}[r]
-2\\ -4\\ -2
\end{amatrix}\\
\proj{(1,-1,1)}{\ell}\amp=  \frac{1}{6}\begin{bmatrix}
1\amp 2\amp 1\\ 2\amp 4\amp 2\\ 1\amp 2\amp 1
\end{bmatrix}
\begin{amatrix}[r]
1\\ -1\\ 1
\end{amatrix}=
\frac{1}{6}\begin{amatrix}[r]
0\\ 0\\ 0
\end{amatrix}\text{.}
\end{align*}
</div>
<p class="continuation">The last two computations, <span class="process-math">\(\proj{(-2,-4,-2)}{\ell}=(-2,-4,-2)\)</span> and <span class="process-math">\(\proj{(1,-1,1)}{\ell}=(0,0,0)\text{,}\)</span> should come as no surprise, since <span class="process-math">\((-2,-4,-2)\in \ell\)</span> and <span class="process-math">\((1,-1,1)\in \ell^\perp\text{.}\)</span> (See <a href="" class="xref" data-knowl="./knowl/ex_orthoproj_props.html" title="Exercise 4.3.6.12">Exercise 4.3.6.12</a>.)</p>
</li>
<li id="li-790">
<p id="p-2532">We have</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-201">
\begin{align*}
d((-2,3,1), \ell) \amp=\norm{(-2,3,1)-\proj{(-2,3,1)}{\ell}} \\
\amp = \norm{\frac{1}{6}(-17,8,1)}=\frac{\sqrt{354}}{6} \\
d((-2,-4,-2), \ell) \amp=\norm{(-2,-4,-2)-\proj{(-2,-4,-2)}{\ell}} \\
\amp = \norm{(0,0,0)}=0 \text{.}
\end{align*}
</div>
<p class="continuation">Again, the second computation should come as no surprise.  Since <span class="process-math">\((-2,-4,-2)\)</span> is itself an element of <span class="process-math">\(\ell\text{,}\)</span> it stands to reason that its distance to <span class="process-math">\(\ell\)</span> is equal to zero.</p>
</li>
</ol></div></div>
</div></article><article class="example example-like" id="eg_projection_plane"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.3.17</span><span class="period">.</span><span class="space"> </span><span class="title">Projection onto planes in <span class="process-math">\(\R^3\)</span>.</span>
</h4>
<p id="p-2533">Any plane <span class="process-math">\(W\subseteq\R^3\)</span> passing through the origin can be described as <span class="process-math">\(W=\{(x,y,z)\in \R^3\colon ax+by+cz=0\}\text{.}\)</span> Equivalently, <span class="process-math">\(W\)</span> is the set of all <span class="process-math">\(\boldx\in \R^3\)</span> satisfying <span class="process-math">\(\boldx\cdot (a,b,c)=0\text{:}\)</span> i.e., <span class="process-math">\(W=\ell^\perp\text{,}\)</span> where <span class="process-math">\(\ell=\Span\{(a,b,c)\text{.}\)</span> Consider the orthogonal decomposition with respect to <span class="process-math">\(\ell\text{:}\)</span></p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eg_projection_line.html">
\begin{equation*}
\boldx=\proj{\boldx}{\ell}+(\boldx-\proj{\boldx}{\ell})\text{.}
\end{equation*}
</div>
<p class="continuation">Since <span class="process-math">\(\boldx-\proj{\boldx}{\ell}\in \ell^\perp=W\)</span> and <span class="process-math">\(\proj{\boldx}{\ell}\in \ell=W^\perp\text{,}\)</span> we see that this is also an orthogonal decomposition with respect to <span class="process-math">\(W\text{!}\)</span> Using the matrix formula for <span class="process-math">\(\operatorname{proj}_\ell\)</span> from <a href="" class="xref" data-knowl="./knowl/eg_projection_line.html" title="Example 4.3.15: Projection onto a line \ell\subseteq \R^3">Example 4.3.15</a>, we have</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eg_projection_line.html" id="md-202">
\begin{align*}
\proj{\boldx}{W} \amp =\boldx-\proj{\boldx}{\ell} \\
\amp = I\boldx -  A\boldx\amp \left(A=\frac{1}{a^2+b^2+c}\begin{bmatrix}
a^2\amp ab\amp ac\\
ab\amp b^2\amp bc\\
ac\amp bc\amp c^2
\end{bmatrix}\right)\\
\amp = (I-A)\boldx\\
\amp = \frac{1}{a^2+b^2+c^2}\begin{bmatrix}b^2+c^2\amp -ab\amp -ac\\ -ab\amp a^2+c^2\amp -bc\\ -ac\amp -bc\amp a^2+b^2 \end{bmatrix}\text{.}
\end{align*}
</div>
<p class="continuation">We conclude that <span class="process-math">\(\operatorname{proj}_W=T_B\text{,}\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eg_projection_line.html">
\begin{equation*}
B=\frac{1}{a^2+b^2+c^2}\begin{bmatrix}b^2+c^2\amp -ab\amp -ac\\ -ab\amp a^2+c^2\amp -bc\\ -ac\amp -bc\amp a^2+b^2 \end{bmatrix}\text{.}
\end{equation*}
</div></article><article class="example example-like" id="example-90"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.3.18</span><span class="period">.</span>
</h4>
<p id="p-2534">Consider the plane <span class="process-math">\(W=\{(x,y,z)\in\R^3\colon x-2y+z=0-\}\text{.}\)</span></p>
<ol class="decimal">
<li id="li-791"><p id="p-2535">Find the matrix <span class="process-math">\(A\)</span> such that <span class="process-math">\(\operatorname{proj}_W=T_A\text{.}\)</span></p></li>
<li id="li-792"><p id="p-2536">Use your matrix formula from (a) to compute <span class="process-math">\(\proj{(2,1,1)}{W}\)</span> and <span class="process-math">\(\proj{(1,1,1)}{W}\text{.}\)</span></p></li>
<li id="li-793"><p id="p-2537">Compute <span class="process-math">\(d((2,1,1),W)\)</span> and <span class="process-math">\(d((1,1,1), W)\text{.}\)</span></p></li>
</ol>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-82" id="solution-82"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-82"><div class="solution solution-like"><ol class="decimal">
<li id="li-794">
<p id="p-2538">Using the general formula described in <a href="" class="xref" data-knowl="./knowl/eg_projection_plane.html" title="Example 4.3.17: Projection onto planes in \R^3">Example 4.3.17</a>, we have</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eg_projection_plane.html">
\begin{equation*}
A=\frac{1}{6}\begin{amatrix}[rrr]
5\amp 2\amp -1\\ 2\amp 2\amp 2\\ -1\amp 2\amp 5
\end{amatrix}\text{.}
\end{equation*}
</div>
</li>
<li id="li-795">
<p id="p-2539">Now compute</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-203">
\begin{align*}
\proj{(2,1,1)}{\ell}\amp= \frac{1}{6}\begin{amatrix}[rrr]
5\amp 2\amp -1\\ 2\amp 2\amp 2\\ -1\amp 2\amp 5
\end{amatrix}
\begin{amatrix}[r]
2 \\ 1\\ 1
\end{amatrix}=
\frac{1}{6}\begin{amatrix}[r]
11\\ 8\\ 5
\end{amatrix}\\
\proj{(1,1,1)}{\ell}\amp= \frac{1}{6}\begin{amatrix}[rrr]
5\amp 2\amp -1\\ 2\amp 2\amp 2\\ -1\amp 2\amp 5
\end{amatrix}
\begin{amatrix}[r]
1 \\ 1\\ 1
\end{amatrix}=
\frac{1}{6}\begin{amatrix}[r]
0\\ 0\\ 0
\end{amatrix}\text{.}
\end{align*}
</div>
</li>
<li id="li-796">
<p id="p-2540">We have</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-204">
\begin{align*}
d((2,1,1), W) \amp = \norm{(2,1,1)-\proj{(2,1,1)}{W}}\\
\amp =\norm{\frac{1}{6}(1,-2,-1)}=\frac{6}{6} \\
d((1,1,1), W) \amp = \norm{(1,1,1)-\proj{(1,1,1)}{W}}\\
\amp =\norm{(0,0,0)}=0 \text{.}
\end{align*}
</div>
</li>
</ol></div></div>
</div></article></section><section class="subsection" id="subsection-69"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.3.4</span> <span class="title">Trigonometric polynomial approximation</span>
</h3>
<p id="p-2541">Consider the inner product space consisting of  <span class="process-math">\(C([0,2\pi])\)</span> along with the integral inner product <span class="process-math">\(\langle f, g\rangle=\int_0^{2\pi}f(x)g(x) \, dx\text{.}\)</span> In <a href="" class="xref" data-knowl="./knowl/eg_orthogonal_functions.html" title="Example 4.2.4">Example 4.2.4</a> we saw that the set</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eg_orthogonal_functions.html">
\begin{equation*}
B=\{1, \cos(x),\sin(x),\cos(2x),\sin(2x), \dots , \cos(nx),\sin(nx)\}
\end{equation*}
</div>
<p class="continuation">is orthogonal with respect to this inner product. Thus <span class="process-math">\(B\)</span> is an orthogonal basis of</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eg_orthogonal_functions.html">
\begin{equation*}
\Span B=\{g\in C([0,2\pi])\colon g(x)=a_0+\sum_{k=1}^na_k\cos kx +b_k\sin kx \text{ for some } a_i, b_i\in \R\}\text{.}
\end{equation*}
</div>
<p class="continuation">We call <span class="process-math">\(W=\Span B\)</span> the space of <dfn class="terminology">trigonometric polynomials of degree at most <span class="process-math">\(n\)</span></dfn>.</p>
<p id="p-2542">Since <span class="process-math">\(B\)</span> is an orthogonal basis of <span class="process-math">\(W\text{,}\)</span> given an arbitrary function <span class="process-math">\(f(x)\in C[0,2\pi]\text{,}\)</span> its orthogonal projection <span class="process-math">\(\hat{f}=\proj{f}{W}\)</span> is given by</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_ortho_proj_formula.html ./knowl/eg_orthogonal_functions.html">
\begin{equation*}
\hat{f}(x)=a_0+a_1\cos(x)+b_1\sin(x)+a_2\cos(2x)+b_2\sin(2x)+\cdots +a_n\cos(nx)+b_n\sin(nx)\text{,}
\end{equation*}
</div>
<p class="continuation">where</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_ortho_proj_formula.html ./knowl/eg_orthogonal_functions.html">
\begin{equation*}
a_0=\frac{1}{2\pi}\int_0^{2\pi} f(x) \ dx, \ a_j=\frac{1}{\pi}\int_0^{2\pi}f(x)\cos(jx)\, dx, \ b_k=\frac{1}{\pi}\int_0^{2\pi}f(x)\sin(kx)\, dx\text{.}
\end{equation*}
</div>
<p class="continuation">Here we are using <a href="" class="xref" data-knowl="./knowl/eq_ortho_proj_formula.html" title="Equation 4.3.2">(4.3.2)</a>, as well as the inner product formulas <span class="process-math">\(\angvec{1,1}=2\pi\)</span> and <span class="process-math">\(\angvec{\cos n x, \cos n x}=\angvec{\sin n x, \sin n x}=\pi\)</span> from <a href="" class="xref" data-knowl="./knowl/eg_orthogonal_functions.html" title="Example 4.2.4">Example 4.2.4</a>.</p>
<p id="p-2543">What is the relationship between <span class="process-math">\(f\)</span> and <span class="process-math">\(\hat{f}\text{?}\)</span> <a href="" class="xref" data-knowl="./knowl/th_orthogonal_projection.html" title="Theorem 4.3.9: Orthogonal projection theorem">Theorem 4.3.9</a> tells us that <span class="process-math">\(\hat{f}\)</span> is the “best” trigonometric polynomial approximation of <span class="process-math">\(f(x)\)</span> of degree at most <span class="process-math">\(n\)</span> in the following sense: given any any other trigonometric polynomial <span class="process-math">\(g\in W\text{,}\)</span> we have</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/th_orthogonal_projection.html">
\begin{equation*}
\left\vert\left\vert f-\hat{f}\right\vert\right\vert\leq \norm{f-g}\text{.}
\end{equation*}
</div>
<p class="continuation">Unpacking the definition of norm in this inner product space, we conclude that</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/th_orthogonal_projection.html">
\begin{equation*}
\int_0^{2\pi} (f-\hat{f})^2\, dx\leq \int_0^{2\pi} (f-g)^2 \, dx
\end{equation*}
</div>
<p class="continuation">for all <span class="process-math">\(g\in W\text{.}\)</span></p>
<p id="p-2544">Thus, given a continuous function <span class="process-math">\(f\)</span> on <span class="process-math">\([0,2\pi]\text{,}\)</span> linear algebra shows us how to find its best trigonometric polynomial approximation of the form</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
g(x)=a_0+\sum_{k=1}^na_k\cos kx +b_k\sin kx\text{.}
\end{equation*}
</div>
<p class="continuation">However, linear algebra does not tell us just how good this approximation is. This question, among others, is tackled by another mathematical theory: <em class="emphasis">Fourier analysis</em>. There we learn that the trigonometric polynomial approximations get arbitrarily close to <span class="process-math">\(f\)</span> as we let <span class="process-math">\(n\)</span> increase. More precisely, letting <span class="process-math">\(\hat{f}_n\)</span> be the orthogonal projection of <span class="process-math">\(f\)</span> onto the space of trigonometric polynomials of degree at most <span class="process-math">\(n\text{,}\)</span> we have</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\lim_{n\to\infty}\norm{f-\hat{f}_n}=0\text{.}
\end{equation*}
</div></section><section class="subsection" id="subsection-70"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.3.5</span> <span class="title">Least-squares solution to linear systems</span>
</h3>
<p id="p-2545">In statistics we often wish to approximate a scatter plot of points <span class="process-math">\(P_i=(X_i, Y_i)\text{,}\)</span> <span class="process-math">\(1\leq i\leq m\text{,}\)</span> with a line <span class="process-math">\(\ell\colon y=mx+b\)</span> that “best fits” the data. “Finding” this line amounts to finding the appropriate slope <span class="process-math">\(m\)</span> and <span class="process-math">\(y\)</span>-intercept <span class="process-math">\(b\text{:}\)</span> i.e., in this setup, the points <span class="process-math">\(P_i=(X_i, Y_i)\)</span> are given, and <span class="process-math">\(m\)</span> and <span class="process-math">\(b\)</span> are the unknowns we wish to find. For the line to perfectly fit the data, we would want</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
Y_i=mX_i+b \text{ for all } 1\leq i\leq m\text{.}
\end{equation*}
</div>
<p class="continuation">In other words <span class="process-math">\((m,b)\)</span> would be a solution to the matrix equation <span class="process-math">\(A\boldx=\boldy\text{,}\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\boldx=\begin{bmatrix}m \\ b\end{bmatrix},A=\begin{bmatrix} X_1\amp 1\\ X_2\amp 1\\ \vdots \amp \vdots \\ X_m\amp 1  \end{bmatrix},  \boldy=\begin{bmatrix} Y_1\\ Y_2\\ \vdots \\ Y_m\end{bmatrix}\text{.}
\end{equation*}
</div>
<p class="continuation">Of course in most situations the provided points do not lie on a line, and thus there is no solution <span class="process-math">\(\boldx\)</span> to the given matrix equation <span class="process-math">\(A\boldx=\boldy\text{.}\)</span> When this is the case we can use the theory of orthogonal projection to find what is called a <em class="emphasis">least-squares</em> solution, which we now describe in detail.</p>
<p id="p-2546">The least-squares method applies to any matrix equation</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/th_fundspaces_matrixtransform.html ./knowl/eq_least-squares_orig.html ./knowl/eq_least-squares_new.html ./knowl/eq_least-squares_orig.html" id="eq_least-squares_orig">
\begin{equation}
\underset{m\times n}{A}\, \underset{n\times 1}{\boldx}=\underset{m\times 1}{\boldy}\text{,}\tag{4.3.5}
\end{equation}
</div>
<p class="continuation">where <span class="process-math">\(A\)</span> and <span class="process-math">\(\boldy\)</span> are given, and <span class="process-math">\(\boldx\)</span> is treated as an unknown vector. Recall that</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/th_fundspaces_matrixtransform.html ./knowl/eq_least-squares_orig.html ./knowl/eq_least-squares_new.html ./knowl/eq_least-squares_orig.html" id="md-205">
\begin{align*}
A\boldx=\boldy \text{ has a solution } \amp\iff \boldy\in \CS A \amp (\knowl{./knowl/th_fundspaces_matrixtransform.html}{\text{Theorem 3.8.6}}) \text{.}
\end{align*}
</div>
<p class="continuation">When <span class="process-math">\(\boldy\notin \CS A\text{,}\)</span> and hence <a href="" class="xref" data-knowl="./knowl/eq_least-squares_orig.html" title="Equation 4.3.5">(4.3.5)</a> does not have a solution, the least-squares method proceeds by replacing <span class="process-math">\(\boldy\)</span> with the element of <span class="process-math">\(W=\CS A\)</span> closest to it: that is, with its <em class="emphasis">orthogonal projection</em> onto <span class="process-math">\(W\text{.}\)</span>  Let <span class="process-math">\(\hat{\boldy}=\proj{\boldy}{W}\text{,}\)</span> where orthogonal projection is taken with respect to the dot product on <span class="process-math">\(\R^m\text{,}\)</span> and consider the adjusted matrix equation</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/th_fundspaces_matrixtransform.html ./knowl/eq_least-squares_orig.html ./knowl/eq_least-squares_new.html ./knowl/eq_least-squares_orig.html" id="eq_least-squares_new">
\begin{equation}
A\boldx=\hat{\boldy}\text{.}\tag{4.3.6}
\end{equation}
</div>
<p class="continuation">By definition of <span class="process-math">\(\operatorname{proj}_W\text{,}\)</span>  we have <span class="process-math">\(\hat{\boldy}\in W=\CS A\text{,}\)</span> and thus there is a solution <span class="process-math">\(\hat{\boldx}\)</span> to <a href="" class="xref" data-knowl="./knowl/eq_least-squares_new.html" title="Equation 4.3.6">(4.3.6)</a>. We call <span class="process-math">\(\hat{\boldx}\)</span> a <dfn class="terminology">least-squares</dfn> solution to <a href="" class="xref" data-knowl="./knowl/eq_least-squares_orig.html" title="Equation 4.3.5">(4.3.5)</a>. Observe that <span class="process-math">\(\hat{\boldx}\)</span> does <em class="emphasis">not</em> necessarily satisfy <span class="process-math">\(A\hat{\boldx}=\boldy\text{;}\)</span> rather, it satisfies <span class="process-math">\(A\hat{\boldx}=\hat{\boldy}\text{.}\)</span> What makes this a “least-squares” solution is that <span class="process-math">\(A\hat{\boldx}=\hat{\boldy}\)</span> is the element of <span class="process-math">\(W=\CS A\)</span> closest to <span class="process-math">\(\boldy\text{.}\)</span> With respect to the dot product, this means that a least-squares solution <span class="process-math">\(\hat{\boldx}\)</span> minimizes the quantity</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/th_fundspaces_matrixtransform.html ./knowl/eq_least-squares_orig.html ./knowl/eq_least-squares_new.html ./knowl/eq_least-squares_orig.html">
\begin{equation*}
\norm{\boldy-A\boldx}=\sqrt{(y_1-y_1')^2+(y_2-y_2')^2+\cdots +(y_n-y_n')^2}\text{,}
\end{equation*}
</div>
<p class="continuation">among all <span class="process-math">\(\boldx\in \R^n\text{.}\)</span></p>
<article class="example example-like" id="eg_least-squares"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.3.19</span><span class="period">.</span><span class="space"> </span><span class="title">Best fitting line.</span>
</h4>
<p id="p-2547">Suppose we wish to find a line <span class="process-math">\(\ell\colon y=mx+b\)</span> that best fits (in the least-square sense) the following data points: <span class="process-math">\(P_1=(-3,1), P_2=(1,2), P_3=(2,3)\text{.}\)</span> Following the discussion above, we seek a solution <span class="process-math">\(\boldx=(m,b)\)</span> to the matrix equation <span class="process-math">\(A\boldx=\boldy\text{,}\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_ortho_proj_formula.html">
\begin{equation*}
\boldx=\begin{bmatrix}m \\ b \end{bmatrix}, A=\begin{amatrix}[rr]-3\amp 1\\ 1\amp 1\\ 2\amp 1 \end{amatrix}  , \boldy=\begin{bmatrix}1\\ 2\\ 3 \end{bmatrix}\text{.}
\end{equation*}
</div>
<p class="continuation">Using Gaussian elimination, we see easily that this equation has no solution: equivalently, <span class="process-math">\(\boldy\notin W=\CS A\text{.}\)</span> Accordingly, we compute <span class="process-math">\(\hat{\boldy}=\proj{\boldy}{W}\)</span> and find a solution to <span class="process-math">\(A\hat{\boldx}=\hat{\boldy}\text{.}\)</span> Conveniently, the set <span class="process-math">\(B=\{(-3,2,1), (1,1,1)\}\)</span> is already an <em class="emphasis">orthogonal</em> basis of <span class="process-math">\(W=\CS A\text{,}\)</span> allowing us to use <a href="" class="xref" data-knowl="./knowl/eq_ortho_proj_formula.html" title="Equation 4.3.2">(4.3.2)</a>:</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_ortho_proj_formula.html">
\begin{equation*}
\hat{\boldy}=\frac{\boldy\cdot (-3,1,2)}{(-3,2,1)\cdot (-3,1,2)}(-3,1,2)+\frac{\boldy\cdot(1,1,1)}{(1,1,1)\cdot (1,1,1)}(1,1,1)=\frac{1}{14}(13, 33, 38)\text{.}
\end{equation*}
</div>
<p class="continuation">Lastly, solving <span class="process-math">\(A\hat{\boldx}=\hat{\boldy}\)</span> yields <span class="process-math">\((m,b)=\hat{\boldx}=(5/14, 2)\text{,}\)</span> and we conclude the line <span class="process-math">\(\ell\colon y=(5/14)x+2\)</span> is the one that best fits the data in the least-squares sense.</p></article><article class="remark remark-like" id="rm_least-squares"><h4 class="heading">
<span class="type">Remark</span><span class="space"> </span><span class="codenumber">4.3.20</span><span class="period">.</span><span class="space"> </span><span class="title">Visualizing least-squares.</span>
</h4>
<p id="p-2548"><a href="" class="xref" data-knowl="./knowl/fig_leastsquares.html" title="Figure 4.3.21: Least-squares visualization">Figure 4.3.21</a> helps us give a graphical interpretation of how the line <span class="process-math">\(\ell\colon y=(5/14)x+2\)</span> best approximates the points <span class="process-math">\(P_1=(-3,1), P_2=(1,2), P_3=(2,3)\text{.}\)</span> <figure class="figure figure-like" id="fig_leastsquares"><div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="external/images/im_leastsquares.svg" role="img" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">4.3.21<span class="period">.</span></span><span class="space"> </span>Least-squares visualization</figcaption></figure> Let <span class="process-math">\(\boldy=(1,2,3)=(y_1,y_2,y_3)\)</span> be the given <span class="process-math">\(y\)</span>-values of the points, and let <span class="process-math">\(\hat{\boldy}=(y_1',y_2',y_3')\)</span> be the orthogonal projection of <span class="process-math">\(\boldy\)</span> onto <span class="process-math">\(\CS A\text{.}\)</span> In the graph the values <span class="process-math">\(\epsilon_i\)</span> denote the vertical difference <span class="process-math">\(\epsilon_i=y_i-y_i'\)</span> between the data points, and our fitting line. The projection <span class="process-math">\(\hat{\boldy}\)</span> makes the error <span class="process-math">\(\norm{\boldy-\hat{\boldy}}=\sqrt{ \epsilon_1^2+\epsilon_2^2+\epsilon_3^2}\)</span> as small as possible. This means if I draw <em class="emphasis">any other line</em> and compute the corresponding differences <span class="process-math">\(\epsilon_i'\)</span> at the <span class="process-math">\(x\)</span>-values  -3, 1 and 2, then</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/fig_leastsquares.html">
\begin{equation*}
\epsilon_1^2+\epsilon_2^2+\epsilon_3^2\leq (\epsilon_1')^2+(\epsilon_2')^2+(\epsilon_3')^2
\end{equation*}
</div></article><p id="p-2549">To compute a least-squares solution to <span class="process-math">\(A\boldx=\boldy\)</span> we must first compute the orthogonal projection of <span class="process-math">\(\boldy\)</span> onto <span class="process-math">\(W=\CS A\text{;}\)</span> and this in turn requires first producing an orthogonal basis of <span class="process-math">\(\CS A\text{,}\)</span> which may require using the Gram-Schmidt procedure. The following result bypasses these potentially onerous steps by  characterizing a least-squares solution to <span class="process-math">\(A\boldx=\boldy\)</span> as a solution to the matrix equation</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A^TA\boldx=A^T\boldy\text{.}
\end{equation*}
</div>
<article class="theorem theorem-like" id="th_leastsquares"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.3.22</span><span class="period">.</span><span class="space"> </span><span class="title">Least-squares matrix formula.</span>
</h4>
<p id="p-2550">Given an <span class="process-math">\(m\times n\)</span> matrix <span class="process-math">\(A\)</span> and <span class="process-math">\(m\times 1\)</span> column vector <span class="process-math">\(\boldy\text{,}\)</span> a vector <span class="process-math">\(\hat{\boldx}\)</span> is a least-squares solution to <span class="process-math">\(A\boldx=\boldy\)</span> if and only if</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_leastsquares_matrix_equation.html" id="eq_leastsquares_matrix_equation">
\begin{equation}
A^TA\boldx=A^T\boldy\text{.}\tag{4.3.7}
\end{equation}
</div>
<p class="continuation">In other words, we can find a least-squares solution by solving the matrix equation <a href="" class="xref" data-knowl="./knowl/eq_leastsquares_matrix_equation.html" title="Equation 4.3.7">(4.3.7)</a> directly.</p></article><article class="hiddenproof" id="proof-85"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-85"><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-85"><article class="hiddenproof"><p id="p-2551">Let <span class="process-math">\(W=\CS A\text{,}\)</span> and let <span class="process-math">\(\hat{\boldy}=\proj{\boldy}{W}\text{.}\)</span> The key observation is that a vector <span class="process-math">\(\hat{\boldx}\)</span> satisfies <span class="process-math">\(A\hat{\boldx}=\hat{\boldy}\)</span> if and only if</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/th_row_null_comp.html">
\begin{equation*}
\boldy=A\hat{\boldx}+(\boldy-A\hat{\boldx})
\end{equation*}
</div>
<p class="continuation">is an orthogonal decomposition of <span class="process-math">\(\boldy\)</span> with respect to <span class="process-math">\(W=\CS A\text{;}\)</span> and this is true if and only if <span class="process-math">\(\boldy-A\hat{\boldx}\in (\CS A)^\perp\text{.}\)</span> Thus we have</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/th_row_null_comp.html" id="md-206">
\begin{align*}
A\hat{\boldx}=\hat{\boldy} \amp\iff \boldy-A\hat{\boldx}\in (\CS A)^\perp \\
\amp\iff \boldy-A\hat{\boldx}\in \NS A^T \amp ((\CS A)^\perp=\NS A^T, \knowl{./knowl/th_row_null_comp.html}{\text{Theorem 4.3.7}}) \\
\amp\iff A^T(\boldy-A\hat{\boldx})=\boldzero \\
\amp \iff A^T\boldy-A^TA\hat{\boldx}=\boldzero\\
\amp \iff A^TA\hat{\boldx}=A^T\boldy\text{.}
\end{align*}
</div></article></div>
<article class="example example-like" id="example-92"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.3.23</span><span class="period">.</span>
</h4>
<p id="p-2552">Consider again the matrix equation <span class="process-math">\(A\boldx=\boldy\)</span> from <a href="" class="xref" data-knowl="./knowl/eg_least-squares.html" title="Example 4.3.19: Best fitting line">Example 4.3.19</a>. According to <a href="" class="xref" data-knowl="./knowl/th_leastsquares.html" title="Theorem 4.3.22: Least-squares matrix formula">Theorem 4.3.22</a> the least-squares solution can be found by solving the equation <span class="process-math">\(A^TA\boldx=A^T\boldy\)</span> for <span class="process-math">\(\boldx\text{.}\)</span> We compute</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eg_least-squares.html ./knowl/th_leastsquares.html" id="md-207">
\begin{align*}
A^TA\amp=\begin{amatrix}[rr]14\amp 0\\ 0\amp 3  \end{amatrix} \amp A^T\boldy\amp =\begin{amatrix}[r] 5\\ 6 \end{amatrix}
\end{align*}
</div>
<p class="continuation">and solve</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eg_least-squares.html ./knowl/th_leastsquares.html">
\begin{equation*}
\begin{amatrix}[rr]14\amp 0\\ 0\amp 3  \end{amatrix}\boldx=\begin{amatrix}[r] 5\\ 6 \end{amatrix}\iff
\boldx=(5/14, 2),
\end{equation*}
</div>
<p class="continuation">just as before.</p></article></section><section class="exercises" id="s_orthogonal_projection_ex"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">4.3.6</span> <span class="title">Exercises</span>
</h3>
<div class="exercisegroup" id="exercisegroup-28">
<h4 class="heading"><span class="title">.</span></h4>
<div class="introduction" id="introduction-66">In each exercise below you are given an inner product space <span class="process-math">\(V\text{,}\)</span> a subspace <span class="process-math">\(W=\Span B\)</span> where <span class="process-math">\(B\)</span> is orthogonal, and a vector <span class="process-math">\(\boldv\in V\text{.}\)</span> Compute <span class="process-math">\(\proj{\boldv}{W}\text{.}\)</span>
</div>
<div class="exercisegroup-exercises">
<article class="exercise exercise-like" id="exercise-265"><h5 class="heading"><span class="codenumber">1<span class="period">.</span></span></h5>
<p id="p-2553"><span class="process-math">\(V=\R^4\)</span> with the dot product; <span class="process-math">\(W=\Span\{(1,1,1,1),(1,-1,1,-1), (1,1,-1,-1)\}\text{;}\)</span> <span class="process-math">\(\boldv=(2,3,-1,1)\)</span></p></article><article class="exercise exercise-like" id="exercise-266"><h5 class="heading"><span class="codenumber">2<span class="period">.</span></span></h5>
<p id="p-2554"><span class="process-math">\(V=\R^3\)</span> with dot product with weights <span class="process-math">\(k_1=1, k_2=2, k_3=1\text{;}\)</span> <span class="process-math">\(W=\{(1,1,1), (1,-1,1),(1,0,-1)\}\text{;}\)</span> <span class="process-math">\(\boldv=(1,2,3)\)</span></p></article><article class="exercise exercise-like" id="exercise-267"><h5 class="heading"><span class="codenumber">3<span class="period">.</span></span></h5>
<p id="p-2555"><span class="process-math">\(V=C([0,2\pi])\)</span> with the integral inner product; <span class="process-math">\(W=\Span\{\cos x, \cos 2x, \sin x\}\text{;}\)</span> <span class="process-math">\(f(x)=3\)</span> for all <span class="process-math">\(x\in [0,2\pi]\)</span></p></article>
</div>
</div>
<article class="exercise exercise-like" id="exercise-268"><h4 class="heading"><span class="codenumber">4<span class="period">.</span></span></h4>
<p id="p-2556">Let <span class="process-math">\(\mathcal{P}\subseteq \R^3\)</span> be the plane passing through the origin with normal vector <span class="process-math">\(\boldn=(1,2,-1)\text{.}\)</span> Find the orthogonal projection of <span class="process-math">\((1,1,1)\)</span> onto <span class="process-math">\(\mathcal{P}\)</span> with respect to the dot product.</p></article><article class="exercise exercise-like" id="exercise-269"><h4 class="heading"><span class="codenumber">5<span class="period">.</span></span></h4>
<p id="p-2557">Recall that the trace <span class="process-math">\(\tr A\)</span> of a square matrix is the sum of its diagonal entries. Let <span class="process-math">\(V=M_{22}\)</span> with inner product <span class="process-math">\(\angvec{A,B}=\tr(A^TB)\text{.}\)</span> (You may take for granted that this operation is indeed an inner product on <span class="process-math">\(M_{22}\text{.}\)</span>) Define <span class="process-math">\(W=\{A\in M_{22}\colon \tr A=0\}\text{.}\)</span></p>
<ol class="lower-alpha">
<li id="li-797"><p id="p-2558">Compute an orthogonal basis for <span class="process-math">\(W\text{.}\)</span> You can do this either by inspection (the space is manageable), or by starting with any basis of <span class="process-math">\(W\)</span> and applying the Gram-Schmidt procedure.</p></li>
<li id="li-798">
<p id="p-2559">Compute <span class="process-math">\(\proj{A}{W}\text{,}\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A=\begin{bmatrix}1\amp 2\\ 1\amp 1 \end{bmatrix}\text{.}
\end{equation*}
</div>
</li>
</ol></article><article class="exercise exercise-like" id="exercise-270"><h4 class="heading"><span class="codenumber">6<span class="period">.</span></span></h4>
<p id="p-2560">Let <span class="process-math">\(V=C([0,1])\)</span> with the integral inner product, and let <span class="process-math">\(f(x)=x\text{.}\)</span> Find the function of the form <span class="process-math">\(g(x)=a+b\cos(2\pi x)+c\sin(2\pi x)\)</span> that “best approximates” <span class="process-math">\(f(x)\)</span> in terms of this inner product: i.e. find the the <span class="process-math">\(g(x)\)</span> of this form that minimizes <span class="process-math">\(d(g,f)\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-28" id="hint-28"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-28"><div class="hint solution-like"><p id="p-2561">The set <span class="process-math">\(S=\{f(x)=1, g(x)=\cos(2\pi x), h(x)=\sin(2\pi x)\}\)</span> is orthogonal with respect to the given inner product.</p></div></div>
</div></article><article class="exercise exercise-like" id="ex_ortho_comp"><h4 class="heading"><span class="codenumber">7<span class="period">.</span></span></h4>
<p id="p-2562">Let <span class="process-math">\((V, \langle , \rangle )\)</span> be an inner product space, let <span class="process-math">\(S=\{\boldw_1, \boldw_2, \dots, \boldw_r\}\subseteq V\text{,}\)</span> and let <span class="process-math">\(W=\Span S\text{.}\)</span> Prove:</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\boldv\in W^\perp \text{ if and only if } \langle \boldv,\boldw_i \rangle=0 \text{ for all } 1\leq i\leq r\text{.}
\end{equation*}
</div>
<p class="continuation">In other words, to check whether an element is in <span class="process-math">\(W^\perp\text{,}\)</span> it suffices to check that it is orthogonal to each element of its spanning set <span class="process-math">\(S\text{.}\)</span></p></article><article class="exercise exercise-like" id="exercise-272"><h4 class="heading"><span class="codenumber">8<span class="period">.</span></span></h4>
<p id="p-2563">Consider the inner product space <span class="process-math">\(\R^4\)</span> together with the dot product. Let</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
W=\{(x_1,x_2,x_3,x_4)\in \R^4\colon x_1=x_3 \text{ and } x_2=x_4\}.
\end{equation*}
</div>
<p class="continuation">Provide orthogonal bases for <span class="process-math">\(W\)</span> and <span class="process-math">\(W^\perp\text{.}\)</span></p></article><article class="exercise exercise-like" id="ex_orthocomp_subspace"><h4 class="heading"><span class="codenumber">9<span class="period">.</span></span></h4>
<p id="p-2564">Prove statements (1) and (2) of <a href="" class="xref" data-knowl="./knowl/th_orthogonal_complement.html" title="Theorem 4.3.5: Orthogonal complement">Theorem 4.3.5</a>.</p></article><article class="exercise exercise-like" id="ex_orthocomp_dim"><h4 class="heading">
<span class="codenumber">10<span class="period">.</span></span><span class="space"> </span><span class="title">Dimension of <span class="process-math">\(W^\perp\)</span>.</span>
</h4>
<p id="p-2565">Prove statement (3) of <a href="" class="xref" data-knowl="./knowl/th_orthogonal_complement.html" title="Theorem 4.3.5: Orthogonal complement">Theorem 4.3.5</a>:  if <span class="process-math">\((V, \ \angvec{\ , \ })\)</span> is an inner product space of dimension <span class="process-math">\(n\text{,}\)</span> and <span class="process-math">\(W\)</span> is a subspace of <span class="process-math">\(V\text{,}\)</span> then</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/th_orthogonal_complement.html">
\begin{equation*}
\dim W+\dim W^\perp=n\text{.}
\end{equation*}
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-29" id="hint-29"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-29"><div class="hint solution-like"><p id="p-2566">By <a href="" class="xref" data-knowl="./knowl/cor_orthonormal_existence.html" title="Corollary 4.2.10: Existence of orthonormal bases">Corollary 4.2.10</a> there is an orthogonal basis <span class="process-math">\(B=\{\boldv_1,\dots ,\boldv_r\}\)</span> of <span class="process-math">\(W\text{,}\)</span> and furthermore, we can extend <span class="process-math">\(B\)</span> to an orthogonal basis <span class="process-math">\(B'=\{\boldv_1,\boldv_2, \dots, \boldv_r, \boldu_1,\dots , \boldu_{n-r}\}\)</span> of all of <span class="process-math">\(V\text{.}\)</span> Show the <span class="process-math">\(\boldu_i\)</span> form a basis for <span class="process-math">\(W^\perp\text{.}\)</span></p></div></div>
</div></article><article class="exercise exercise-like" id="exercise-275"><h4 class="heading"><span class="codenumber">11<span class="period">.</span></span></h4>
<p id="p-2567">Prove <a href="" class="xref" data-knowl="./knowl/cor_orthoproj_linear.html" title="Corollary 4.3.14: Orthgonal projection is linear">Corollary 4.3.14</a> following the suggestion in the text.</p></article><article class="exercise exercise-like" id="ex_orthoproj_props"><h4 class="heading"><span class="codenumber">12<span class="period">.</span></span></h4>
<p id="p-2568">Let <span class="process-math">\(V\)</span> an inner product space, and let <span class="process-math">\(W\subseteq V\)</span> be a finite-dimensional subspace. Prove the following statements:</p>
<ol class="lower-alpha">
<li id="li-799"><p id="p-2569"><span class="process-math">\(\boldv\in W\)</span> if and only if <span class="process-math">\(\proj{\boldv}{W}=\boldv\text{;}\)</span></p></li>
<li id="li-800"><p id="p-2570"><span class="process-math">\(\boldv\in W^\perp\)</span> if and only if  <span class="process-math">\(\proj{\boldv}{W}=\boldzero\text{.}\)</span></p></li>
</ol></article><article class="exercise exercise-like" id="exercise-277"><h4 class="heading"><span class="codenumber">13<span class="period">.</span></span></h4>
<p id="p-2571">We consider the problem of fitting a collection of data points <span class="process-math">\((x,y)\)</span> with a quadratic curve of the form <span class="process-math">\(y=f(x)=ax^2+bx+c\text{.}\)</span> Thus we are <em class="emphasis">given</em> some collection of points <span class="process-math">\((x,y)\text{,}\)</span> and we <em class="emphasis">seek</em> parameters <span class="process-math">\(a,
b, c\)</span> for which the graph of <span class="process-math">\(f(x)=ax^2+bx+c\)</span> “best fits” the points in some way.</p>
<ol class="lower-alpha">
<li id="li-801"><p id="p-2572">Show, using linear algebra, that if we are given any three points <span class="process-math">\((x,y)=(r_1,s_1), (r_2,s_2), (r_3,s_3)\text{,}\)</span> where the <span class="process-math">\(x\)</span>-coordinates <span class="process-math">\(r_i\)</span> are all distinct, then there is a <em class="emphasis">unique</em> choice of <span class="process-math">\(a,b,c\)</span> such that the corresponding quadratic function agrees <em class="emphasis">precisely</em> with the data. In other words, given just about any three points in the plane, there is a unique quadratic curve connecting them.</p></li>
<li id="li-802">
<p id="p-2573">Now suppose we are given the four data points</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
P_1=(0,2), P_2=(1,0), P_3=(2,2), P_4=(3,6)\text{.}
\end{equation*}
</div>
<ol class="lower-roman">
<li id="li-803"><p id="p-2574">Use the least-squares method described in the lecture notes to come up with a quadratic function <span class="process-math">\(y=f(x)\)</span> that “best fits” the data.</p></li>
<li id="li-804"><p id="p-2575">Graph the function <span class="process-math">\(f\)</span> you found, along with the points <span class="process-math">\(P_i\text{.}\)</span> (You may want to use technology.) Use your graph to explain precisely in what sense <span class="process-math">\(f\)</span> “best fits” the data.</p></li>
</ol>
</li>
</ol></article></section></section></div></main>
</div>
</body>
</html>
