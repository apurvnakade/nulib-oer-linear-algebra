<!DOCTYPE html>
<html lang="en-US">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2022-02-07T10:11:38-06:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body>
<article class="hiddenproof"><h4 class="heading"><span class="type">Proof</span></h4>
<p>Recall that to show two statements \(\mathcal{P}\) and \(\mathcal{Q}\) are equivalent, we must show two implications: \(\mathcal{P}\implies \mathcal{Q}\text{,}\) and \(\mathcal{Q}\implies \mathcal{P}\text{.}\) Instead of doing this for each possible pair of sentences above, we ease our work load by instead showing the following <em xmlns:svg="http://www.w3.org/2000/svg" class="emphasis">cycle</em> of implications:</p>
<div xmlns:svg="http://www.w3.org/2000/svg" class="displaymath">
\begin{equation*}
(1)\implies(2)\implies(3)\implies(4)\implies(5)\implies (1)\text{.}
\end{equation*}
</div>
<p class="continuation">Since implication is transitive, starting at any point in our cycle and making our way around the chain of implications, we see that any one of the propositions implies any other proposition.</p>
<article class="case"><h5 class="heading">\((1)\implies (2)\).</h5>
<p>Suppose \(A^{-1}\) exists. Given any column vector \(\boldb\text{,}\) we have</p>
<div xmlns:svg="http://www.w3.org/2000/svg" class="displaymath">
\begin{align*}
A\boldx=\boldb \iff \boldx=A^{-1}\boldb \amp (\knowl{./knowl/th_inverse_cancel.html}{\text{Theorem 3.3.3}})\text{,}
\end{align*}
</div>
<p class="continuation">which shows that \(\boldx=A^{-1}\boldb\) is the unique solution to \(A\boldx=\boldb\text{.}\)</p></article><article class="case"><h5 class="heading">\((2)\implies (3)\).</h5>
<p>Clearly, if \(A\boldx=\boldb\) has a unique solution for <em xmlns:svg="http://www.w3.org/2000/svg" class="emphasis">any</em> choice of \(\boldb\text{,}\) then it has a unique solution for the <em xmlns:svg="http://www.w3.org/2000/svg" class="emphasis">particular</em> choice \(\boldb=\boldzero\text{.}\) Since \(\boldx=\boldzero_{n\times 1}\) is clearly a solution to the equation, it must be the only solution.</p></article><article class="case"><h5 class="heading">\((3)\implies (4)\).</h5>
<p>Row reduce \(A\) to a matrix \(U\) in <em xmlns:svg="http://www.w3.org/2000/svg" class="emphasis">reduced</em> row echelon form using Gauss-Jordan elimination. (See <a href="" class="xref" data-knowl="./knowl/th_matrixforms.html" title="Theorem 2.2.10: Row equivalent matrix forms">Theorem 2.2.10</a>.) Since the set of solutions to \(A\boldx=\boldzero\) is identical to the set of solutions to \(U\boldx=\boldzero\) (apply <a href="" class="xref" data-knowl="./knowl/s_systems_th_rowops.html" title="Theorem 2.1.12: Row equivalence theorem">Theorem 2.1.12</a> to their corresponding linear systems), we see that \(\boldzero\) is the only solution to \(U\boldx=\boldzero\text{.}\) <a href="" class="xref" data-knowl="./knowl/th_solveSystem.html" title="Theorem 2.3.5: Solving linear systems">Theorem 2.3.5</a> now implies \(U\) has a leading one in each column. Since \(U\) is \(n\times n\) and in <em xmlns:svg="http://www.w3.org/2000/svg" class="emphasis">reduced</em> row echelon form, it follows that \(U\) must be the identity matrix. (Convince yourself of this.) Thus \(A\) is row equivalent to \(U=I\text{,}\) the identity matrix.</p></article><article class="case"><h5 class="heading">\((4)\implies (5)\).</h5>
<p>If \(A\) is row equivalent to \(I\text{,}\) then according to our discussion after <a href="" class="xref" data-knowl="./knowl/eq_GE_via_elem_mat.html" title="Equation 3.4.3">(3.4.3)</a>, we have \(I=E_rE_{r-1}\cdots E_1A\) for some collection of elementary matrices \(E_i\text{.}\) Since elementary matrices are invertible we can multiply both sides of this equation by</p>
<div xmlns:svg="http://www.w3.org/2000/svg" class="displaymath">
\begin{equation*}
(E_rE_{r-1}\cdots E_1)^{-1}=E_1^{-1}E_{2}^{-1}\cdots E_r^{-1}
\end{equation*}
</div>
<p class="continuation">to conclude</p>
<div xmlns:svg="http://www.w3.org/2000/svg" class="displaymath">
\begin{equation*}
A=E_1^{-1}E_{2}^{-1}\cdots E_r^{-1}\text{.}
\end{equation*}
</div>
<p class="continuation">Since inverses of elementary matrices are elementary (<a href="" class="xref" data-knowl="./knowl/th_inverse_elem.html" title="Theorem 3.4.3: Inverses of elementary matrices">Theorem 3.4.3</a>), we conclude that \(A\) is a product of elementary matrices.</p></article><article class="case"><h5 class="heading">\((5)\implies (1)\).</h5>
<p>If \(A\) is a product of elementary matrices, then it is a product of invertible matrices. Since products of invertible matrices are invertible, we conclude that \(A\) is invertible.</p></article></article><span class="incontext"><a href="s_invertibility_theorem.html#proof-22">in-context</a></span>
</body>
</html>
